[\#999 Issue](https://github.com/rear/rear/issues/999) `closed`: since SLES12-SP2 btrfs quota via "snapper setup-quota" is needed
=================================================================================================================================

**Labels**: `enhancement`, `fixed / solved / done`

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) opened issue at [2016-09-16 08:14](https://github.com/rear/rear/issues/999):

After
[https://github.com/rear/rear/issues/944](https://github.com/rear/rear/issues/944)
and
[https://github.com/rear/rear/issues/963](https://github.com/rear/rear/issues/963)
which was in the end a false-positive  
I finally found (by luck) a real change from SLES12-SP1 to SP2  
that actually requires an appropriate enhancement  
in Relax-and-Recover:

Since SLES12-SP2 btrfs quota via "snapper setup-quota" is used  
which means "snapper setup-quota" must be called  
during "rear recover" if that was used in the original system.

I.e. during "rear mkbackup" I need to autodetect whether or not  
btrfs quota via "snapper setup-quota" is used and if yes store that  
info somehow in disklayout.conf (I assume filesystem quota info  
belongs to disklayout.conf) so that "rear recover" could call  
"snapper setup-quota" when needed.

Regarding how btrfs quota is used for snapper see  
[http://snapper.io/2016/05/18/space-aware-cleanup.html](http://snapper.io/2016/05/18/space-aware-cleanup.html)

For the reasoning behind why btrfs quota is used for snapper see  
[https://lists.opensuse.org/opensuse-factory/2016-09/msg00074.html](https://lists.opensuse.org/opensuse-factory/2016-09/msg00074.html)  
[https://lists.opensuse.org/opensuse-factory/2016-09/msg00121.html](https://lists.opensuse.org/opensuse-factory/2016-09/msg00121.html)  
[https://lists.opensuse.org/opensuse-factory/2016-09/msg00124.html](https://lists.opensuse.org/opensuse-factory/2016-09/msg00124.html)

For details probably best is to read the whole (very interesting)  
"btrfs quotas enabled by default" mail thread in Sep 2016  
on the <opensuse-factory@opensuse.org> mailing list.

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2016-09-16 08:52](https://github.com/rear/rear/issues/999#issuecomment-247550710):

This issue is no bug in Relax-and-Recover because

a)  
missing support for newest stuff in newest Linux distribution versions  
is never ever a bug (but it is an enhancement), and

b)  
current "rear recover" (without support for "snapper setup-quota")  
works sufficiently well.

After current "rear recover" the admin must manually re-setup  
btrfs quota for snapper in the recovered system according to  
what is described under "steps for manually setup" in  
[http://snapper.io/2016/05/18/space-aware-cleanup.html](http://snapper.io/2016/05/18/space-aware-cleanup.html)

How I did manual btrfs quota re-setup for snapper  
in a recovered SLES12-SP2 system  
(I run "yast users" only as an example to get some snapshots):

<pre>
# btrfs qgroup show -p /
ERROR: can't perform the search - No such file or directory
ERROR: can't list qgroups: No such file or directory
# snapper setup-quota
Quota failure (qgroup already set).
# snapper get-config | grep QGROUP
QGROUP                 | 1/0  
# snapper set-config QGROUP=
# snapper get-config | grep QGROUP
QGROUP                 |      
# snapper setup-quota
# btrfs qgroup show -p /
qgroupid         rfer         excl parent  
--------         ----         ---- ------  
0/5          16.00KiB     16.00KiB ---     
...
0/278        16.00KiB     16.00KiB ---
0/279         3.15GiB      3.15GiB ---     
1/0             0.00B        0.00B ---     
# snapper list
Type   | # | Pre # | Date                          | User | Cleanup | Description           | Userdata
-------+---+-------+-------------------------------+------+---------+-----------------------+---------
single | 0 |       |                               | root |         | current               |         
single | 1 |       | Fri 16 Sep 2016 10:27:55 CEST | root |         | first root filesystem |         
# yast users
# snapper list
Type   | # | Pre # | Date                          | User | Cleanup | Description           | Userdata
-------+---+-------+-------------------------------+------+---------+-----------------------+---------
single | 0 |       |                               | root |         | current               |         
single | 1 |       | Fri 16 Sep 2016 10:27:55 CEST | root |         | first root filesystem |         
pre    | 2 |       | Fri 16 Sep 2016 10:33:04 CEST | root | number  | yast users            |         
post   | 3 | 2     | Fri 16 Sep 2016 10:33:10 CEST | root | number  |                       |         
# btrfs subvolume list / | grep snapshots
ID 278 gen 51 top level 257 path @/.snapshots
ID 279 gen 49 top level 278 path @/.snapshots/1/snapshot
ID 284 gen 48 top level 278 path @/.snapshots/2/snapshot
ID 285 gen 49 top level 278 path @/.snapshots/3/snapshot
# btrfs qgroup show -p /
qgroupid         rfer         excl parent  
--------         ----         ---- ------  
0/5          16.00KiB     16.00KiB ---     
...
0/278        16.00KiB     16.00KiB ---
0/279         3.15GiB     16.00KiB ---     
0/284         3.15GiB      3.16MiB 1/0     
0/285         3.15GiB     16.00KiB 1/0     
1/0             0.00B      3.14MiB ---     
# btrfs quota rescan /
quota rescan started
# btrfs quota rescan -s /
no rescan operation in progress
# btrfs qgroup show -p /
qgroupid         rfer         excl parent  
--------         ----         ---- ------  
0/5          16.00KiB     16.00KiB ---     
...
0/278        16.00KiB     16.00KiB ---
0/279         3.15GiB     16.00KiB ---     
0/284         3.15GiB      3.16MiB 1/0     
0/285         3.15GiB     16.00KiB 1/0     
1/0           3.15GiB      3.17MiB ---     
</pre>

The last "btrfs qgroup show -p /" here matches that output  
in
[http://snapper.io/2016/05/18/space-aware-cleanup.html](http://snapper.io/2016/05/18/space-aware-cleanup.html)  
so that my manual btrfs quota setup for snapper looks o.k.

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2016-09-16 10:25](https://github.com/rear/rear/issues/999#issuecomment-247567993):

I tried to automate it with  
(long line shown wrapped here):

<pre>
POST_RECOVERY_SCRIPT=( 
 '/usr/bin/snapper --no-dbus -r /mnt/local set-config QGROUP=
 && echo snapper set empty QGROUP succeeded
 || echo snapper set empty QGROUP failed'
 '/usr/bin/snapper --no-dbus -r /mnt/local setup-quota
 && echo snapper setup-quota succeeded
 || echo snapper setup-quota failed' )
</pre>

in /etc/rear/local.conf but that does not work because  
usr/share/rear/wrapup/default/50\_post\_recovery\_script.sh  
runs

<pre>
        eval "${POST_RECOVERY_SCRIPT[@]}"
</pre>

instead of

<pre>
    for command in "${POST_RECOVERY_SCRIPT[@]}" ; do
        eval "$command"
    done
</pre>

but after I also changed 50\_post\_recovery\_script.sh this way  
I have automated btrfs quota re-setup for snapper  
during "rear recover" and in the recovered system I have now  
(I run "yast users" only as an example to get some snapshots):

<pre>
# btrfs qgroup show -p /
qgroupid         rfer         excl parent  
--------         ----         ---- ------  
0/5          16.00KiB     16.00KiB ---     
...
0/278        16.00KiB     16.00KiB ---     
0/279         3.15GiB      3.15GiB ---     
1/0             0.00B        0.00B ---     
255/260      24.00KiB     24.00KiB ---     
# yast users
# snapper list
Type   | # | Pre # | Date                          | User | Cleanup | Description           | Userdata
-------+---+-------+-------------------------------+------+---------+-----------------------+---------
single | 0 |       |                               | root |         | current               |         
single | 1 |       | Fri 16 Sep 2016 11:49:56 CEST | root |         | first root filesystem |         
pre    | 2 |       | Fri 16 Sep 2016 12:12:40 CEST | root | number  | yast users            |         
post   | 3 | 2     | Fri 16 Sep 2016 12:12:47 CEST | root | number  |                       |         
# btrfs subvolume list / | grep snapshots
ID 278 gen 45 top level 257 path @/.snapshots
ID 279 gen 45 top level 278 path @/.snapshots/1/snapshot
ID 284 gen 44 top level 278 path @/.snapshots/2/snapshot
ID 285 gen 45 top level 278 path @/.snapshots/3/snapshot
# btrfs qgroup show -p /
qgroupid         rfer         excl parent  
--------         ----         ---- ------  
0/5          16.00KiB     16.00KiB ---     
...
0/278        16.00KiB     16.00KiB ---     
0/279         3.15GiB     16.00KiB ---     
0/284         3.15GiB      3.16MiB 1/0     
0/285         3.15GiB     16.00KiB 1/0     
1/0             0.00B      3.14MiB ---     
255/260      24.00KiB     24.00KiB ---     
</pre>

All what is missing is "btrfs quota rescan /"  
to get for the "1/0" btrfs qgroup the actually  
right amount of "rfer" (referenced size).  
I did that manually:

<pre>
# btrfs quota rescan /
quota rescan started
# btrfs qgroup show -p / | grep '1/0'
...
1/0           3.15GiB      3.45MiB ---
...
</pre>

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2016-09-16 12:50](https://github.com/rear/rear/issues/999#issuecomment-247592409):

It seems "btrfs quota rescan /" does not work reliably  
(or does not show reliably results):

I did one more recovery and now in the recovered system  
I did several times "btrfs quota rescan /" without an effect.

After each "btrfs quota rescan /" I waited some time  
but then I got several times

<pre>
# btrfs qgroup show -p /
...
1/0             0.00B        0.00B ---
...
</pre>

that changed after some time into

<pre>
# btrfs qgroup show -p /
...
1/0             0.00B      3.17MiB ---
...
</pre>

until finally it became

<pre>
# btrfs quota rescan /
quota rescan started
# btrfs qgroup show -p /
...
1/0           3.15GiB      3.50MiB ---
...
</pre>

Of course I had also tried "btrfs quota rescan -w /"  
and "btrfs quota rescan -s /" but I never got a respose  
that a rescan operation would be currently still running  
(i.e. for me it seems any rescan operation was basically  
immediately finished).

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2016-09-16 13:37](https://github.com/rear/rear/issues/999#issuecomment-247602454):

To get during "rear recover" automated btrfs quota  
re-setup for snapper if that is used in the original system  
the following works for me in /etc/rear/local.conf  
(a single long line shown wrapped here):

<pre>
POST_RECOVERY_SCRIPT=(
 '/usr/bin/snapper --no-dbus -r /mnt/local get-config
 | grep -q "^QGROUP.*1/0"
 && ( echo snapper setup-quota
   && /usr/bin/snapper --no-dbus -r /mnt/local set-config QGROUP=
   && /usr/bin/snapper --no-dbus -r /mnt/local setup-quota
   || echo snapper setup-quota failed )
 || echo snapper setup-quota not used' )
</pre>

without any additional adaptions in  
usr/share/rear/wrapup/default/50\_post\_recovery\_script.sh  
(and without "btrfs quota rescan" support).

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2016-09-20 11:41](https://github.com/rear/rear/issues/999#issuecomment-248277367):

I asked our (i.e. SUSE's) snapper expert and according to him  
running "snapper setup-quota" alone is sufficient to setup btrfs quota  
for snapper (i.e. no additional "btrfs quota rescan /" needed).

Therefore the POST\_RECOVERY\_SCRIPT in the current  
SLE12-SP2-btrfs-example.conf is sufficient to fix the issue.

I tested it on SLES12-SP2-RC2 with its default btrfs structure  
and for me it "just works".

------------------------------------------------------------------------

\[Export of Github issue for
[rear/rear](https://github.com/rear/rear).\]
