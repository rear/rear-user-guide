[\#2377 Issue](https://github.com/rear/rear/issues/2377) `closed`: Remove duplicates in COPY\_AS\_IS PROGS REQUIRED\_PROGS LIBS before processing?
==================================================================================================================================================

**Labels**: `enhancement`, `cleanup`, `fixed / solved / done`

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) opened issue at [2020-04-22 06:56](https://github.com/rear/rear/issues/2377):

Currently the elements in the arrays  
COPY\_AS\_IS PROGS REQUIRED\_PROGS LIBS  
are processed one by one "as is" by scripts like  
build/GNU/Linux/100\_copy\_as\_is.sh  
build/GNU/Linux/390\_copy\_binaries\_libraries.sh  
so that duplicate array elements are processed as often as they appear.

This way nothing goes wrong, it only takes more time if there are
duplicates.  
In particular the ordering of the elements in the arrays is kept.  
Perhaps the ordering in COPY\_AS\_IS might be important.

I wonder if it is worth the effort to remove duplicates  
in particular to filter out duplicates that keeps the ordering of the
elements.

I will experiment a bit with that...

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2020-04-22 09:27](https://github.com/rear/rear/issues/2377#issuecomment-617663569):

At least for my test case the COPY\_AS\_IS array  
results only a few duplicates:

    # sort /tmp/rear.RX0ACBwlXC2bRiS/tmp/copy-as-is-filelist | uniq -cd
          2 /etc/localtime
          2 /etc/modules-load.d/
          2 /usr/share/kbd/keymaps/legacy/i386/qwerty/defkeymap.map.gz

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2020-04-22 12:50](https://github.com/rear/rear/issues/2377#issuecomment-617759345):

For PROGS and REQUIRED\_PROGS duplicates are already removed  
in build/GNU/Linux/390\_copy\_binaries\_libraries.sh  
via the `sort -u` in this code

    local all_binaries=( $( for bin in "${PROGS[@]}" "${REQUIRED_PROGS[@]}" ; do
                                bin_path="$( get_path "$bin" )"
                                if test -x "$bin_path" ; then
                                    echo $bin_path
                                    Log "Found binary $bin_path"
                                fi
                            done 2>>/dev/$DISPENSABLE_OUTPUT_DEV | sort -u ) )

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2020-04-22 12:57](https://github.com/rear/rear/issues/2377#issuecomment-617762660):

For LIBS duplicates are already removed in the following way:

In build/GNU/Linux/390\_copy\_binaries\_libraries.sh there is

    local all_libs=( "${LIBS[@]}" $( RequiredSharedObjects "${all_binaries[@]}" "${LIBS[@]}" ) )

and the `RequiredSharedObjects` function in lib/linux-functions.sh

    function RequiredSharedObjects () {
        ...
        for file_for_ldd in $@ ; do
            ...
            ldd $file_for_ldd
            ...
        done 2>>/dev/$DISPENSABLE_OUTPUT_DEV | awk ' /^\t.+ => not found/ { print "Shared object " $1 " not found" > "/dev/stderr" }
                                                     /^\t.+ => \// { print $3 }
                                                     /^\t\// && !/ => / { print $1 } ' | sort -u
    }

has a `sort -u`

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2020-04-23 09:48](https://github.com/rear/rear/issues/2377#issuecomment-618301702):

Filtering out duplicates and keeping the ordering via

    printf '%s\n' "${ARRAY[@]}" | awk '!seen[$0]++'

seems to run rather fast even for huge arrays:

    # unset arr

    # for i in $( seq 1000 9999 ) ; do arr+=( "element $i with \ * ' ? special chars" ) ; done

    # for i in $( seq 9999 -1 1000 ) ; do arr+=( "element $i with \ * ' ? special chars" ) ; done

    # printf '%s\n' "${arr[@]}" | wc -l
    18000

    # printf '%s\n' "${arr[@]}" | head -n3
    element 1000 with \ * ' ? special chars
    element 1001 with \ * ' ? special chars
    element 1002 with \ * ' ? special chars

    # printf '%s\n' "${arr[@]}" | tail -n3
    element 1002 with \ * ' ? special chars
    element 1001 with \ * ' ? special chars
    element 1000 with \ * ' ? special chars

    # time printf '%s\n' "${arr[@]}" | awk '!seen[$0]++' | wc -l
    9000

    real    0m0.160s
    user    0m0.167s
    sys     0m0.028s

    # printf '%s\n' "${arr[@]}" | awk '!seen[$0]++' | head -n3
    element 1000 with \ * ' ? special chars
    element 1001 with \ * ' ? special chars
    element 1002 with \ * ' ? special chars

    # printf '%s\n' "${arr[@]}" | awk '!seen[$0]++' | tail -n3
    element 9997 with \ * ' ? special chars
    element 9998 with \ * ' ? special chars
    element 9999 with \ * ' ? special chars

For me COPS\_AS\_IS has about 130 elements  
so a more realistic test with an array that has 200 duplicate elements:

    # unset arr

    # for i in $( seq 100 299 ) ; do arr+=( "element $i with \ * ' ? special chars" ) ; done
    # for i in $( seq 299 -1 100 ) ; do arr+=( "element $i with \ * ' ? special chars" ) ; done

    # printf '%s\n' "${arr[@]}" | wc -l
    400

    # time printf '%s\n' "${arr[@]}" | awk '!seen[$0]++' | wc -l
    200

    real    0m0.012s
    user    0m0.011s
    sys     0m0.007s

I used a not so fast Intel Core i3-4000M CPU @ 2.40GHz for that tests  
cf.
[https://github.com/rear/rear/issues/2364\#issuecomment-616567193](https://github.com/rear/rear/issues/2364#issuecomment-616567193)

So filtering out duplicates and keeping the ordering in COPY\_AS\_IS  
that needs to run only once in build/GNU/Linux/100\_copy\_as\_is.sh  
could be a reasonable thing to have cleaner working code there.  
From my personal point of view our code looks sloppy and careless  
when we let `tar` needlessly copy duplicated things several times.

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2020-04-23 15:48](https://github.com/rear/rear/issues/2377#issuecomment-618476943):

Via
[https://github.com/rear/rear/pull/2378](https://github.com/rear/rear/pull/2378)  
I implemented filtering out duplicates in `COPY_AS_IS`  
and also removing duplicates in the `copy_as_is_filelist_file`

#### <img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50">[jsmeix](https://github.com/jsmeix) commented at [2020-04-27 13:29](https://github.com/rear/rear/issues/2377#issuecomment-619986511):

With
[https://github.com/rear/rear/pull/2378](https://github.com/rear/rear/pull/2378)
merged  
this issue is done.

------------------------------------------------------------------------

\[Export of Github issue for
[rear/rear](https://github.com/rear/rear).\]
