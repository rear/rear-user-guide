<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="Relax-and-Recover (ReaR) User Guide Documentation"/>
    <meta property="og:description" content="This is an umbrella documentation project for all Relax-and-Recover (ReaR) kind of documentation ans starting with a good User Guide."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://relax-and-recover.org/rear-user-guide/"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://relax-and-recover.org/rear-user-guide/img/rear_logo_50.png"/>
    <meta property="og:image:width" content="50"/>
    <meta property="og:image:height" content="50"/>
    
    <title>#2514 PR merged: Wipe disks before recreating partitions/volumes/filesystems/... - Relax-and-Recover (ReaR) User Guide Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/rear.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "#2514 PR merged: Wipe disks before recreating partitions/volumes/filesystems/...";
        var mkdocs_page_input_path = "issues/2020-11-11.2514.pr.merged.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', "366986045", "auto");
        ga('send', 'pageview');
      </script>
    
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Relax-and-Recover (ReaR) User Guide Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">BASICS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/introduction.html">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/history.html">Bit of History</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/getting-started.html">Getting started with ReaR</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/configuration.html">Basic configuration</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SCENARIOS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../scenarios/index.html">Scenarios Overview</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">DEVELOPMENT</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../development/github-pr.html">Make a pull request with GitHub</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/index.html">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear27.html">Release Notes ReaR 2.7</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear26.html">Release Notes ReaR 2.6</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/knownproblems.html">Known Problems and Workarounds</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ISSUES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Issues History</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">#2514 PR merged: Wipe disks before recreating partitions/volumes/filesystems/...</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2514_pr_merged_wipe_disks_before_recreating_partitionsvolumesfilesystems"><a href="https://github.com/rear/rear/pull/2514">#2514 PR</a> <code>merged</code>: Wipe disks before recreating partitions/volumes/filesystems/...<a class="headerlink" href="#2514_pr_merged_wipe_disks_before_recreating_partitionsvolumesfilesystems" title="Permanent link">&para;</a></h1>
<p><strong>Labels</strong>: <code>enhancement</code>, <code>fixed / solved / done</code>, <code>severe improvement</code></p>
<h4 id="jsmeix_opened_issue_at_2020-11-11_1512"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> opened issue at <a href="https://github.com/rear/rear/pull/2514">2020-11-11 15:12</a>:<a class="headerlink" href="#jsmeix_opened_issue_at_2020-11-11_1512" title="Permanent link">&para;</a></h4>
<p>New layout/recreate/default/150_wipe_disks.sh to wipe disks before
recreating.</p>
<ul>
<li>
<p>Type: <strong>Enhancement</strong></p>
</li>
<li>
<p>Impact: <strong>High</strong></p>
</li>
<li>
<p>Reference to related issue (URL):<br />
<a href="https://github.com/rear/rear/issues/799">https://github.com/rear/rear/issues/799</a></p>
</li>
<li>
<p>How was this pull request tested?<br />
    On my SLES15 test system with LUKS and LVM:</p>
</li>
</ul>
<!-- -->

<pre><code># lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT
NAME                                               KNAME     PKNAME    TRAN TYPE  FSTYPE       SIZE MOUNTPOINT
/dev/sda                                           /dev/sda            ata  disk                20G 
|-/dev/sda1                                        /dev/sda1 /dev/sda       part                 8M 
`-/dev/sda2                                        /dev/sda2 /dev/sda       part  crypto_LUKS   20G 
  `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2 /dev/dm-0 /dev/sda2      crypt LVM2_member   20G 
    |-/dev/mapper/system-swap                      /dev/dm-1 /dev/dm-0      lvm   swap           2G [SWAP]
    |-/dev/mapper/system-root                      /dev/dm-2 /dev/dm-0      lvm   btrfs       12.6G /
    `-/dev/mapper/system-home                      /dev/dm-3 /dev/dm-0      lvm   xfs          5.4G /home
/dev/sdb                                           /dev/sdb            ata  disk                 1G 
|-/dev/sdb1                                        /dev/sdb1 /dev/sdb       part  crypto_LUKS  307M 
| `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00004-part1 /dev/dm-4 /dev/sdb1      crypt ext4         305M /luks1test
`-/dev/sdb2                                        /dev/sdb2 /dev/sdb       part  crypto_LUKS  409M
</code></pre>
<ul>
<li>Brief description of the changes in this pull request:</li>
</ul>
<p>New layout/recreate/default/150_wipe_disks.sh to wipe disks.<br />
The disks that will be completely wiped are those disks<br />
where in diskrestore.sh the create_disk_label function is called<br />
(the create_disk_label function calls "parted -s $disk mklabel
$label")<br />
i.e. the disks that will be completely overwritten by diskrestore.sh.<br />
This implements
<a href="https://github.com/rear/rear/issues/799">https://github.com/rear/rear/issues/799</a><br />
"Clean up disks before recreating partitions/volumes/filesystems/..."<br />
The intent is to be also used later as a precondition for the future<br />
new 'storage' stage/code as future replacement of the 'layout'
stage/code<br />
cf.
<a href="https://github.com/rear/rear/issues/2510">https://github.com/rear/rear/issues/2510</a></p>
<h4 id="jsmeix_commented_at_2020-11-11_1539"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-725494390">2020-11-11 15:39</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-11-11_1539" title="Permanent link">&para;</a></h4>
<p>The interesting part is that in the ReaR recovery system<br />
on replacement hardware that has the same disk layout<br />
as the original system has (because I did a "rear recover"<br />
of the original system on the replacement hardware)<br />
I do not see all those nested block devices that do actually exist<br />
on the disk of the replacement hardware.<br />
In the ReaR recovery system I only get before 150_wipe_disks.sh is run</p>
<pre><code>rear&gt; lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT,UUID
NAME        KNAME     PKNAME   TRAN TYPE FSTYPE       SIZE MOUNTPOINT UUID
/dev/sda    /dev/sda           ata  disk               20G            
|-/dev/sda1 /dev/sda1 /dev/sda      part                8M            
`-/dev/sda2 /dev/sda2 /dev/sda      part crypto_LUKS   20G            0c58676a-bcb6-42be-8e1c-46a24d954ca7
/dev/sdb    /dev/sdb           ata  disk                1G            
|-/dev/sdb1 /dev/sdb1 /dev/sdb      part crypto_LUKS  307M            fb79b19e-0e6d-4570-aa96-fa968d6e0795
`-/dev/sdb2 /dev/sdb2 /dev/sdb      part crypto_LUKS  409M            52bc31e4-e9dc-4de0-99cc-d9cca3a886b0
/dev/sr0    /dev/sr0           ata  rom  iso9660     76.9M            2020-11-11-15-35-02-75
</code></pre>
<p>and while 150_wipe_disks.sh is run I get on the terminal<br />
with <code>export MIGRATION_MODE='true'</code> and then <code>rear -D recover</code></p>
<pre><code>Disks to be overwritten: /dev/sda /dev/sdb 
1) Confirm disks to be completely overwritten and continue 'rear recover'
2) Use Relax-and-Recover shell and return back to here
3) Abort 'rear recover'
(default '1' timeout 300 seconds)
1
UserInput: Valid choice number result 'Confirm disks to be completely overwritten and continue 'rear recover''
User confirmed disks to be overwritten
Wiping child devices of /dev/sda in reverse ordering: /dev/sda2 /dev/sda1 /dev/sda 
Wiping device /dev/sda2
Wiping device /dev/sda1
Wiping device /dev/sda
Wiping child devices of /dev/sdb in reverse ordering: /dev/sdb2 /dev/sdb1 /dev/sdb 
Wiping device /dev/sdb2
Wiping device /dev/sdb1
Wiping device /dev/sdb
</code></pre>
<p>and after 150_wipe_disks.sh is run I get</p>
<pre><code>rear&gt; lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT,UUID
NAME     KNAME    PKNAME TRAN TYPE FSTYPE   SIZE MOUNTPOINT UUID
/dev/sda /dev/sda        ata  disk           20G            
/dev/sdb /dev/sdb        ata  disk            1G            
/dev/sr0 /dev/sr0        ata  rom  iso9660 76.9M            2020-11-11-15-35-02-75
</code></pre>
<p>so 150_wipe_disks.sh wiped all what was visible in the recovery
system.</p>
<p>Nevertheless there is still metadata on the disk from those nested block
devices<br />
that are invisible in the recovery system</p>
<pre><code>  `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2 /dev/dm-0 /dev/sda2      crypt LVM2_member   20G 
    |-/dev/mapper/system-swap                      /dev/dm-1 /dev/dm-0      lvm   swap           2G [SWAP]
    |-/dev/mapper/system-root                      /dev/dm-2 /dev/dm-0      lvm   btrfs       12.6G /
    `-/dev/mapper/system-home                      /dev/dm-3 /dev/dm-0      lvm   xfs          5.4G /home
...
| `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00004-part1 /dev/dm-4 /dev/sdb1      crypt ext4         305M /luks1test
</code></pre>
<p>so that metadata from nested block devices could still get in the way<br />
when recreating things exactly as they have been before on the disk.</p>
<h4 id="jsmeix_commented_at_2020-11-11_1611"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-725512823">2020-11-11 16:11</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-11-11_1611" title="Permanent link">&para;</a></h4>
<p>FYI<br />
excerpts from dialog that I had yesterday with a SUSE colleague<br />
who is an expert in storage setup:</p>
<p>I asked him:</p>
<pre><code>I have a question about RAID devices deep inside a disk partition:
I would assume when I wipe the first and last 16 MiB of a partition
then sufficient RAID metadata is wiped so that automatisms could
no longer detect that there is anything RAID related deep inside
the partition.
I assume the automatisms would not scan the whole partition
to find RAID related things deep inside
(i.e. I assume when nothing is found at the beginning or end
the whole partition is ignored).
</code></pre>
<p>He repiled:</p>
<pre><code>Suppose you have a mirroring RAID,
and inside the RAID two partitions,
the second one starts at 1GB and includes a XFS.
The signature of that XFS is now not within the first 16 MB
of the underlying partition of the RAID.
So sda1 and sdb1 make up md0, that has md0p1 and md0p2.
Cleaning 16 MB of sda1 and sdb1 will not clean md0p2.
</code></pre>
<p>I replied</p>
<pre><code>Yes, but my expectation was that cleaning 16 MB of sda1 and sdb1
will clean md0 so that md0p1 and md0p2 get ignored.
But now I see an issue even if md0 was cleaned this way.
When the same RAID setup is recreated on that disk
with exactly same values
then after recreating md0 the old md0p1 and md0p2
may reappear instantly
which lets the commands that intend to recreate md0p1
and md0p2 fail and
also the old XFS may reappear but here enforced
"mksf.xfs -f" helps (which is done e.g. in ReaR).
</code></pre>
<p>Then I had "A totally offhanded crazy idea"</p>
<pre><code>A totally offhanded crazy idea:
Perhaps it works best in practice to not care about old stuff on the disk
but recreate the disk partitions with an offset of e.g. 1 MiB
(or 4 MiB or 8 MiB to be more on the safe side for SSDs alignments).
Who would care (in particular in case of disaster recovery) in practice
about 8 MiB less disk space for the last partition on each disk?
Of course only optionally when requested so the user can decide.
</code></pre>
<p>After sleeping over it I think that "totally offhanded crazy idea"<br />
is perhaps not so "totally crazy" because such a thing is needed
anyway<br />
for properly migrating from rotating rust disks to SSDs:</p>
<p>When migrating from rotating rust disks to SSDs new disk alignment<br />
is usually needed so that the partition boundaries on the SSDs<br />
match the phsyical "sector size" or "segment size" or "erase block
size"<br />
(i.e. the physical read/write unit size) of the SSD<br />
which is 4 MiB or 8 MiB or even more on nowadays SSDs<br />
cf. USB_PARTITION_ALIGN_BLOCK_SIZE in default.conf<br />
<a href="https://github.com/rear/rear/blob/master/usr/share/rear/conf/default.conf#L853">https://github.com/rear/rear/blob/master/usr/share/rear/conf/default.conf#L853</a></p>
<p>So when migrating from rotating rust disks to SSDs<br />
the partition begin values must be adapted to be<br />
multiples of the physical read/write unit size of the SSD<br />
i.e. multiples of 4 MiB or 8 MiB or even more.</p>
<p>So when migrating from rotating rust disks to SSDs the partition begin
values<br />
(that are usually multiples of 1 MiB or less on rotating rust disks)<br />
need to be shifted to multiples of 4 MiB or 8 MiB or more<br />
which means in the end to recreate the disk partitions with an offset.</p>
<p>So an option to recreate the disk partitions with an offset<br />
is needed anyway in ReaR and such an option can then<br />
also be (mis)-used to avoid issues when recreating on a used disk<br />
the same storage objects at exact same positions on the disk<br />
(i.e. to avoid exact same positions when recreating same storage
objects)<br />
in particular when recreating on the same hardware (i.e. when<br />
the replacement hardware is the original system hardware).</p>
<p>Of course when we recreate the disk partitions with an offset<br />
the next needed feature is to also adapt the sizes of nested<br />
storage objects inside disk partitions like LVM volumes<br />
which is yet another feature that we need since a long time<br />
to make migration easier for the user.</p>
<h4 id="jsmeix_commented_at_2020-11-12_1435"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-726116193">2020-11-12 14:35</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-11-12_1435" title="Permanent link">&para;</a></h4>
<p>FYI and for completeness here a full "rear -D recover" terminal output<br />
when recreating this storage structure</p>
<pre><code># lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT,UUID
NAME                                               KNAME     PKNAME    TRAN TYPE  FSTYPE       SIZE MOUNTPOINT UUID
/dev/sda                                           /dev/sda            ata  disk                20G            
|-/dev/sda1                                        /dev/sda1 /dev/sda       part                 8M            
`-/dev/sda2                                        /dev/sda2 /dev/sda       part  crypto_LUKS   20G            0c58676a-bcb6-42be-8e1c-46a24d954ca7
  `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2 /dev/dm-0 /dev/sda2      crypt LVM2_member   20G            zJalOt-2mjE-OrW4-MRxO-ErID-beeL-rRwzOp
    |-/dev/mapper/system-swap                      /dev/dm-1 /dev/dm-0      lvm   swap           2G [SWAP]     a88670c6-43c7-4024-822f-f0fa0d00cfbc
    |-/dev/mapper/system-root                      /dev/dm-2 /dev/dm-0      lvm   btrfs       12.6G /          bd326c18-0806-47d7-a740-97d5047d7de4
    `-/dev/mapper/system-home                      /dev/dm-3 /dev/dm-0      lvm   xfs          5.4G /home      c9a5ebc9-3eac-4aa8-a768-731295af64a8
/dev/sdb                                           /dev/sdb            ata  disk                 1G            
|-/dev/sdb1                                        /dev/sdb1 /dev/sdb       part  crypto_LUKS  307M            fb79b19e-0e6d-4570-aa96-fa968d6e0795
| `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00004-part1 /dev/dm-4 /dev/sdb1      crypt ext4         305M /luks1test 745a0d13-2b73-4a81-a81c-96906c45ef5a
`-/dev/sdb2                                        /dev/sdb2 /dev/sdb       part  crypto_LUKS  409M            30376f43-60fd-4fc7-af0c-fad8063d5a1a
  `-/dev/mapper/luks2test                          /dev/dm-5 /dev/sdb2      crypt ext4         405M /luks2test 850e0fcc-6739-4190-9940-0b27cb82ee66
/dev/sr0                                           /dev/sr0            ata  rom   iso9660      657M            2020-01-08-15-17-54-22
/dev/sr1                                           /dev/sr1            ata  rom   iso9660      8.2G            2020-01-08-15-53-34-34
</code></pre>
<p>on replacement hardware where this storage structure<br />
had been already recreated by a "rear recover" run before.</p>
<p>In the recovery system on the replacement hardware<br />
only this partial storage structure is visible (i.e. only disks with
their partitions):</p>
<pre><code>RESCUE linux-uxxi:~ # lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT,UUID
NAME        KNAME     PKNAME   TRAN TYPE FSTYPE       SIZE MOUNTPOINT UUID
/dev/sda    /dev/sda           ata  disk               20G            
|-/dev/sda1 /dev/sda1 /dev/sda      part                8M            
`-/dev/sda2 /dev/sda2 /dev/sda      part crypto_LUKS   20G            0c58676a-bcb6-42be-8e1c-46a24d954ca7
/dev/sdb    /dev/sdb           ata  disk                1G            
|-/dev/sdb1 /dev/sdb1 /dev/sdb      part crypto_LUKS  307M            fb79b19e-0e6d-4570-aa96-fa968d6e0795
`-/dev/sdb2 /dev/sdb2 /dev/sdb      part crypto_LUKS  409M            30376f43-60fd-4fc7-af0c-fad8063d5a1a
/dev/sr0    /dev/sr0           ata  rom  iso9660     76.9M            2020-11-12-13-37-02-39

RESCUE linux-uxxi:~ # rear -D recover
Relax-and-Recover 2.6 / Git
Running rear recover (PID 810 date 2020-11-12 15:18:47)
Using log file: /var/log/rear/rear-linux-uxxi.log
Running workflow recover within the ReaR rescue/recovery system
Starting required daemons for NFS: RPC portmapper (portmap or rpcbind) and rpc.statd if available.
Started RPC portmapper 'rpcbind'.
RPC portmapper 'rpcbind' available.
Started rpc.statd.
RPC status rpc.statd available.
Using backup archive '/tmp/rear.0pEcVkENRxqJSI5/outputfs/linux-uxxi/backup.tar.gz'
Will do driver migration (recreating initramfs/initrd)
Calculating backup archive size
Backup archive size is 1.5G     /tmp/rear.0pEcVkENRxqJSI5/outputfs/linux-uxxi/backup.tar.gz (compressed)
Comparing disks
Device sda has expected (same) size 21474836480 bytes (will be used for 'recover')
Device sdb has expected (same) size 1073741824 bytes (will be used for 'recover')
Disk configuration looks identical
UserInput -I DISK_LAYOUT_PROCEED_RECOVERY needed in /usr/share/rear/layout/prepare/default/250_compare_disks.sh line 148
Proceed with 'recover' (yes) otherwise manual disk layout configuration is enforced
(default 'yes' timeout 30 seconds)

UserInput: No real user input (empty or only spaces) - using default input
UserInput: No choices - result is 'yes'
User confirmed to proceed with 'recover'
Marking component '/dev/sda' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/sda1' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/sda2' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/sdb' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/sdb1' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/sdb2' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/cr_ata-QEMU_HARDDISK_QM00004-part1' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'pv:/dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/system' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/system-home' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/system-root' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/system-swap' as done in /var/lib/rear/layout/disktodo.conf
Doing SLES-like btrfs subvolumes setup for /dev/mapper/system-root on / (BTRFS_SUBVOLUME_SLES_SETUP contains /dev/mapper/system-root)
SLES12-SP1 (and later) btrfs subvolumes setup needed for /dev/mapper/system-root (default subvolume path contains '@/.snapshots/')
Marking component 'fs:/' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'fs:/luks1test' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'fs:/home' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/.snapshots' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/var' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/usr/local' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/srv' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/boot/grub2/x86_64-efi' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/root' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/tmp' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/boot/grub2/i386-pc' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'btrfsmountedsubvol:/opt' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'swap:/dev/mapper/system-swap' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/luks2test' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'fs:/luks2test' as done in /var/lib/rear/layout/disktodo.conf
Wiping child devices of /dev/sda in reverse ordering: /dev/sda2 /dev/sda1 /dev/sda 
Wiped first 16777216 bytes of /dev/sda2
Wiped last 16777216 bytes of /dev/sda2
Wiped first 8388608 bytes of /dev/sda1
Skip wiping at the end of /dev/sda1 (dvice size 8388608 not greater than the bytes that were wiped)
Wiped first 16777216 bytes of /dev/sda
Wiped last 16777216 bytes of /dev/sda
Wiping child devices of /dev/sdb in reverse ordering: /dev/sdb2 /dev/sdb1 /dev/sdb 
Wiped first 16777216 bytes of /dev/sdb2
Wiped last 16777216 bytes of /dev/sdb2
Wiped first 16777216 bytes of /dev/sdb1
Wiped last 16777216 bytes of /dev/sdb1
Wiped first 16777216 bytes of /dev/sdb
Wiped last 16777216 bytes of /dev/sdb
Start system layout restoration.
Disk '/dev/sda': creating 'gpt' partition table
Disk '/dev/sda': creating partition number 1 with name ''sda1''
Disk '/dev/sda': creating partition number 2 with name ''sda2''
Disk '/dev/sdb': creating 'gpt' partition table
Disk '/dev/sdb': creating partition number 1 with name ''sdb1''
Disk '/dev/sdb': creating partition number 2 with name ''sdb2''
Creating LUKS volume cr_ata-QEMU_HARDDISK_QM00004-part1 on /dev/sdb1
Set the password for LUKS volume cr_ata-QEMU_HARDDISK_QM00004-part1 (for 'cryptsetup luksFormat' on /dev/sdb1):
Enter passphrase for /dev/sdb1: 
Enter the password for LUKS volume cr_ata-QEMU_HARDDISK_QM00004-part1 (for 'cryptsetup luksOpen' on /dev/sdb1):
Enter passphrase for /dev/sdb1: 
Creating LUKS volume cr_ata-QEMU_HARDDISK_QM00001-part2 on /dev/sda2
Set the password for LUKS volume cr_ata-QEMU_HARDDISK_QM00001-part2 (for 'cryptsetup luksFormat' on /dev/sda2):
Enter passphrase for /dev/sda2: 
Enter the password for LUKS volume cr_ata-QEMU_HARDDISK_QM00001-part2 (for 'cryptsetup luksOpen' on /dev/sda2):
Enter passphrase for /dev/sda2: 
Creating LVM PV /dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2
Restoring LVM VG 'system'
Sleeping 3 seconds to let udev or systemd-udevd create their devices...
Creating filesystem of type btrfs with mount point / on /dev/mapper/system-root.
Mounting filesystem /
Running snapper/installation-helper
Creating filesystem of type ext4 with mount point /luks1test on /dev/mapper/cr_ata-QEMU_HARDDISK_QM00004-part1.
Mounting filesystem /luks1test
Creating filesystem of type xfs with mount point /home on /dev/mapper/system-home.
Mounting filesystem /home
Creating swap on /dev/mapper/system-swap
Creating LUKS volume luks2test on /dev/sdb2
Set the password for LUKS volume luks2test (for 'cryptsetup luksFormat' on /dev/sdb2):
Enter passphrase for /dev/sdb2: 
Enter the password for LUKS volume luks2test (for 'cryptsetup luksOpen' on /dev/sdb2):
Enter passphrase for /dev/sdb2: 
Creating filesystem of type ext4 with mount point /luks2test on /dev/mapper/luks2test.
Mounting filesystem /luks2test
Disk layout created.
Restoring from '/tmp/rear.0pEcVkENRxqJSI5/outputfs/linux-uxxi/backup.tar.gz' (restore log in /var/lib/rear/restore/recover.backup.tar.gz.810.restore.log) ...
Backup restore program 'tar' started in subshell (PID=4932)
Restored 128 MiB [avg. 43877 KiB/sec] 
...
Restored 3073 MiB [avg. 34581 KiB/sec] 
OK
Restored 3073 MiB in 94 seconds [avg. 33477 KiB/sec]
Restoring finished (verify backup restore log messages in /var/lib/rear/restore/recover.backup.tar.gz.810.restore.log)
Created SELinux /mnt/local/.autorelabel file : after reboot SELinux will relabel all files
Recreating directories (with permissions) from /var/lib/rear/recovery/directories_permissions_owner_group
Migrating disk-by-id mappings in certain restored files in /mnt/local to current disk-by-id mappings ...
Replacing restored udev rule '/mnt/local//etc/udev/rules.d/70-persistent-net.rules' with the one from the ReaR rescue system
Migrating restored network configuration files according to the mapping files ...
Running mkinitrd...
Recreated initrd (/sbin/mkinitrd).
Installing GRUB2 boot loader...
Determining where to install GRUB2 (no GRUB2_INSTALL_DEVICES specified)
Found possible boot disk /dev/sda - installing GRUB2 there
Finished 'recover'. The target system is mounted at '/mnt/local'.
Exiting rear recover (PID 810) and its descendant processes ...
Running exit tasks
You should also rm -Rf /tmp/rear.0pEcVkENRxqJSI5
</code></pre>
<p>Still in the recovery system on the replacement hardware<br />
after "rear recover" the whole storage structure is visible again:</p>
<pre><code>RESCUE linux-uxxi:~ # lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT,UUID
NAME                                       KNAME     PKNAME    TRAN TYPE  FSTYPE       SIZE MOUNTPOINT           UUID
/dev/sda                                   /dev/sda            ata  disk                20G                      
|-/dev/sda1                                /dev/sda1 /dev/sda       part                 8M                      
`-/dev/sda2                                /dev/sda2 /dev/sda       part  crypto_LUKS   20G                      0c58676a-bcb6-42be-8e1c-46a24d954ca7
  `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00001-part2
                                           /dev/dm-1 /dev/sda2      crypt LVM2_member   20G                      zJalOt-2mjE-OrW4-MRxO-ErID-beeL-rRwzOp
    |-/dev/mapper/system-swap              /dev/dm-2 /dev/dm-1      lvm   swap           2G                      a88670c6-43c7-4024-822f-f0fa0d00cfbc
    |-/dev/mapper/system-home              /dev/dm-3 /dev/dm-1      lvm   xfs          5.4G /mnt/local/home      c9a5ebc9-3eac-4aa8-a768-731295af64a8
    `-/dev/mapper/system-root              /dev/dm-4 /dev/dm-1      lvm   btrfs       12.6G /mnt/local           bd326c18-0806-47d7-a740-97d5047d7de4
/dev/sdb                                   /dev/sdb            ata  disk                 1G                      
|-/dev/sdb1                                /dev/sdb1 /dev/sdb       part  crypto_LUKS  307M                      fb79b19e-0e6d-4570-aa96-fa968d6e0795
| `-/dev/mapper/cr_ata-QEMU_HARDDISK_QM00004-part1
|                                          /dev/dm-0 /dev/sdb1      crypt ext4         305M /mnt/local/luks1test 745a0d13-2b73-4a81-a81c-96906c45ef5a
`-/dev/sdb2                                /dev/sdb2 /dev/sdb       part  crypto_LUKS  409M                      30376f43-60fd-4fc7-af0c-fad8063d5a1a
  `-/dev/mapper/luks2test                  /dev/dm-5 /dev/sdb2      crypt ext4         405M /mnt/local/luks2test 850e0fcc-6739-4190-9940-0b27cb82ee66
/dev/sr0                                   /dev/sr0            ata  rom   iso9660     76.9M                      2020-11-12-13-37-02-39
</code></pre>
<h4 id="jsmeix_commented_at_2020-11-16_1450"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-728110687">2020-11-16 14:50</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-11-16_1450" title="Permanent link">&para;</a></h4>
<p>With<br />
<a href="https://github.com/rear/rear/pull/2514/commits/e107f317088a5578940b8130f155e049f01b5403">https://github.com/rear/rear/pull/2514/commits/e107f317088a5578940b8130f155e049f01b5403</a><br />
I get LVM remainders well removed on a used disk.<br />
For now I tested without plain LVM that is not inside LUKS:</p>
<pre><code>NAME                        KNAME     PKNAME    TRAN TYPE FSTYPE       SIZE MOUNTPOINT
/dev/sda                    /dev/sda            ata  disk               20G 
`-/dev/sda1                 /dev/sda1 /dev/sda       part LVM2_member   20G 
  |-/dev/mapper/system-root /dev/dm-0 /dev/sda1      lvm  ext4         7.2G /
  |-/dev/mapper/system-swap /dev/dm-1 /dev/sda1      lvm  swap           2G [SWAP]
  `-/dev/mapper/system-home /dev/dm-2 /dev/sda1      lvm  ext4        10.8G /home
</code></pre>
<p>Removing things inside LUKS comes next...<br />
Here my "rear recover" excerpts<br />
(again on a ystem where "rear recover" was already run before):</p>
<pre><code>RESCUE # lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,SIZE,MOUNTPOINT
NAME        KNAME     PKNAME   TRAN TYPE FSTYPE      SIZE MOUNTPOINT
/dev/sda    /dev/sda           ata  disk              20G            
`-/dev/sda1 /dev/sda1 /dev/sda      part LVM2_member  20G
/dev/sr0    /dev/sr0           ata  rom  udf          71M

RESCUE # lvscan
  inactive          '/dev/system/home' [10.79 GiB] inherit
  inactive          '/dev/system/root' [7.20 GiB] inherit
  inactive          '/dev/system/swap' [2.00 GiB] inherit

RESCUE # vgscan
  Reading all physical volumes.  This may take a while...
  Found volume group "system" using metadata type lvm2

RESCUE # pvscan
  PV /dev/sda1   VG system          lvm2 [20.00 GiB / 8.00 MiB free]
  Total: 1 [20.00 GiB] / in use: 1 [20.00 GiB] / in no VG: 0 [0   ]

RESCUE # rear -D recover
...
User confirmed to proceed with 'recover'
Marking component '/dev/sda' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/sda1' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'pv:/dev/sda1' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/system' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/system-home' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/system-root' as done in /var/lib/rear/layout/disktodo.conf
Marking component '/dev/mapper/system-swap' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'fs:/' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'fs:/home' as done in /var/lib/rear/layout/disktodo.conf
Marking component 'swap:/dev/mapper/system-swap' as done in /var/lib/rear/layout/disktodo.conf
Removed LVM logical volume /dev/system/home
Removed LVM logical volume /dev/system/root
Removed LVM logical volume /dev/system/swap
Removed LVM volume group system
Removed LVM physical volume /dev/sda1
Wiping child devices of /dev/sda in reverse ordering: /dev/sda1 /dev/sda 
Wiped first 16777216 bytes of /dev/sda1
Wiped last 16777216 bytes of /dev/sda1
Wiped first 16777216 bytes of /dev/sda
Wiped last 16777216 bytes of /dev/sda
Start system layout restoration.
Disk '/dev/sda': creating 'msdos' partition table
Disk '/dev/sda': creating partition number 1 with name 'primary'
Creating LVM PV /dev/sda1
Restoring LVM VG 'system'
Sleeping 3 seconds to let udev or systemd-udevd create their devices...
Creating filesystem of type ext4 with mount point / on /dev/mapper/system-root.
Mounting filesystem /
Creating filesystem of type ext4 with mount point /home on /dev/mapper/system-home.
Mounting filesystem /home
Creating swap on /dev/mapper/system-swap
Disk layout created.
Restoring from '/tmp/rear.NpjvSjqHZzWRnWe/outputfs/linux-u89h/backup.tar.gz' ...
</code></pre>
<h4 id="jsmeix_commented_at_2020-11-17_1534"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-729009274">2020-11-17 15:34</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-11-17_1534" title="Permanent link">&para;</a></h4>
<p>It could cause horrible damage if all LVM PVs VGs and LVs get removed<br />
that pvscan/vgscan/lvscan can detect in the recovery system:</p>
<p>Assume there are three disks:<br />
A 100 GiB /dev/sda where the basic operating system is stored<br />
plus a 2 TiB /dev/sdb and a 3 TiB /dev/sdc where the actual data<br />
is stored using LVM with PVs on /dev/sdb and /dev/sdc.</p>
<p>"rear mkbackup" only creates a backup of the files on /dev/sda.<br />
For the about 5 TiB of actual data a separated backup is used.</p>
<p>Assume disk /dev/sda broke down and gets replaced<br />
but /dev/sdb and /dev/sdc had no issues and are kept as is.</p>
<p>So "rear recover" must not touch anything on /dev/sdb or /dev/sdc<br />
but pvscan/vgscan/lvscan detect LVM objects on all disks<br />
in the recovery system.</p>
<p>Accordingly only those PVs get removed that are on a disk which will be
wiped.<br />
Disks which will be wiped are the ones where
<code>parted -s $disk mklabel $label</code><br />
will be called in diskrestore.sh - i.e. disks that will be completely
overwritten.</p>
<p>This is the main reason behind for<br />
<a href="https://github.com/rear/rear/pull/2514/commits/ae48d3ca8d8c251c71c6322bc99ee46ac7b61fa5">https://github.com/rear/rear/pull/2514/commits/ae48d3ca8d8c251c71c6322bc99ee46ac7b61fa5</a></p>
<p>Additionally neither any VGs nor any LVs get removed because VGs and LVs
could be<br />
spread over several disks so that it is not easily possible to determine
which disks belong<br />
to VGs and LVs (stricly speaking it seems to be not possible with
reasonable effort,<br />
at least not for now).</p>
<p>If time permits I will investigate if that is possible with reasonable
effort as follows:<br />
LVs belong to a VG and a VG has several PVs that are on one or more
disks.<br />
Those disks where the PVs for a VG are located are the disks that belong
to that VG<br />
and the same disks belong to all LVs in that VG.<br />
When all disks that belong to a VG will be wiped<br />
then this VG and its LVs can be removed.</p>
<p>The reason behind why it is better to remove first LVs, then VGs, and
finally PVs is<br />
that this way things work cleanly (no longer needed to forcefully remove
PVs with<br />
loud <code>WARNING</code> messages that look as if things went wrong or are done
wrong).</p>
<h4 id="jsmeix_commented_at_2020-11-17_1600"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-729025710">2020-11-17 16:00</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-11-17_1600" title="Permanent link">&para;</a></h4>
<p>Another part that is implemented in<br />
<a href="https://github.com/rear/rear/pull/2514/commits/ae48d3ca8d8c251c71c6322bc99ee46ac7b61fa5">https://github.com/rear/rear/pull/2514/commits/ae48d3ca8d8c251c71c6322bc99ee46ac7b61fa5</a><br />
is removing nested storage objects inside LUKS volumes.</p>
<p>To make nested storage objects in LUKS volumes visible<br />
the LUKS volumes must be opened via<br />
<code>cryptsetup luksOpen $luks_device $luks_mapping_name</code><br />
then nested storage objects in LUKS volumes can be removed<br />
and finally those LUKS volumes must be closed again<br />
via <code>cryptsetup luksClose $luks_mapping_name</code><br />
because leaving LUKS volumes open leads to errors later<br />
when "parted -s $disk mklabel $label" is called in the diskrestore.sh
script<br />
because then 'parted' fails with the following error message:</p>
<pre><code>Partitions ... have been written, but we have been unable to inform
the kernel of the change, probably because it/they are in use.
As a result, the old partition(s) will remain in use.
You should probably reboot now before making further changes.
</code></pre>
<p>In this case it works to reboot the recovery system<br />
and then a second "rear recover" usually succeeds<br />
because after plain wiping a disk with LUKS volumes<br />
the LUKS metadata/signatures are no longer there<br />
so that LUKS cannot be in use in the rebooted recovery system<br />
and "parted -s $disk mklabel $label" can succeed.<br />
But we like to do all in one single "rear recover" run<br />
so we need to clean up LUKS storage objects properly.</p>
<h4 id="jsmeix_commented_at_2020-12-09_0852"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-741629134">2020-12-09 08:52</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-12-09_0852" title="Permanent link">&para;</a></h4>
<p>@gdha<br />
thank you for looking at this one!</p>
<p>In its current state it cleans up old LVM stuff on the disk<br />
so you could try it out in its current state.</p>
<p>The next thing I like to add is "final power to the user"<br />
i.e. a config variable so that the user can specify<br />
if he wants to let ReaR wipe the disks before recreating<br />
or what exact disks ReaR should wipe<br />
or not wipe any disk at all.<br />
With that I could merge it with by default disabled disk wipe
functionality<br />
to get current things into master code so that interested users could
try it out<br />
and provide feedback how good or bad (it could destroy unwanted stuff)
it works.</p>
<p>To try out this particular git branch that contains the current state of
this pull request<br />
you could do something like</p>
<pre><code># git clone --single-branch --branch jsmeix-wipe-disks-before-recreating https://github.com/rear/rear.git
</code></pre>
<h4 id="gdha_commented_at_2020-12-09_1519"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-741839369">2020-12-09 15:19</a>:<a class="headerlink" href="#gdha_commented_at_2020-12-09_1519" title="Permanent link">&para;</a></h4>
<p>@jsmeix will ask to redo my test next week to see if the wipe disks
resolves the below issue during recovery.</p>
<pre><code>+++ lvm vgcreate --physicalextentsize 4096k vg00 /dev/sda2 /dev/sde
 WARNING: Failed to connect to lvmetad. Falling back to device scanning.
 A volume group called vg00 already exists.
</code></pre>
<p>The current work-around is the do a <code>vgremove vg --force</code> on the command
line before <code>rear recover</code> or by adding the next code in the vgcreation
part of diskrestore.sh like:<br />
<code>lvm vgremove vg02 --force || true</code><br />
Both work-arounds are equal for success, however, I think your code
would have the same result.</p>
<h4 id="gdha_commented_at_2020-12-09_1529"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-741845966">2020-12-09 15:29</a>:<a class="headerlink" href="#gdha_commented_at_2020-12-09_1529" title="Permanent link">&para;</a></h4>
<p>@jsmeix The file <code>README.wipe_disks</code> would probably better move to the
<a href="http://relax-and-recover.org/rear-user-guide/">ReaR User Guide</a> as it
is a bit invisible <code>/usr/share/rear/layout/recreate/default</code> directory
to the user.</p>
<h4 id="jsmeix_commented_at_2020-12-09_1533"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-741848807">2020-12-09 15:33</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-12-09_1533" title="Permanent link">&para;</a></h4>
<p>I placed the README.wipe_disks intentionally next to the scripts
because<br />
it is not meant for the user but as background information for further
developers<br />
of such scripts (I don't what to have all that as a comment in the
script itself ;-)</p>
<h4 id="olivero2_commented_at_2020-12-10_2246"><img src="https://avatars.githubusercontent.com/u/4660803?v=4" width="50"><a href="https://github.com/OliverO2">OliverO2</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-742848262">2020-12-10 22:46</a>:<a class="headerlink" href="#olivero2_commented_at_2020-12-10_2246" title="Permanent link">&para;</a></h4>
<p>I have just stumbled into this and might not have seen enough. I
understand that <code>wipefs</code> is missing recursion and LUKS inspection
capabilities, and for example one needs to wipe GPT partitions before
wiping a GPT disk. But it is also my understanding that <code>wipefs</code> knows
(via <code>libblkid</code>) all "filesystem, raid or partition-table signatures".</p>
<p>So I wonder: Why is it not sufficient to use <code>wipefs</code> in a bottom-up
fashion with all of these file systems, volumes and partitions?</p>
<h4 id="jsmeix_commented_at_2020-12-11_0824"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-743049696">2020-12-11 08:24</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-12-11_0824" title="Permanent link">&para;</a></h4>
<p>In general see my personal experiences that I described in<br />
layout/recreate/default/README.wipe_disks<br />
and also in the comments in<br />
layout/recreate/default/150_wipe_disks.sh</p>
<p>In particular wiping LVM stuff is horrible according to my personal
experience.<br />
I found no simple and straightforward way how to generically wipe LVM
stuff.</p>
<p>To see the block devices that belong to LVM the LVM things need to be
activated<br />
cf. layout/recreate/default/README.wipe_disks<br />
but when LVM things are activated (in use) one cannot <code>wipefs</code> their
block devices<br />
(similar as I won't <code>wipefs</code> a disk partition while its filesystem is
still mounted).</p>
<p>So - according to my personal experience - the only way to get rid of
LVM stuff<br />
is to deconstruct the LVM stuff with LVM tools step by step from LVs to
VGs to PVs<br />
cf. layout/recreate/default/150_wipe_disks.sh</p>
<p>But deconstructing LVM stuff step by step from LVs to VGs to PVs gets
impossible<br />
when there are LVs that belong to VGs that contain disks that should not
be wiped.</p>
<p>So in my current layout/recreate/default/150_wipe_disks.sh I only
remove PVs<br />
because things could get too complicated with VGs and LVs - at least too
complicated<br />
for my current understanding - I am not a real LVM expert.</p>
<p>For example assume there is<br />
a smaller /dev/sda for the operating system on LVs that are in a VG
"system"<br />
and a huge /dev/sdb for a database on LVs that are in a VG "data".<br />
The "rear mkbackup" backup is only for the operating system.<br />
The database backup is a separated backup.<br />
Over the time /dev/sda became too small for the operating system<br />
so as band aid a PV /dev/sdb123 was added to the VG "system"<br />
to enlarge the LVs that belong to the VG "system"<br />
so some operating system LVM stuff is then also on sdb.<br />
Years later that bad aid is still there - of course!<br />
Then some soft-error destroyed the operating system<br />
and it should be recreated with "rear recover" on same hardware.<br />
Currently I don't know how to properly deconstruct<br />
only the LVM stuff that belongs to the operating system<br />
but not touch the LVM stuff that belongs to the database<br />
in an automated way.<br />
The "automated" is the crucial point here.<br />
The admin who knows his system can of course manually deconstruct<br />
only his operating system's LVM stuff before he runs "rear recover".</p>
<p>But when the LVM stuff is inside a LUKS volume and the one who runs
"rear recover"<br />
does not know the LUKS password (e.g. because he got the replacement
hardware<br />
with its used disk "as is") it is impossible to see the block devices
that belong to LVM<br />
so one needs a low-level generic method to wipe things which is <code>dd</code><br />
cf. layout/recreate/default/README.wipe_disks</p>
<p>When one has the <code>dd</code> method there is no longer a need to use <code>wipefs</code><br />
because the <code>dd</code> method wipes more than what <code>wipefs</code> would do and<br />
the <code>dd</code> method "just works" and doesn't sometimes ask the user
questions<br />
or refuses to work as <code>wipefs</code> woud do unless called with <code>-f</code> or
wipes<br />
incomplete unless called with <code>-a</code> or more unexpected things like that.</p>
<p>I think wipefs is meant to be used within a systern that currently
runs<br />
so it must operate carefully to not destroy the running system.</p>
<p>In contrast what we need during "rear recover" is some generic "brutal"
method<br />
that "just works" (as far as possible with reasonable effort) to wipe
the target disk.</p>
<p>If the recovery system and the backup is on a disk<br />
(e.g. via OUTPUT=USB plus BACKUP_URL=usb:///)<br />
we must carefully avoid to apply that generic "just works" method<br />
by accident also on the disk where the backup is.</p>
<p>So my main problem is:<br />
On the one hand I need a generic "brutal" method that "just works".<br />
On the other hand that method must be very careful and not wipe<br />
by accident things on wrong disks (cf. the above example where<br />
that PV /dev/sdb123 is on a wrong disk).<br />
I.e. I am looking for a "careful and brutal" method that "just works"
;-)</p>
<h4 id="olivero2_commented_at_2020-12-11_2343"><img src="https://avatars.githubusercontent.com/u/4660803?v=4" width="50"><a href="https://github.com/OliverO2">OliverO2</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-743483945">2020-12-11 23:43</a>:<a class="headerlink" href="#olivero2_commented_at_2020-12-11_2343" title="Permanent link">&para;</a></h4>
<p>I see. I have never tried to wipe LVM stuff, so I wonder why this is
really different. I've tried to play a bit with that stuff, so here are
my thoughts:</p>
<ul>
<li>
<p>I was wondering whether a limited amount of <code>dd</code> overwriting would
    be sufficient. For example, Btrfs can position an <a href="https://btrfs.wiki.kernel.org/index.php/On-disk_Format#Superblock">additional
    superblock</a>
    at a location slightly above 25 TB(!):</p>
<blockquote>
<p>The primary superblock is located at 0x1 0000 (6410 KiB). Mirror
copies of the superblock are located at physical addresses
0x400 0000 (6410 MiB) and 0x40 0000 0000 (25610 GiB), if these
locations are valid. Superblock copies are updated simultaneously.</p>
</blockquote>
<p>On the other hand:</p>
<blockquote>
<p>Note that btrfs only recognizes disks with a valid 0x1 0000
superblock; otherwise, there would be confusion with other
filesystems.</p>
</blockquote>
<p>So if there is no code making the kernel look for signatures at high
locations, all will be fine, but do we really know?</p>
</li>
<li>
<p>My experiences with wiping 'regular' disks have been positive so far
    as long as wipefs is used 'correctly':</p>
<ul>
<li>
<p><code>wipefs /dev/XXX</code> (without <code>-a</code>) always just prints the
    signatures it has found and does not actually wipe anything. I
    always use it for simulation like I would use other tools with a
    <code>--dry-run</code> option.</p>
</li>
<li>
<p><code>wipefs -a /dev/XXX</code> wipes just that device or partition
    specified. It does so reliably. That device or partition will
    not come back. But for a device, any partitions inside have not
    been wiped and will now exist as 'ghost' partitions. They will
    come back once a matching partition table is re-created.</p>
</li>
<li>
<p>To wipe a multi-partition disk, I always use
    <code>wipefs -a /dev/XXX?* /dev/XXX</code>, which will wipe partitions
    first, then the partition table. No ghosts anymore.</p>
</li>
<li>
<p>The <code>wipefs</code> <a href="http://manpages.ubuntu.com/manpages/bionic/en/man8/wipefs.8.html">manual
    page</a>
    says about <code>-f, --force</code>:</p>
<blockquote>
<p>Force erasure, even if the filesystem is mounted. This is
required in order to<br />
erase a partition-table signature on a block device.</p>
</blockquote>
<p>This seems to be wrong. I was able to successfully wipe a GPT
partition table on a block device without <code>--force</code> as long as
no partition was actually mounted.</p>
</li>
<li>
<p><code>wipefs</code> even correctly detects and wipes Btrfs signatures on a
    Btrfs RAID1 combination. I was able to unmount the Btrfs file
    system, then wipe one drive, then use <code>btrfs replace</code> to bring
    it back. And Btrfs is quite picky. It will scan for file systems
    regardless of any attempts to actually mount them. And it will
    group former RAID1 member disks if they happen to contain the
    same file system UUID (so one must be carful when cloning
    disks).</p>
</li>
</ul>
</li>
<li>
<p>Regarding LVM, this worked:</p>
<pre><code>root@test-u20-04-s:~/rear# lvs -v
  LV   VG        #Seg Attr       LSize   Maj Min KMaj KMin Pool Origin Data%  Meta%  Move Cpy%Sync Log Convert LV UUID                                LProfile
  db   vg_data      1 -wi-a----- 199.00g  -1  -1  253    1                                                     8lPIgd-l3RL-J3ve-dX0V-gMX8-sbcU-0SmtKi
  root vg_system    1 -wi-a----- 255.00g  -1  -1  253    0                                                     5cg23v-H5iU-gySJ-lj8x-sNOU-JT8D-W3hPsZ

root@test-u20-04-s:~/rear# vgs -v
  VG        Attr   Ext   #PV #LV #SN VSize    VFree    VG UUID                                VProfile
  vg_data   wz--n- 4.00m   1   1   0 &lt;200.00g 1020.00m rJFtmB-SKVi-9T4D-Fraz-7Xo7-rbmn-iQIUET
  vg_system wz--n- 4.00m   2   1   0  311.99g   56.99g seaOqS-AUiz-511n-GIav-Bgv9-RXht-3yZcKK

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME           MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE      LABEL      PARTLABEL        PARTUUID
sda              8:0    0   256G  0 disk
├─sda1           8:1    0   512M  0 part /boot/efi  vfat                                    90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2           8:2    0     1G  0 part            vfat        RESCUE SYS Rescue System    9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3           8:3    0 254.5G  0 part /          btrfs                                   1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb              8:16   0   256G  0 disk            LVM2_member
sdc              8:32   0   256G  0 disk
├─sdc1           8:33   0   200G  0 part            LVM2_member            Linux filesystem 47d3f897-a5d9-473e-982a-be7184df8b33
│ └─vg_data-db 253:1    0   199G  0 lvm
└─sdc2           8:34   0    56G  0 part            LVM2_member            Linux filesystem 31e597a5-9fa4-4dfd-91cf-8059de6e4641

root@test-u20-04-s:~/rear# lvremove /dev/vg_*/*
Do you really want to remove and DISCARD active logical volume vg_data/db? [y/n]: y
  Logical volume "db" successfully removed
Do you really want to remove and DISCARD active logical volume vg_system/root? [y/n]: y
  Logical volume "root" successfully removed

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE      LABEL      PARTLABEL        PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                                    90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat        RESCUE SYS Rescue System    9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                                   1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk            LVM2_member
sdc      8:32   0   256G  0 disk
├─sdc1   8:33   0   200G  0 part            LVM2_member            Linux filesystem 47d3f897-a5d9-473e-982a-be7184df8b33
└─sdc2   8:34   0    56G  0 part            LVM2_member            Linux filesystem 31e597a5-9fa4-4dfd-91cf-8059de6e4641

root@test-u20-04-s:~/rear# vgremove vg_data vg_system
  Volume group "vg_data" successfully removed
  Volume group "vg_system" successfully removed

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE      LABEL      PARTLABEL        PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                                    90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat        RESCUE SYS Rescue System    9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                                   1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk            LVM2_member
sdc      8:32   0   256G  0 disk
├─sdc1   8:33   0   200G  0 part            LVM2_member            Linux filesystem 47d3f897-a5d9-473e-982a-be7184df8b33
└─sdc2   8:34   0    56G  0 part            LVM2_member            Linux filesystem 31e597a5-9fa4-4dfd-91cf-8059de6e4641

root@test-u20-04-s:~/rear# pvremove /dev/sdb /dev/sdc?
  Labels on physical volume "/dev/sdb" successfully wiped.
  Labels on physical volume "/dev/sdc1" successfully wiped.
  Labels on physical volume "/dev/sdc2" successfully wiped.

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE LABEL      PARTLABEL        PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                               90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat   RESCUE SYS Rescue System    9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                              1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk
sdc      8:32   0   256G  0 disk
├─sdc1   8:33   0   200G  0 part                              Linux filesystem 47d3f897-a5d9-473e-982a-be7184df8b33
└─sdc2   8:34   0    56G  0 part                              Linux filesystem 31e597a5-9fa4-4dfd-91cf-8059de6e4641

root@test-u20-04-s:~/rear# wipefs -a /dev/sdb /dev/sdc?* /dev/sdc
/dev/sdc: 8 bytes were erased at offset 0x00000200 (gpt): 45 46 49 20 50 41 52 54
/dev/sdc: 8 bytes were erased at offset 0x3ffffffe00 (gpt): 45 46 49 20 50 41 52 54
/dev/sdc: 2 bytes were erased at offset 0x000001fe (PMBR): 55 aa
/dev/sdc: calling ioctl to re-read partition table: Success

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE LABEL      PARTLABEL     PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                            90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat   RESCUE SYS Rescue System 9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                           1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk
sdc      8:32   0   256G  0 disk
</code></pre>
</li>
<li>
<p>Regarding LUKS, you wrote in <code>README.wipe_disks</code>:</p>
<blockquote>
<p>then it is impossible to wipe nested block devices inside LUKS
volumes.</p>
</blockquote>
<p>But: There is no need to wipe anything inside an encrypted LUKS
container. Once the container is destroyed, there is no way that any
'ghost' structures inside might come back. Their data and metadata
was encrypted and cannot be decrypted ever once the LUKS container's
information is gone. So destructing a LUKS container leaves only
harmless garbled data behind with no need to wipe any further.
Rebuilding a new LUKS container on the same disk/partition will not
interpret any data which might have been stored there previously.</p>
<p>And <code>wipefs</code> erases a LUKS container properly:</p>
<pre><code>root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE LABEL      PARTLABEL     PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                            90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat   RESCUE SYS Rescue System 9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                           1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk
└─sdb1   8:17   0   256G  0 part                                            2ae405db-4889-45b1-8d45-8809bc842a11
sdc      8:32   0   256G  0 disk

root@test-u20-04-s:~/rear# cryptsetup -v --cipher aes-xts-plain64 --key-size 512 --hash sha256 --iter-time 2000 --use-random --verify-passphrase luksFormat /dev/sdb1

WARNING!
========
This will overwrite data on /dev/sdb1 irrevocably.

Are you sure? (Type uppercase yes): YES
Enter passphrase for /dev/sdb1:
Verify passphrase:
Key slot 0 created.
Command successful.

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE      LABEL      PARTLABEL     PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                                 90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat        RESCUE SYS Rescue System 9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                                1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk
└─sdb1   8:17   0   256G  0 part            crypto_LUKS                          2ae405db-4889-45b1-8d45-8809bc842a11
sdc      8:32   0   256G  0 disk

root@test-u20-04-s:~/rear# wipefs -a /dev/sdb?
/dev/sdb1: 6 bytes were erased at offset 0x00000000 (crypto_LUKS): 4c 55 4b 53 ba be
/dev/sdb1: 6 bytes were erased at offset 0x00004000 (crypto_LUKS): 53 4b 55 4c ba be

root@test-u20-04-s:~/rear# lsblk -I 8 -o +FSTYPE,LABEL,PARTLABEL,PARTUUID
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT FSTYPE LABEL      PARTLABEL     PARTUUID
sda      8:0    0   256G  0 disk
├─sda1   8:1    0   512M  0 part /boot/efi  vfat                            90ea8592-8662-4456-9c11-e27c08ebdcc4
├─sda2   8:2    0     1G  0 part            vfat   RESCUE SYS Rescue System 9aa84d1c-10aa-49f4-b6ae-12819208115e
└─sda3   8:3    0 254.5G  0 part /          btrfs                           1c8be966-ab4b-4afd-9dbd-114b68526e45
sdb      8:16   0   256G  0 disk
└─sdb1   8:17   0   256G  0 part                                            2ae405db-4889-45b1-8d45-8809bc842a11
sdc      8:32   0   256G  0 disk
</code></pre>
</li>
</ul>
<h4 id="gdha_commented_at_2020-12-12_1308"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-743753662">2020-12-12 13:08</a>:<a class="headerlink" href="#gdha_commented_at_2020-12-12_1308" title="Permanent link">&para;</a></h4>
<p>@OliverO2 one remark about vgremove - if you use <code>vgremove --force</code> then
it will remove all lvols within the VG and remove also the UUID of the
VG.</p>
<h4 id="gdha_commented_at_2020-12-12_1310"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-743753957">2020-12-12 13:10</a>:<a class="headerlink" href="#gdha_commented_at_2020-12-12_1310" title="Permanent link">&para;</a></h4>
<p>@jsmeix upcoming Thursday I have a change planned to test this wipe
script during a recovery which is currently missing in the rear-2.4
version of RHEL 7/8 and then I have a case towards RedHat.</p>
<h4 id="gdha_commented_at_2020-12-17_1319"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-747434614">2020-12-17 13:19</a>:<a class="headerlink" href="#gdha_commented_at_2020-12-17_1319" title="Permanent link">&para;</a></h4>
<p>@jsmeix As promised I give you an update of my tests:</p>
<ul>
<li>used your updated wipe scripts from this PR to the rear-2.4
    (/usr/share/rear/layout/recreate/default/120_confirm_wipedisk_disks.sh,
    /usr/share/rear/layout/recreate/default/150_wipe_disks.sh,
    /usr/share/rear/lib/layout-functions.sh and
    /usr/share/rear/layout/prepare/GNU/Linux/100_include_partition_code.sh)
    and created an ISO + backup</li>
<li>did a recover test (putty log:
    <a href="https://gist.github.com/gdha/0c8304dbbbaeebf51e74ad01bf8f2b63">https://gist.github.com/gdha/0c8304dbbbaeebf51e74ad01bf8f2b63</a>) -
    we see device sda gets wiped out, but then we are getting an error:</li>
</ul>
<!-- -->

<pre><code>+++ lvm vgcreate --physicalextentsize 4096k vg00 /dev/sda2 /dev/sde
  A volume group called vg00 already exists.
</code></pre>
<ul>
<li>When we edit script:
    <code>/usr/share/rear/layout/prepare/GNU/Linux/110_include_lvm_code.sh</code>
    and add line:</li>
</ul>
<!-- -->

<pre><code>lvm vgremove $vg --force --yes &gt;&amp;2 || true
</code></pre>
<p>at line 136 it works fine - see gist
<a href="https://gist.github.com/gdha/504ee70b3dda0867f1dfa7298f5604f3">https://gist.github.com/gdha/504ee70b3dda0867f1dfa7298f5604f3</a>
that contains the restore log file, the diskrestore.sh script and the
modification explanation details.</p>
<h4 id="github-actions_commented_at_2021-02-16_0201"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-779534251">2021-02-16 02:01</a>:<a class="headerlink" href="#github-actions_commented_at_2021-02-16_0201" title="Permanent link">&para;</a></h4>
<p>Stale pull request message</p>
<h4 id="jsmeix_commented_at_2021-02-23_1040"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-784105674">2021-02-23 10:40</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-02-23_1040" title="Permanent link">&para;</a></h4>
<p>When I find a bit more time for ReaR I will continue here</p>
<h4 id="jsmeix_commented_at_2021-03-16_1353"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-800276456">2021-03-16 13:53</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-16_1353" title="Permanent link">&para;</a></h4>
<p>With recent commits here there is <code>DISKS_TO_BE_WIPED='false'</code> in
default.conf<br />
so that by default no disk is wiped to avoid regressions until this
feature was more tested.</p>
<h4 id="jsmeix_commented_at_2021-03-16_1356"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-800278460">2021-03-16 13:56</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-16_1356" title="Permanent link">&para;</a></h4>
<p>@rear/contributors<br />
if there are no objections I would like to merge it tomorrow afternoon<br />
in its current incomplete state so that it can be tested by interested<br />
users who try out our current ReaR upstream master code<br />
via explicit <code>DISKS_TO_BE_WIPED=''</code> in etc/rear/local.conf</p>
<h4 id="jsmeix_commented_at_2021-03-17_1314"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-801070480">2021-03-17 13:14</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-17_1314" title="Permanent link">&para;</a></h4>
<p>Works sufficiently well for me for my recent tests today<br />
(i.e. works as the current code is intended to do)<br />
so I will merge it now.</p>
<h4 id="jsmeix_commented_at_2021-03-17_1352"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-801097878">2021-03-17 13:52</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-17_1352" title="Permanent link">&para;</a></h4>
<p>@OliverO2<br />
thank you for your valuable input in your comment above<br />
<a href="https://github.com/rear/rear/pull/2514#issuecomment-743483945">https://github.com/rear/rear/pull/2514#issuecomment-743483945</a></p>
<p>As time permits I will have a closer look how I could improve my current
code.</p>
<p>I think my current code is overcomplicated regarding LUKS encrypted
voulmes<br />
i.e. I think you are right that one does not need to care about what is
inside<br />
a LUKS encrypted voulme.</p>
<p>Regarding "safely and properly deconstruct/remove LVM things":</p>
<p>My primary concern is to ensure that never ever an automatism in ReaR<br />
may accidentally destroy things on disks that should not be touched by
ReaR,<br />
cf.
<a href="https://github.com/rear/rear/pull/2514#issuecomment-729009274">https://github.com/rear/rear/pull/2514#issuecomment-729009274</a></p>
<p>To avoid that basic storage objects like partitions get destroyed<br />
on disks that should not be touched by ReaR I set<br />
DISKS_TO_BE_WIPED to those disks where<br />
in diskrestore.sh the create_disk_label function is called<br />
(the create_disk_label function calls "parted -s $disk mklabel
$label")<br />
i.e. those disks where things will be completely recreated from scratch.</p>
<p>I was thinking I could do the same for LVM storage objects:<br />
Inspect diskrestore.sh and determine those LVs VGs and PVs<br />
that will be completely recreated from scratch.</p>
<p>As far as I see this is basically what<br />
<a href="https://github.com/rear/rear/pull/2564">https://github.com/rear/rear/pull/2564</a><br />
implements (but there only for VGs).</p>
<p>BUT:<br />
The LVs VGs and PVs in diskrestore.sh that will be completely recreated
from scratch<br />
don't need to be those that exist on an already used disk that is used
for recovery.</p>
<p>Only when the system should be recreated on the unchanged hardware<br />
of the original system (e.g. to recover from soft errors), then<br />
LVs VGs and PVs exist on the unchanged system disk(s)<br />
which match the LVs VGs and PVs in diskrestore.sh<br />
that will be completely recreated from scratch.</p>
<p>So removing the LVs VGs and PVs in diskrestore.sh<br />
that will be completely recreated from scratch can be done in any case<br />
and it helps to recreate a system on the unchanged hardware of the
original system<br />
but this is not a complete solution to get rid of existing LVM stuff<br />
on an arbitrary already used replacement disk.</p>
<p>On the other hand existing LVM stuff on an arbitrary already used
replacement disk<br />
should usually not get in the way when it is not the unchanged disk of
the original system<br />
because partitions are likely recreated at different locations compare
to where they<br />
have been before on the already used replacement disk so the existing
LVM stuff<br />
will not automatically re-appear because the LVM metadata will not
re-appear<br />
at the expected place in the recreated partitions.</p>
<p>So removing the LVs VGs and PVs in diskrestore.sh<br />
that will be completely recreated from scratch is likely a<br />
sufficiently complete way to get rid of existing LVM stuff in practice.</p>
<p>I will think a bit more about that...</p>
<h4 id="jsmeix_commented_at_2021-06-25_0611"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/pull/2514#issuecomment-868251058">2021-06-25 06:11</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-06-25_0611" title="Permanent link">&para;</a></h4>
<p>Currently we have in
usr/share/rear/layout/recreate/default/150_wipe_disks.sh</p>
<pre><code># Wipe RAID plus LVM plus LUKS metadata.
# To wipe RAID Superblocks it is sufficient to wipe 133 KiB at the beginning and at the end of the device.
# To wipe to wipe LVM metadata is should be sufficient to wipe 4 MiB at the beginning and at the end of the device.
# To wipe LUKS headers is should be sufficient to wipe 8 MiB at the beginning of the device.
# To wipe RAID superblocks plus LVM metadata plus LUKS headers it should be sufficient to
# wipe 8 MiB + 4 MiB + 1 MiB = 13 MiB at the beginning of the device and to
# wipe 4 MiB + 1 MiB = 5 MiB at the end of the device.
# To be future proof (perhaps LUKS may add a backup header at the end of the device)
# wiping 16 MiB at the beginning and at the end of the device should be sufficiently safe.
</code></pre>
<p>based on what I collected about size and location of metadata in<br />
usr/share/rear/layout/recreate/default/README.wipe_disks</p>
<p>Now I spotted by luck (more pecisely because of
<a href="https://github.com/rear/rear/issues/2638">https://github.com/rear/rear/issues/2638</a>)<br />
<a href="https://github.com/rear/rear/issues/540#issuecomment-71819883">https://github.com/rear/rear/issues/540#issuecomment-71819883</a><br />
that reads</p>
<pre><code>Regarding clean up DRBD I found in
https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_drbd_overview.html

DRBD uses the last 128 MB of the raw device for metadata
</code></pre>
<p>So wiping 16 MiB at the beginning and at the end of the device<br />
seems to be not sufficiently safe in case of DRBD.</p>
<p>To enhance usr/share/rear/layout/recreate/default/150_wipe_disks.sh<br />
to also wipe DRBD (plus RAID plus LVM plus LUKS) metadata<br />
wiping 128 MiB (at the beginning and) at the end of the device<br />
should be sufficiently safe.</p>
<hr />
<p>[Export of Github issue for
<a href="https://github.com/rear/rear">rear/rear</a>.]</p>
              
            </div>
          </div>

<footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2024 - CC0 1.0 Universal<br />Give <a href="https://github.com/rear/rear-user-guide/issues/new?title=issues/2020-11-11.2514.pr.merged.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
