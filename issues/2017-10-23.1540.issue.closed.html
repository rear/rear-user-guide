<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="Relax-and-Recover (ReaR) User Guide Documentation"/>
    <meta property="og:description" content="This is an umbrella documentation project for all Relax-and-Recover (ReaR) kind of documentation ans starting with a good User Guide."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://relax-and-recover.org/rear-user-guide/"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://relax-and-recover.org/rear-user-guide/img/rear_logo_50.png"/>
    <meta property="og:image:width" content="50"/>
    <meta property="og:image:height" content="50"/>
    
    <title>#1540 Issue closed: ReaR does not support RAID 1 mdraid Intel IMSM/RST based firmware RAID containers - Relax-and-Recover (ReaR) User Guide Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/rear.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "#1540 Issue closed: ReaR does not support RAID 1 mdraid Intel IMSM/RST based firmware RAID containers";
        var mkdocs_page_input_path = "issues/2017-10-23.1540.issue.closed.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', "366986045", "auto");
        ga('send', 'pageview');
      </script>
    
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Relax-and-Recover (ReaR) User Guide Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">BASICS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/introduction.html">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/history.html">Bit of History</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/getting-started.html">Getting started with ReaR</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/configuration.html">Basic configuration</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SCENARIOS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../scenarios/index.html">Scenarios Overview</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">DEVELOPMENT</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../development/github-pr.html">Make a pull request with GitHub</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/index.html">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear27.html">Release Notes ReaR 2.7</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear26.html">Release Notes ReaR 2.6</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/knownproblems.html">Known Problems and Workarounds</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ISSUES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Issues History</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">#1540 Issue closed: ReaR does not support RAID 1 mdraid Intel IMSM/RST based firmware RAID containers</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="1540_issue_closed_rear_does_not_support_raid_1_mdraid_intel_imsmrst_based_firmware_raid_containers"><a href="https://github.com/rear/rear/issues/1540">#1540 Issue</a> <code>closed</code>: ReaR does not support RAID 1 mdraid Intel IMSM/RST based firmware RAID containers<a class="headerlink" href="#1540_issue_closed_rear_does_not_support_raid_1_mdraid_intel_imsmrst_based_firmware_raid_containers" title="Permanent link">&para;</a></h1>
<p><strong>Labels</strong>: <code>enhancement</code>, <code>needs sponsorship</code>,
<code>special hardware or VM</code>, <code>no-issue-activity</code></p>
<h4 id="v-vidr_opened_issue_at_2017-10-23_0248"><img src="https://avatars.githubusercontent.com/u/32947803?v=4" width="50"><a href="https://github.com/v-vidr">v-vidr</a> opened issue at <a href="https://github.com/rear/rear/issues/1540">2017-10-23 02:48</a>:<a class="headerlink" href="#v-vidr_opened_issue_at_2017-10-23_0248" title="Permanent link">&para;</a></h4>
<h4 id="relax-and-recover_rear_issue_template">Relax-and-Recover (ReaR) Issue Template<a class="headerlink" href="#relax-and-recover_rear_issue_template" title="Permanent link">&para;</a></h4>
<p>Fill in the following items before submitting a new issue<br />
(quick response is not guaranteed with free support):</p>
<ul>
<li>
<p>rear version (/usr/sbin/rear -V):<br />
    1.17, 2.0, 2.2</p>
</li>
<li>
<p>OS version (cat /etc/rear/os.conf or lsb_release -a):<br />
    RHEL and SUSE SAP 12 SP1</p>
</li>
<li>
<p>rear configuration files (cat /etc/rear/site.conf or cat
    /etc/rear/local.conf):</p>
</li>
</ul>
<pre>
OUTPUT=ISO
BACKUP=NETFS
BACKUP_URL=cifs://1.1.1.1/backup
NETFS_KEEP_OLD_BACKUP_COPY=yes
EXCLUDE_VG=( vgHANA-data-HC2 vgHANA-data-HC3 vgHANA-log-HC2 vgHANA-log-HC3 vgHANA-shared-HC2 vgHANA-shared-HC3 )
BACKUP_PROG_EXCLUDE=("${BACKUP_PROG_EXCLUDE[@]}" '/media' '/var/tmp/*' '/var/crash' '/hana' '/usr/sap' '/proc')
BACKUP_OPTIONS="cred=/etc/rear/cifs,vers=2.0"
</pre>

<ul>
<li>
<p>Are you using legacy BIOS or UEFI boot?<br />
    UEFI</p>
</li>
<li>
<p>Brief description of the issue:<br />
    Restored failed with device or resource busy</p>
</li>
<li>
<p>Work-around, if any:<br />
    Earlier we received error invalid number of devices. I have modified
    diskrestore.sh and defined 2 devices.</p>
</li>
</ul>
<p>We have configured two Physical Hard drives with software Raid and raid
level is 1.</p>
<p>I am able successfully backed up data and rescue image. we tried to
restore the complete os using rescue image.</p>
<p>We received error invalid number of devices and I have changed to 2
devices in diskrestore.sh</p>
<p>later we received device or resource busy error.</p>
<p>All screenshots are attached<br />
<img alt="device
busy" src="https://user-images.githubusercontent.com/32947803/31870274-aa35765c-b7ca-11e7-93d6-9277a27c9f6c.PNG" /><br />
<img alt="invalid_logs" src="https://user-images.githubusercontent.com/32947803/31870275-aae18c62-b7ca-11e7-87bb-7464f1a70337.PNG" /><br />
<img alt="invalid_number_devices" src="https://user-images.githubusercontent.com/32947803/31870276-ab51c9be-b7ca-11e7-8e88-a6c529956bad.PNG" /></p>
<p>Is I am missing anything in my configuration or restore procedure.</p>
<h4 id="jsmeix_commented_at_2017-10-27_1005"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-339930549">2017-10-27 10:05</a>:<a class="headerlink" href="#jsmeix_commented_at_2017-10-27_1005" title="Permanent link">&para;</a></h4>
<p>@vsrdigi<br />
that nobody replied up to now indicates that your particular issue<br />
is a "new and somewhat inexplicable special case" where<br />
currently nobody (including myself) has a good idea<br />
what the root cause could be.</p>
<p>When during "rear recover" a "Device resource busy" error<br />
appears for a valid device node like "/dev/sda" that should<br />
not be already in use - but actually it is already in use -<br />
my blind guess is that you run "rear recover" not on<br />
a machine with a perfectly clean harddisk "/dev/sda"<br />
but on a machine where "/dev/sda" was already used before.</p>
<p>For example when you do "rear recover" on the same machine<br />
where you did "rear mkbackup" before, then "rear recover" runs<br />
on a machine where "/dev/sda" still contails all the data of the<br />
original system, in particular same partitioning data and possibly<br />
same "special data" of whatever higher-level block devices.</p>
<p>When you run "rear recover" with an already used harddisk,<br />
see for general information what issues that may cause<br />
<a href="https://github.com/rear/rear/issues/799">https://github.com/rear/rear/issues/799</a><br />
and also follow the links therein.</p>
<p>In general how to debug when "rear recover" fails<br />
see for some very basic information the section<br />
"Debugging issues with Relax-and-Recover" in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<p>Some general background information how things work<br />
that should help to better understand how one can<br />
debug an issue when "rear recover" fails:</p>
<p>In general how to debug when a particular ReaR script fails, see<br />
<a href="https://github.com/rear/rear/issues/1532#issuecomment-336383117">https://github.com/rear/rear/issues/1532#issuecomment-336383117</a><br />
i.e. add a "rear_shell" call directly before the command that fails<br />
(this even works inside the diskrestore.sh script, see below).</p>
<p>But when "rear recover" fails things are more complicated:</p>
<p>During "rear mkbackup" or "rear mkrescue" certain ReaR scripts<br />
create the disklayout.conf file and the ReaR recovery system<br />
where that system-specific disklayout.conf file gets included.</p>
<p>During "rear recover" certain ReaR scripts generate<br />
the system-specific diskrestore.sh script according<br />
to the data in the disklayout.conf file in the recovery system<br />
and finally that generated diskrestore.sh script is run<br />
which does the actual work.</p>
<p>Therefore when "rear recover" fails the root cause could be:</p>
<p>1.)<br />
In the scripts that run during "rear mkbackup" or "rear mkrescue"<br />
to create the disklayout.conf file when that file contains wrong data<br />
or when the scripts have insufficient code to create right data<br />
for this or that special case.</p>
<p>2.)<br />
In the scripts that run during "rear recover" that generate<br />
the diskrestore.sh script when there are wrong commands<br />
or when there is insufficient code in the diskrestore.sh script<br />
to recreate the disk layout for this or that special case.</p>
<h4 id="jsmeix_commented_at_2017-10-27_1021"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-339933779">2017-10-27 10:21</a>:<a class="headerlink" href="#jsmeix_commented_at_2017-10-27_1021" title="Permanent link">&para;</a></h4>
<p>Again only a blind guess:<br />
In particular regarding "disk is busy"<br />
I found "mdadm ... locks the disk" in<br />
layout/prepare/GNU/Linux/100_include_partition_code.sh</p>
<pre>
if grep -q md /proc/mdstat &>/dev/null; then
    mdadm --stop -s >&2 || echo "stop mdadm failed"
    # Prevent udev waking up mdadm later.
    # Reasoning: At least on RHEL6 when parted created a raid partition on disk,
    # udev (via /lib/udev/rules.d/65-md-incremental.rules) wakes up mdadm which locks the disk,
    # so further parted commands with the disk will fail since the disk is busy now.
    # The /lib/udev/rules.d/65-md-incremental.rules detects anaconda (the Red Hat installer),
    # and if it find itself running under anaconda, it will not run.
    # Accordingly also for other installers (in particular the ReaR installer)
    # this rule should not be there (and other Linux distros probably do not have it)
    # which means removing it is the right solution to make ReaR work also for RHEL6:
    if [ -e /lib/udev/rules.d/65-md-incremental.rules ] ; then
        rm -f /lib/udev/rules.d/65-md-incremental.rules || echo "rm 65-md-incremental.rules failed"
    fi
fi
</pre>

<h4 id="v-vidr_commented_at_2017-10-28_1251"><img src="https://avatars.githubusercontent.com/u/32947803?v=4" width="50"><a href="https://github.com/v-vidr">v-vidr</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-340188704">2017-10-28 12:51</a>:<a class="headerlink" href="#v-vidr_commented_at_2017-10-28_1251" title="Permanent link">&para;</a></h4>
<p>I tried below options:</p>
<p>Option 1:<br />
a. wipefs -a -f /dev/sad{a,b}</p>
<p>b. zero-superblock using mdadm</p>
<p>c. Performed recover and result is failed with /dev/sda device or
resource busy.</p>
<p>Option2:<br />
a. Reboot machine</p>
<p>b. wipefs -a -f /dev/sad{a,b}</p>
<p>c. zero-superblock using mdadm</p>
<ol>
<li>before starting recover performed step 1 and step 2</li>
</ol>
<p>c. Performed recover and result is failed with /dev/sda device or
resource busy.</p>
<p>Option3:</p>
<p>a. reboot machine</p>
<p>b. tried creating manual raid and result is failed with /dev/sda device
or resource busy.</p>
<h4 id="jsmeix_commented_at_2017-11-09_1137"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-343129121">2017-11-09 11:37</a>:<a class="headerlink" href="#jsmeix_commented_at_2017-11-09_1137" title="Permanent link">&para;</a></h4>
<p>Meanwhile I got some background information<br />
via a SUSE-internal issue that we have for this.</p>
<p>I try to explain (as far as I - as a RAID noob - understand it)<br />
what actually goes on here:</p>
<p>The used hardware has two SSD disks<br />
using Intel Rapid Storage Technology<br />
that is used as mdraid IMSM based containers.</p>
<p>The Intel Matrix Storage Manager (IMSM) has been<br />
superseded by the Intel Rapid Storage Technology (RST).<br />
It is a firmware based RAID located in the Intel chip set<br />
and mdadm controls that firmware.</p>
<p>See<br />
<a href="https://en.wikipedia.org/wiki/Intel_Matrix_RAID">https://en.wikipedia.org/wiki/Intel_Matrix_RAID</a><br />
which reads (excerpt)</p>
<pre>
Matrix RAID is a computer storage
technology marketed by Intel.
It is a firmware, rather than hardware
or software, RAID system.
</pre>

<p>so that it is not "software RAID" but "firmware RAID"<br />
that is called "hardware-assisted software RAID"<br />
or "hybrid model RAID" or even "fake RAID"<br />
cf. the section "Firmware- and driver-based" in<br />
<a href="https://en.wikipedia.org/wiki/RAID#Implementations">https://en.wikipedia.org/wiki/RAID#Implementations</a></p>
<p>Accordingly this issue is in the end the same as<br />
<a href="https://github.com/rear/rear/issues/1094">https://github.com/rear/rear/issues/1094</a><br />
and<br />
<a href="https://github.com/rear/rear/issues/1460">https://github.com/rear/rear/issues/1460</a></p>
<p>Currently ReaR has no support at all for that.</p>
<h4 id="jsmeix_commented_at_2017-11-10_0910"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-343418305">2017-11-10 09:10</a>:<a class="headerlink" href="#jsmeix_commented_at_2017-11-10_0910" title="Permanent link">&para;</a></h4>
<p>For example how one could check whether or not there is<br />
Intel Matrix RAID hardware (excerpts):</p>
<pre>
# lspci | grep RAID
00:1f.2 RAID bus controller: Intel Corporation C600/X79 series chipset SATA RAID Controller ...
</pre>

<p>and whether or not the Intel Matrix RAID hardware<br />
is actually used or usable for RAID (i.e. when more than<br />
one disk is attached to the Intel Matrix RAID bus controller):</p>
<pre>
# mdadm --detail-platform
...
Platform : Intel(R) Rapid Storage Technology enterprise
...
RAID Levels : raid0 raid1 raid10 raid5
...
I/O Controller : /sys/devices/pci0000:00/0000:00:1f.2 ...
Port0 : /dev/sda ...
Port1 : - no device attached -
Port2 : /dev/sdb ...
</pre>

<p>Cf.<br />
<a href="https://www.intel.com.au/content/dam/www/public/us/en/documents/white-papers/rst-linux-paper.pdf">https://www.intel.com.au/content/dam/www/public/us/en/documents/white-papers/rst-linux-paper.pdf</a><br />
and<br />
<a href="https://unix.stackexchange.com/questions/273819/how-do-i-rebuild-create-assemble-an-imsm-raid-0-array-from-disk-images-instead">https://unix.stackexchange.com/questions/273819/how-do-i-rebuild-create-assemble-an-imsm-raid-0-array-from-disk-images-instead</a></p>
<h4 id="jsmeix_commented_at_2018-04-05_1023"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-378889658">2018-04-05 10:23</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-04-05_1023" title="Permanent link">&para;</a></h4>
<p>An addedum only FYI about "AMD UEFI fake RAID":</p>
<p>Recently I noticed that it seems also<br />
AMD provides some kind of "BIOS fake RAID" which is<br />
probably better called "AMD UEFI fake RAID" in this case<br />
because I noticed in a SUSE internal mail (excerpts):</p>
<pre>
... a community person who has been trying with partial success
to get AMD BIOS "fake RAID" running under Ubuntu.
This RAID chip is included in several ASUS boards.

Here is the guy's experience:
https://kwikr.de/Howto_Windows_Ubuntu_AMD-RAID.html

The AMD driver package under
https://support.amd.com/en-us/download/chipset?os=Linux+x86_64
is pretty pathetic and supports Ubuntu only.
</pre>

<p>As far as I understand<br />
<a href="https://kwikr.de/Howto_Windows_Ubuntu_AMD-RAID.html">https://kwikr.de/Howto_Windows_Ubuntu_AMD-RAID.html</a><br />
and<br />
<a href="https://community.amd.com/message/2833733">https://community.amd.com/message/2833733</a><br />
on a first glance it seems such an<br />
"AMD UEFI fake RAID" basically works by</p>
<ul>
<li>enabling and creating an AMD-RAID in UEFI</li>
<li>using the AMD rcraid kernel module (also in initrd for booting)</li>
</ul>
<p>Accordingly I hope for "rear recover" on replacement hardware we could
assume<br />
that enabling AMD-RAID in UEFI and creating an AMD-RAID in UEFI<br />
is already done on the replacement hardware where "rear recover" runs<br />
(i.e. the replacement hardware must have been already prepared<br />
to use such an "AMD UEFI fake RAID").</p>
<p>Additionally I hope that the AMD rcraid kernel module (also in initrd
for booting)<br />
gets automatically inherited from the original system where it is loaded
so that<br />
the ReaR recovery system kernel also uses the AMD rcraid kernel module.</p>
<p>In particular it seems that with such an "AMD UEFI fake RAID"<br />
each filesystem gets installed into a single normal looking<br />
partition device node like /dev/sdb5, cf.<br />
<code>assume ... you installed Ubuntu on /dev/sdb5</code><br />
in
<a href="https://kwikr.de/Howto_Windows_Ubuntu_AMD-RAID.html">https://kwikr.de/Howto_Windows_Ubuntu_AMD-RAID.html</a><br />
so that (hopefully) nothing special needs to be done or set up by ReaR<br />
when such an "AMD UEFI fake RAID" is used.</p>
<h4 id="jsmeix_commented_at_2018-06-28_0729"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-400939598">2018-06-28 07:29</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-06-28_0729" title="Permanent link">&para;</a></h4>
<p>I need to get first and foremost a basic understanding about<br />
MD software RAID setup to be able to implement support for<br />
"RAID 1 mdraid Intel IMSM/RST based firmware RAID containers"<br />
in ReaR.</p>
<p>This precondition will be my SUSE Hack Week 17 project:<br />
<a href="https://hackweek.suse.com/17/projects/get-a-basic-understanding-about-md-software-raid-setup">https://hackweek.suse.com/17/projects/get-a-basic-understanding-about-md-software-raid-setup</a></p>
<h4 id="pcahyna_commented_at_2018-07-14_2339"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-405056622">2018-07-14 23:39</a>:<a class="headerlink" href="#pcahyna_commented_at_2018-07-14_2339" title="Permanent link">&para;</a></h4>
<p>I think it is not Intel IMSM/RST hardware specific really. You can use
the ddf container format for testing (<code>-e ddf</code>), it should behave in a
similar way to imsm and you don't need hardware support.<br />
(actually even IMSM containers can be created on hardware which does not
support them by setting <code>IMSM_NO_PLATFORM=1</code> in the environment. See the
mdadm man page.)<br />
FYI how I created the array with a single disk for testing:<br />
(you probably want to remove all partitions on /dev/sdb first)</p>
<pre><code>mdadm -C --force /dev/md/imsm /dev/sdb -n 1 -e imsm
mdadm -C /dev/md/vol0 --force --level=0 --raid-devices=1  /dev/md/imsm
</code></pre>
<p>After reboot the names changed to /dev/md/imsm0 and /dev/md/vol0_0.
Anyway, /dev/md/vol0_0 is a RAID device now, one can create a
filesystem on it and it will autoassemble after reboot.</p>
<pre><code>mdadm  --detail /dev/md/imsm0

/dev/md/imsm0:
           Version : imsm
        Raid Level : container
     Total Devices : 1

   Working Devices : 1


              UUID : e12cfa7f:fd172137:1cca35b0:a9fbb2ce
     Member Arrays : /dev/md/vol0_0

    Number   Major   Minor   RaidDevice

       -       8       16        -        /dev/sdb

mdadm  --detail /dev/md/vol0_0

/dev/md/vol0_0:
         Container : /dev/md/imsm0, member 0
        Raid Level : raid0
        Array Size : 78147584 (74.53 GiB 80.02 GB)
      Raid Devices : 1
     Total Devices : 1

             State : clean 
    Active Devices : 1
   Working Devices : 1
    Failed Devices : 0
     Spare Devices : 0

        Chunk Size : 128K

Consistency Policy : none


              UUID : 8f34580e:83d09b9e:6c835f5b:93ca4125
    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
</code></pre>
<p>this command has the easiest format for machine parsing and clearly
shows what devices are inside a container:</p>
<pre><code>mdadm --detail --scan --config=partitions

ARRAY /dev/md/imsm0 metadata=imsm UUID=e12cfa7f:fd172137:1cca35b0:a9fbb2ce
ARRAY /dev/md/vol0_0 container=/dev/md/imsm0 member=0 UUID=8f34580e:83d09b9e:6c835f5b:93ca4125
</code></pre>
<p>And you can replace <code>imsm</code> by <code>ddf</code> above for a hardware-independent
solution. (Although I have not gotten it to autoassemble this way.)</p>
<h4 id="jsmeix_commented_at_2018-07-16_0921"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-405190910">2018-07-16 09:21</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-07-16_0921" title="Permanent link">&para;</a></h4>
<p>@pcahyna<br />
WOW!<br />
Many thanks for your descriptive information.<br />
I will try it as soon as time permits.</p>
<p>It would help me so much if I could sufficiently well simulate that
stuff<br />
on QEMU/KVM virtual machines with two virtual harddisks for a "real"
RAID1<br />
because then I can keep the original system on one virtual machine<br />
and test "rear recover" on another same virtual machine<br />
which speeds up development (also because virtual machines<br />
that run on one same powerful host system are really fast<br />
because then all happens within the host's main memory).</p>
<h4 id="gdha_commented_at_2018-10-05_0914"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-427298795">2018-10-05 09:14</a>:<a class="headerlink" href="#gdha_commented_at_2018-10-05_0914" title="Permanent link">&para;</a></h4>
<p>@jsmeix is the Dedicated Priority Support label still required as the
case has been open for a long time? Personally I have the feeling this
is more a consultancy request than a support call. We cannot help this
user without his special HW...</p>
<h4 id="v-vidr_commented_at_2018-10-05_0929"><img src="https://avatars.githubusercontent.com/u/32947803?v=4" width="50"><a href="https://github.com/v-vidr">v-vidr</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-427302648">2018-10-05 09:29</a>:<a class="headerlink" href="#v-vidr_commented_at_2018-10-05_0929" title="Permanent link">&para;</a></h4>
<p>Hi,</p>
<p>If required we will arrange hardware for testing for remote session</p>
<h4 id="schlomo_commented_at_2018-10-05_0943"><img src="https://avatars.githubusercontent.com/u/101384?v=4" width="50"><a href="https://github.com/schlomo">schlomo</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-427306325">2018-10-05 09:43</a>:<a class="headerlink" href="#schlomo_commented_at_2018-10-05_0943" title="Permanent link">&para;</a></h4>
<p>@v-vidr I guess that this case is one where you should consider paid
ReaR consulting to either come on-site or remotely work on your hardware
to add support for this use case to ReaR.</p>
<p>Just out of curiosity: If this is in fact a software RAID, what exactly
is the benefit of the IMSM format over the regular mdadm formats? Is is
about BIOS/UEFI support for booting off the 2nd disk if the first disk
fails?</p>
<p>I am asking because it might be easier to use standard software RAID
instead. With UEFI you also don't need to mess with boot sectors any
more as we had to with BIOS.</p>
<h4 id="jsmeix_commented_at_2018-10-05_1236"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-427350885">2018-10-05 12:36</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-10-05_1236" title="Permanent link">&para;</a></h4>
<p>@gdha<br />
I had set the Dedicated Priority Support label because we at SUSE<br />
have an internal feature request to get that implemented in ReaR<br />
which is currently pending at certain levels of management<br />
evaluation and approval steps that need to be done as<br />
preconditions until someone will get the needed hardware<br />
so that it could be implemented.</p>
<p>@v-vidr<br />
I cannot imagine how remote hardware could be of real help<br />
to get such a feature implemented with reasonable effort<br />
because at least I need direct testing of "rear recover"<br />
on directly available hardware (where for me that "hardware"<br />
is usually a virtual machine where I also completely own its<br />
host system (i.e. I am root on the virtualization host) so that<br />
I can do directly whatever I need and want).</p>
<p>@schlomo<br />
since I learned about "firmware RAID" I was wondering<br />
what its actual benefit is compared to kernel software RAID,<br />
it particular because the Linux Raid wiki<br />
<a href="https://raid.wiki.kernel.org/index.php/Linux_Raid">https://raid.wiki.kernel.org/index.php/Linux_Raid</a><br />
reads (excerpt):</p>
<pre>
BIOS / firmware RAID aka fake raid cards:

* offer a few performance benefits (like CPU, bus and
  RAM offloading), but may often be much slower
  than SW raid (link?)

* if the 'raid' card or motherboard dies then you often
  have to find an exact replacement and this can be
  tricky for older cards

* if drives move to other machines the data can't easily
  be read

* there is usually no monitoring or reporting on the
  array - if a problem occurs then it may not show up
  unless the machine is rebooted *and* someone is
  actually watching the BIOS boot screen (or until
  multiple errors occur and your data is lost)

* you are entrusting your data to unpatchable software
  written into a BIOS that has probably not been tested,
  has no support mechanism and almost no community.

* having seen how many bugs the kernel works around
  in various BIOSes it would be optimistic to think that
  the BIOS RAID has no bugs. 

Given the point of RAID is usually to reduce risk
it is fair to say that using fakeraid is a terrible idea
and it's better to focus energy on either true HW raid
or in-kernel SW raid .... but there is nothing stopping you :)
</pre>

<p>@v-vidr<br />
only out of curiosity could you explain to us what the reason is<br />
(in particular what the actual benefit is in your particular use-case)<br />
why you use firmware RAID instead of kernel software RAID<br />
which would - by the way - also "just work" with current ReaR<br />
(at least when you use a usual kernel software RAID setup).</p>
<p>In general when someone likes to use ReaR for disaster recovery<br />
he must set up his original system so that ReaR can recreate it<br />
but not the other way round because that might only work by luck,<br />
cf. sections like "Disaster recovery does not just work" and<br />
"Let's face it: Deployment via the recovery installer is a must" and<br />
"The limitation is what the special ReaR recovery system can do" in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<h4 id="pcahyna_commented_at_2018-10-26_1839"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-433505219">2018-10-26 18:39</a>:<a class="headerlink" href="#pcahyna_commented_at_2018-10-26_1839" title="Permanent link">&para;</a></h4>
<p>@v-vidr can you provide hardware with remote access to console,
including BIOS setup (e.g. serial port)?</p>
<h4 id="jsmeix_commented_at_2018-11-21_1454"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-440691790">2018-11-21 14:54</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-11-21_1454" title="Permanent link">&para;</a></h4>
<p>For the fun of it:<br />
There is <code>IRST</code> as in <code>Intel Rapid Storage Technology</code> - e.g. see<br />
<a href="https://pcsupport.lenovo.com/de/en/products/laptops-and-netbooks/lenovo-v-series-laptops/v110-15isk/downloads/ds113245">https://pcsupport.lenovo.com/de/en/products/laptops-and-netbooks/lenovo-v-series-laptops/v110-15isk/downloads/ds113245</a></p>
<pre>
Intel Rapid Storage Technology (IRST) Driver ...
</pre>

<p>and <code>IRST</code> as in <code>Intel Rapid Start Technology</code> - e.g. see<br />
<a href="https://www.dell.com/support/article/de/de/debsdt1/sln265682/how-to-setup-intel-rapid-start-technology-in-the-uefi-mode?lang=en">https://www.dell.com/support/article/de/de/debsdt1/sln265682/how-to-setup-intel-rapid-start-technology-in-the-uefi-mode?lang=en</a></p>
<pre>
What is Intel Rapid Start Technology (IRST) ...
</pre>

<p>Perhaps the next one will be <code>IRST</code> as in
<code>Intel Rapid Something Technology</code><br />
or whatever you may imagine what the <code>S</code> therein could stand for... ;-)</p>
<h4 id="github-actions_commented_at_2020-07-01_0133"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/1540#issuecomment-652134793">2020-07-01 01:33</a>:<a class="headerlink" href="#github-actions_commented_at_2020-07-01_0133" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<hr />
<p>[Export of Github issue for
<a href="https://github.com/rear/rear">rear/rear</a>.]</p>
              
            </div>
          </div>

<footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2024 - CC0 1.0 Universal<br />Give <a href="https://github.com/rear/rear-user-guide/issues/new?title=issues/2017-10-23.1540.issue.closed.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
