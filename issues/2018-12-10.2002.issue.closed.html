<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="Relax-and-Recover (ReaR) User Guide Documentation"/>
    <meta property="og:description" content="This is an umbrella documentation project for all Relax-and-Recover (ReaR) kind of documentation ans starting with a good User Guide."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://relax-and-recover.org/rear-user-guide/"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://relax-and-recover.org/rear-user-guide/img/rear_logo_50.png"/>
    <meta property="og:image:width" content="50"/>
    <meta property="og:image:height" content="50"/>
    
    <title>#2002 Issue closed: SLES12SP3 on Power - Getting disk layout recreation failed error and disk mapping to /dev/sda. - Relax-and-Recover (ReaR) User Guide Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/rear.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "#2002 Issue closed: SLES12SP3 on Power - Getting disk layout recreation failed error and disk mapping to /dev/sda.";
        var mkdocs_page_input_path = "issues/2018-12-10.2002.issue.closed.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', "366986045", "auto");
        ga('send', 'pageview');
      </script>
    
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Relax-and-Recover (ReaR) User Guide Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">BASICS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/introduction.html">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/history.html">Bit of History</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/getting-started.html">Getting started with ReaR</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/configuration.html">Basic configuration</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SCENARIOS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../scenarios/index.html">Scenarios Overview</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">DEVELOPMENT</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../development/github-pr.html">Make a pull request with GitHub</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/index.html">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear27.html">Release Notes ReaR 2.7</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear26.html">Release Notes ReaR 2.6</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/knownproblems.html">Known Problems and Workarounds</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ISSUES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Issues History</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">#2002 Issue closed: SLES12SP3 on Power - Getting disk layout recreation failed error and disk mapping to /dev/sda.</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2002_issue_closed_sles12sp3_on_power_-_getting_disk_layout_recreation_failed_error_and_disk_mapping_to_devsda"><a href="https://github.com/rear/rear/issues/2002">#2002 Issue</a> <code>closed</code>: SLES12SP3 on Power - Getting disk layout recreation failed error and disk mapping to /dev/sda.<a class="headerlink" href="#2002_issue_closed_sles12sp3_on_power_-_getting_disk_layout_recreation_failed_error_and_disk_mapping_to_devsda" title="Permanent link">&para;</a></h1>
<p><strong>Labels</strong>: <code>support / question</code>, <code>fixed / solved / done</code></p>
<h4 id="ccjung_opened_issue_at_2018-12-10_1937"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> opened issue at <a href="https://github.com/rear/rear/issues/2002">2018-12-10 19:37</a>:<a class="headerlink" href="#ccjung_opened_issue_at_2018-12-10_1937" title="Permanent link">&para;</a></h4>
<h4 id="relax-and-recover_rear_issue_template">Relax-and-Recover (ReaR) Issue Template<a class="headerlink" href="#relax-and-recover_rear_issue_template" title="Permanent link">&para;</a></h4>
<p>Fill in the following items before submitting a new issue<br />
(quick response is not guaranteed with free support):</p>
<ul>
<li>
<p>ReaR version ("/usr/sbin/rear -V"): Relax-and-Recover
    2.4-git.0.6ec9075.unknown / 2018-12-05</p>
</li>
<li>
<p>OS version ("cat /etc/rear/os.conf" or "lsb_release -a" or "cat
    /etc/os-release"):</p>
</li>
</ul>
<!-- -->

<pre><code>rear&gt; cat /etc/rear/os.conf
OS_VENDOR=SUSE_LINUX
OS_VERSION=12.3
# The following information was added automatically by the mkbackup workflow:
ARCH='Linux-ppc64le'
OS='GNU/Linux'
OS_VERSION='12.3'
OS_VENDOR='SUSE_LINUX'
OS_VENDOR_VERSION='SUSE_LINUX/12.3'
OS_VENDOR_ARCH='SUSE_LINUX/ppc64le'
# End of what was added automatically by the mkbackup workflow.
</code></pre>
<ul>
<li>ReaR configuration files ("cat /etc/rear/site.conf" and/or "cat
    /etc/rear/local.conf"):</li>
</ul>
<!-- -->

<pre><code>rear&gt; cat /etc/rear/local.conf
# Default is to create Relax-and-Recover rescue media as ISO image
# set OUTPUT to change that
# set BACKUP to activate an automated (backup and) restore of your data
# Possible configuration values can be found in /usr/share/rear/conf/default.conf
#
# This file (local.conf) is intended for manual configuration. For configuration
# through packages and other automated means we recommend creating a new
# file named site.conf next to this file and to leave the local.conf as it is.
# Our packages will never ship with a site.conf.
AUTOEXCLUDE_MULTIPATH=n
BOOT_OVER_SAN=y
REAR_INITRD_COMPRESSION=lzma
OUTPUT=ISO
ISO_MAX_SIZE=4000
BACKUP=NETFS
BACKUP_URL=iso:///iso_fs/REAR_BACKUP
ISO_DIR=/iso_fs/REAR_ISO
TMPDIR=/iso_fs/REAR_TEMP
OUTPUT_URL=null
BOOT_FROM_SAN=y
EXCLUDE_MOUNTPOINTS=( /iso_fs /opt/IBM/ITM /opt/commvault /opt/splunkforwarder /opt/teamquest /var/opt/BESClient /usr/sap /hana/data /hana/log /hana/shared /usr/sap/basis /usr/sap/srm /PA_backup )
EXCLUDE_COMPONENTS=( /dev/mapper/20017380033e700f3 /dev/mapper/20017380033e700f4 /dev/mapper/20017380033e700f5 /dev/mapper/20017380033e700f6 /dev/mapper/20017380033e700f7 /dev/mapper/20017380033e700f8 /dev/mapper/20017380033e700f9 /dev/mapper/20017380033e700fa /dev/mapper/20017380033e700ff /dev/mapper/20017380033e70100 /dev/mapper/20017380033e70101 /dev/mapper/20017380033e70102 )
</code></pre>
<ul>
<li>
<p>Hardware (PC or PowerNV BareMetal or ARM) or virtual machine (KVM
    guest or PoverVM LPAR): PowerVM LPAR</p>
</li>
<li>
<p>System architecture (x86 compatible or PPC64/PPC64LE or what exact
    ARM device): PPC64LE</p>
</li>
<li>
<p>Firmware (BIOS or UEFI or Open Firmware) and bootloader (GRUB or
    ELILO or Petitboot): GRUB</p>
</li>
<li>
<p>Storage (lokal disk or SSD) and/or SAN (FC or iSCSI or FCoE) and/or
    multipath (DM or NVMe): SAN FC</p>
</li>
<li>
<p>Description of the issue (ideally so that others can reproduce it):
    Needed to boot multiple times to get the disk serial number for
    mapping and also get disk layout recreation script failed</p>
</li>
</ul>
<!-- -->

<pre><code>Comparing disks
Ambiguous possible target disks need manual configuration (more than one with same size found)
Switching to manual disk layout configuration
Using /dev/sda (same size) for recreating /dev/mapper/20017380033e700f2
Current disk mapping table (source =&gt; target):
  /dev/mapper/20017380033e700f2 =&gt; /dev/sda


Creating partitions for disk /dev/mapper/20017380033e7012f (msdos)
UserInput -I LAYOUT_CODE_RUN needed in /usr/share/rear/layout/recreate/default/200_run_layout_code.sh line 127
The disk layout recreation script failed
</code></pre>
<ul>
<li>
<p>Workaround, if any:</p>
</li>
<li>
<p>Attachments, as applicable ("rear -D mkrescue/mkbackup/recover"
    debug log files):</p>
</li>
</ul>
<p><a href="https://github.com/rear/rear/files/2664654/20181210_RearRestore.log.log">20181210_RearRestore.log.log</a></p>
<h4 id="jsmeix_commented_at_2018-12-11_0904"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446124563">2018-12-11 09:04</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-11_0904" title="Permanent link">&para;</a></h4>
<p>The<br />
<a href="https://github.com/rear/rear/files/2664654/20181210_RearRestore.log.log">https://github.com/rear/rear/files/2664654/20181210_RearRestore.log.log</a><br />
contains (excerpts):</p>
<pre><code>++ source /var/lib/rear/layout/diskrestore.sh
+++ LogPrint 'Start system layout restoration.'
...
+++ lvm vgchange -a n
  /run/lvm/lvmetad.socket: connect failed: No such file or directory
  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.
  Found duplicate PV emSwP2fCyavSeeBOCALUdOqYCw41Lc01: using /dev/mapper/SIBM_2810XIV_78033E7012F-part3 not /dev/mapper/20017380033e7012f-part3
  Using duplicate PV /dev/mapper/SIBM_2810XIV_78033E7012F-part3 which is last seen, replacing /dev/mapper/20017380033e7012f-part3
  Found duplicate PV emSwP2fCyavSeeBOCALUdOqYCw41Lc01: using /dev/mapper/SIBM_2810XIV_78033E7012F-part3 not /dev/sdd3
  Using duplicate PV /dev/mapper/SIBM_2810XIV_78033E7012F-part3 from subsystem DM, ignoring /dev/sdd3
...
+++ lvm vgchange -a n system_vg
  /run/lvm/lvmetad.socket: connect failed: No such file or directory
  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.
  Found duplicate PV emSwP2fCyavSeeBOCALUdOqYCw41Lc01: using /dev/mapper/SIBM_2810XIV_78033E7012F-part3 not /dev/mapper/20017380033e7012f-part3
  Using duplicate PV /dev/mapper/SIBM_2810XIV_78033E7012F-part3 which is last seen, replacing /dev/mapper/20017380033e7012f-part3
  Found duplicate PV emSwP2fCyavSeeBOCALUdOqYCw41Lc01: using /dev/mapper/SIBM_2810XIV_78033E7012F-part3 not /dev/sdd3
  Using duplicate PV /dev/mapper/SIBM_2810XIV_78033E7012F-part3 from subsystem DM, ignoring /dev/sdd3
  0 logical volume(s) in volume group "system_vg" now active
+++ lvm pvcreate -ff --yes -v --uuid emSwP2-fCya-vSee-BOCA-LUdO-qYCw-41Lc01 --norestorefile /dev/mapper/20017380033e7012f-part3
  /run/lvm/lvmetad.socket: connect failed: No such file or directory
  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.
  Found duplicate PV emSwP2fCyavSeeBOCALUdOqYCw41Lc01: using /dev/mapper/SIBM_2810XIV_78033E7012F-part3 not /dev/mapper/20017380033e7012f-part3
  Using duplicate PV /dev/mapper/SIBM_2810XIV_78033E7012F-part3 which is last seen, replacing /dev/mapper/20017380033e7012f-part3
  Found duplicate PV emSwP2fCyavSeeBOCALUdOqYCw41Lc01: using /dev/mapper/SIBM_2810XIV_78033E7012F-part3 not /dev/sdd3
  Using duplicate PV /dev/mapper/SIBM_2810XIV_78033E7012F-part3 from subsystem DM, ignoring /dev/sdd3
  uuid emSwP2-fCya-vSee-BOCA-LUdO-qYCw-41Lc01 already in use on "/dev/mapper/SIBM_2810XIV_78033E7012F-part3"
++ ((  1 == 0  ))
++ true
+++ UserInput -I LAYOUT_CODE_RUN -p 'The disk layout recreation script failed' ...
</code></pre>
<p>so that it seems the command</p>
<pre><code>lvm pvcreate -ff --yes -v --uuid emSwP2-fCya-vSee-BOCA-LUdO-qYCw-41Lc01 --norestorefile /dev/mapper/20017380033e7012f-part3
</code></pre>
<p>in diskrestore.sh exits with non-zero exit code because of</p>
<pre><code>uuid emSwP2-fCya-vSee-BOCA-LUdO-qYCw-41Lc01 already in use on "/dev/mapper/SIBM_2810XIV_78033E7012F-part3"
</code></pre>
<p>and that lets diskrestore.sh abort because it is run with <code>set -e</code>.</p>
<p>@schabrolles<br />
could you have a look what goes on here because I have no experience<br />
with possible complications because of "duplicates" in case of
multipath.</p>
<h4 id="jsmeix_commented_at_2018-12-11_0907"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446125589">2018-12-11 09:07</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-11_0907" title="Permanent link">&para;</a></h4>
<p>@suseusr168<br />
can you also attach your disklayout.conf and your diskrestore.sh that
fails,<br />
cf. "Debugging issues with Relax-and-Recover" at<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<h4 id="schabrolles_commented_at_2018-12-11_0924"><img src="https://avatars.githubusercontent.com/u/19491077?u=0021b16ab426902cbe676f6831f41607bbe4d441&v=4" width="50"><a href="https://github.com/schabrolles">schabrolles</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446130883">2018-12-11 09:24</a>:<a class="headerlink" href="#schabrolles_commented_at_2018-12-11_0924" title="Permanent link">&para;</a></h4>
<p>@suseusr168</p>
<p>It seems you have a problem with your multipath configuration.<br />
It seems you have one 112G device but multipath output shows 2</p>
<pre><code>create: 20017380033e7012f undef IBM,2810XIV
size=112G features='0' hwhandler='0' wp=undef
`-+- policy='service-time 0' prio=50 status=undef
  |- 2:0:0:1 sda 8:0  undef ready running
  `- 2:0:1:1 sdb 8:16 undef ready running
create: SIBM_2810XIV_78033E7012F undef IBM,2810XIV
size=112G features='0' hwhandler='0' wp=undef
`-+- policy='service-time 0' prio=50 status=undef
  `- 3:0:0:1 sdc 8:32 undef ready running
</code></pre>
<p>I think <code>20017380033e7012f</code> detected via <code>HBA 2</code> points to the same than
<code>SIBM_2810XIV_78033E7012F</code> detected by <code>HBA 3</code> (same 78033E7012F ID).</p>
<p>Could you check your SAN zoning configuration?<br />
send me the content of your <code>multipath.conf</code></p>
<h4 id="ccjung_commented_at_2018-12-11_1236"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446187565">2018-12-11 12:36</a>:<a class="headerlink" href="#ccjung_commented_at_2018-12-11_1236" title="Permanent link">&para;</a></h4>
<p>I use the find command starting from / and cannot find the
multipath.conf on the target system (eniesdbs107). I don't see much
files in /mnt/local.</p>
<p>Where is the multipath.conf file located on the target system?</p>
<p>This is the multipath.conf file on the source system (which was the
cloned from system):</p>
<pre><code>eniesdbs101:/ # cat /usr/lib/modules-load.d/multipath.conf
# Load device-handler and multipath module at boot
scsi_dh_alua
scsi_dh_emc
scsi_dh_rdac
dm_multipath
</code></pre>
<p>Everytime I boot up the LPAR, I get different destination disk names
such as</p>
<pre><code>/dev/sda
/dev/SIBM_2810XIV_78033E7012F
/dev/mapper/20017380033e7012f
</code></pre>
<p>I manually edited the /var/lib/rear/layout/disk_mappings with the disk
serial number<br />
<code>/dev/mapper/20017380033e7012f</code> and it fails on the disk layout
recreation script.</p>
<p>Attached is the diskrestore.sh and disklayout.conf. I added .log
extension in order to attach the files to the issue ticket.</p>
<p><a href="https://github.com/rear/rear/files/2667519/diskrestore.sh.log">diskrestore.sh.log</a><br />
<a href="https://github.com/rear/rear/files/2667521/disklayout.conf.log">disklayout.conf.log</a></p>
<h4 id="schabrolles_commented_at_2018-12-11_1352"><img src="https://avatars.githubusercontent.com/u/19491077?u=0021b16ab426902cbe676f6831f41607bbe4d441&v=4" width="50"><a href="https://github.com/schabrolles">schabrolles</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446209344">2018-12-11 13:52</a>:<a class="headerlink" href="#schabrolles_commented_at_2018-12-11_1352" title="Permanent link">&para;</a></h4>
<p>@suseusr168, As said previously, I think you have a problem with your
SAN configuration.</p>
<p>Do you try to restore the backup on the same system with the same Disks
or a different one ?</p>
<p>When device SIBM_XXXX is detected, this usually tells there is a
problem.<br />
You should have this:</p>
<pre><code>multipath -ll
create: 20017380033e7012f undef IBM,2810XIV
size=112G features='0' hwhandler='0' wp=undef
`-+- policy='service-time 0' prio=50 status=undef
  |- 2:0:0:1 sda 8:0  undef ready running
  |- 2:0:1:1 sdb 8:16 undef ready running
  `- 3:0:0:1 sdc 8:32 undef ready running
</code></pre>
<p>I suspect a SAN zoning configuration problem or something like that:</p>
<ul>
<li>Are the 2 FC port connected to the same SAN zone ?</li>
<li>How many XIV controller module do you have ?</li>
</ul>
<h4 id="jsmeix_commented_at_2018-12-11_1424"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446219972">2018-12-11 14:24</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-11_1424" title="Permanent link">&para;</a></h4>
<p>@schabrolles<br />
only out of curiosity I have a question regarding your above<br />
<a href="https://github.com/rear/rear/issues/2002#issuecomment-446130883">https://github.com/rear/rear/issues/2002#issuecomment-446130883</a><br />
where you wrote "same 78033E7012F ID":<br />
I see <code>...7380033e7012f</code> and <code>...78033E7012F</code> where the trailing
<code>033e7012f</code> matches.<br />
Is that trailing <code>033e7012f</code> what you mean with "same 78033E7012F ID"?<br />
Sorry if it is a stupid question - I am a total multipath noob.</p>
<h4 id="ccjung_commented_at_2018-12-11_1438"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446224626">2018-12-11 14:38</a>:<a class="headerlink" href="#ccjung_commented_at_2018-12-11_1438" title="Permanent link">&para;</a></h4>
<p>Yes, it is the same trailing 033e7012f. I must have copied the wrong
string with uppercase.</p>
<p>This same backup was restored to 4 out 5 differentLPARs with no
problems. We haven't tried restoring to itself.</p>
<p>Lucky me, I got the one that doesn't work.</p>
<p>I'll ask the SAN team to check the zone configuration. I'll update the
ticket with what I find.</p>
<p>Thanks everyone for your help.</p>
<h4 id="schabrolles_commented_at_2018-12-11_1521"><img src="https://avatars.githubusercontent.com/u/19491077?u=0021b16ab426902cbe676f6831f41607bbe4d441&v=4" width="50"><a href="https://github.com/schabrolles">schabrolles</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446240420">2018-12-11 15:21</a>:<a class="headerlink" href="#schabrolles_commented_at_2018-12-11_1521" title="Permanent link">&para;</a></h4>
<p>@jsmeix, you are right, there is the same END (which represent the LUNID
part on the storage).<br />
What I don't understand is the slight difference we can observe before
(<strong>7380</strong>033e7012f vs <strong>78</strong>033E7012F).<br />
It is like the disk presented has a different name depending on the FC
card or the Storage controller we use to get access to our remote
storage (which is not normal).</p>
<p>@suseusr168, could you also send us the output of<br />
<code>multipath -ll</code><br />
<code>udevadm info -n sda</code><br />
<code>udevadm info -n sdb</code><br />
<code>udevadm info -n sdc</code></p>
<h4 id="ccjung_commented_at_2018-12-11_1604"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446257262">2018-12-11 16:04</a>:<a class="headerlink" href="#ccjung_commented_at_2018-12-11_1604" title="Permanent link">&para;</a></h4>
<p>Here are the output requested:</p>
<pre><code>RESCUE eniesdbs101:~ # multipath -ll
SIBM_2810XIV_78033E7012F dm-0 IBM,2810XIV
size=112G features='2 queue_if_no_path retain_attached_hw_handler' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  `- 3:0:1:1 sdd 8:48 active ready running

RESCUE eniesdbs101:~ # multipath -ll
SIBM_2810XIV_78033E7012F dm-0 IBM,2810XIV
size=112G features='2 queue_if_no_path retain_attached_hw_handler' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  `- 3:0:1:1 sdd 8:48 active ready running

RESCUE eniesdbs101:~ # udevadm info -n sda
P: /devices/vio/3000000c/host2/rport-2:0-0/target2:0:0/2:0:0:1/block/sda
N: sda
S: disk/by-id/scsi-20017380033e7012f
S: disk/by-path/fc-0x5001738033e70140-lun-1
E: COMPAT_SYMLINK_GENERATION=1
E: DEVLINKS=/dev/disk/by-path/fc-0x5001738033e70140-lun-1 /dev/disk/by-id/scsi-20017380033e7012f
E: DEVNAME=/dev/sda
E: DEVPATH=/devices/vio/3000000c/host2/rport-2:0-0/target2:0:0/2:0:0:1/block/sda
E: DEVTYPE=disk
E: ID_BUS=scsi
E: ID_MODEL=2810XIV
E: ID_MODEL_ENC=2810XIV\x20\x20\x20\x20\x20\x20\x20\x20\x20
E: ID_PART_TABLE_TYPE=dos
E: ID_PART_TABLE_UUID=0005141b
E: ID_PATH=fc-0x5001738033e70140-lun-1
E: ID_PATH_TAG=fc-0x5001738033e70140-lun-1
E: ID_REVISION=0000
E: ID_SCSI=1
E: ID_SCSI_SERIAL=78033E7012F
E: ID_SERIAL=20017380033e7012f
E: ID_SERIAL_SHORT=0017380033e7012f
E: ID_TARGET_PORT=0
E: ID_TYPE=disk
E: ID_VENDOR=IBM
E: ID_VENDOR_ENC=IBM\x20\x20\x20\x20\x20
E: MAJOR=8
E: MINOR=0
E: MPATH_SBIN_PATH=/sbin
E: SUBSYSTEM=block
E: TAGS=:systemd:
E: USEC_INITIALIZED=14505103

RESCUE eniesdbs101:~ # udevadm info -n sdb
P: /devices/vio/3000000c/host2/rport-2:0-1/target2:0:1/2:0:1:1/block/sdb
N: sdb
S: disk/by-id/scsi-20017380033e7012f
S: disk/by-path/fc-0x5001738033e70171-lun-1
E: COMPAT_SYMLINK_GENERATION=1
E: DEVLINKS=/dev/disk/by-id/scsi-20017380033e7012f /dev/disk/by-path/fc-0x5001738033e70171-lun-1
E: DEVNAME=/dev/sdb
E: DEVPATH=/devices/vio/3000000c/host2/rport-2:0-1/target2:0:1/2:0:1:1/block/sdb
E: DEVTYPE=disk
E: ID_BUS=scsi
E: ID_MODEL=2810XIV
E: ID_MODEL_ENC=2810XIV\x20\x20\x20\x20\x20\x20\x20\x20\x20
E: ID_PART_TABLE_TYPE=dos
E: ID_PART_TABLE_UUID=0005141b
E: ID_PATH=fc-0x5001738033e70171-lun-1
E: ID_PATH_TAG=fc-0x5001738033e70171-lun-1
E: ID_REVISION=0000
E: ID_SCSI=1
E: ID_SCSI_SERIAL=78033E7012F
E: ID_SERIAL=20017380033e7012f
E: ID_SERIAL_SHORT=0017380033e7012f
E: ID_TARGET_PORT=0
E: ID_TYPE=disk
E: ID_VENDOR=IBM
E: ID_VENDOR_ENC=IBM\x20\x20\x20\x20\x20
E: MAJOR=8
E: MINOR=16
E: MPATH_SBIN_PATH=/sbin
E: SUBSYSTEM=block
E: TAGS=:systemd:
E: USEC_INITIALIZED=14503334

RESCUE eniesdbs101:~ # udevadm info -n sdc
P: /devices/vio/3000000d/host3/rport-3:0-0/target3:0:0/3:0:0:1/block/sdc
N: sdc
S: disk/by-path/fc-0x5001738033e70182-lun-1
E: COMPAT_SYMLINK_GENERATION=1
E: DEVLINKS=/dev/disk/by-path/fc-0x5001738033e70182-lun-1
E: DEVNAME=/dev/sdc
E: DEVPATH=/devices/vio/3000000d/host3/rport-3:0-0/target3:0:0/3:0:0:1/block/sdc
E: DEVTYPE=disk
E: ID_PART_TABLE_TYPE=dos
E: ID_PART_TABLE_UUID=0005141b
E: ID_PATH=fc-0x5001738033e70182-lun-1
E: ID_PATH_TAG=fc-0x5001738033e70182-lun-1
E: MAJOR=8
E: MINOR=32
E: MPATH_SBIN_PATH=/sbin
E: SUBSYSTEM=block
E: TAGS=:systemd:
E: USEC_INITIALIZED=14511717
</code></pre>
<h4 id="ccjung_commented_at_2018-12-11_1605"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446257630">2018-12-11 16:05</a>:<a class="headerlink" href="#ccjung_commented_at_2018-12-11_1605" title="Permanent link">&para;</a></h4>
<p>What should I ask the SAN team to check from their side?</p>
<h4 id="ccjung_commented_at_2018-12-11_2006"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446343401">2018-12-11 20:06</a>:<a class="headerlink" href="#ccjung_commented_at_2018-12-11_2006" title="Permanent link">&para;</a></h4>
<p>We found the problem. Thank you for directing us in the right direction
to look at the SAN.</p>
<p>Here is the info from the SAN admin:<br />
Looking at eniesdbs107.... for the SAN zoning, everything seemed
proper.... at first glance.</p>
<p>The FC's (C12/C13) show the lone XIV disk (100GB rootgv) as a single FC
device being delivered thru 4 distinct disk paths.</p>
<p>However - I see the same XIV I/O port - 5001738033e70140 - being used as
one of the 2 zoned ports for both:</p>
<p>Fcs0. slot C12 - fab A -OSHost - WWPN = c0507609f1840030</p>
<p>Fcs3. slot C15 - fab A - Host - WWPN = c0507609f1840036</p>
<p>They were specifically requested to be unique.. and this appears to be
causing us an issue with our restore - we need this rectified, the
sooner the better.</p>
<p>It appears all the aliases were using the same WWPN so the SAN team will
fix the zoning.</p>
<h4 id="schabrolles_commented_at_2018-12-12_0926"><img src="https://avatars.githubusercontent.com/u/19491077?u=0021b16ab426902cbe676f6831f41607bbe4d441&v=4" width="50"><a href="https://github.com/schabrolles">schabrolles</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446519714">2018-12-12 09:26</a>:<a class="headerlink" href="#schabrolles_commented_at_2018-12-12_0926" title="Permanent link">&para;</a></h4>
<p>@suseusr168, thanks for the feedback.<br />
Could you please tell us when everything is OK to close this issue.</p>
<h4 id="ccjung_commented_at_2018-12-12_1333"><img src="https://avatars.githubusercontent.com/u/29214472?u=58ae98d864664297a19702de40bf4767338e396b&v=4" width="50"><a href="https://github.com/ccjung">ccjung</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446589251">2018-12-12 13:33</a>:<a class="headerlink" href="#ccjung_commented_at_2018-12-12_1333" title="Permanent link">&para;</a></h4>
<p>@schabrolles ,<br />
I was able to restore the LPAR now after the SAN team fixed the zoning.</p>
<p>One question, we noticed sometimes the target disk mapping is /dev/sda
when we first boot up the LPAR for restore. If we reboot the LPAR again
to start a restore, we see the /dev/mapper/<disk serial number>.</p>
<p>Is there a way to fix this or we just need to reboot till we see the
disk mapping to /dev/mapper/<disk serial number> (which we have been
doing to work around this /dev/sda mapping)?</p>
<p>You can close out this issue.</p>
<p>Thanks again for your help.</p>
<h4 id="schabrolles_commented_at_2018-12-12_1549"><img src="https://avatars.githubusercontent.com/u/19491077?u=0021b16ab426902cbe676f6831f41607bbe4d441&v=4" width="50"><a href="https://github.com/schabrolles">schabrolles</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446635663">2018-12-12 15:49</a>:<a class="headerlink" href="#schabrolles_commented_at_2018-12-12_1549" title="Permanent link">&para;</a></h4>
<p>@suseusr168,</p>
<p>If it proposes <code>/dev/sda</code> instead of <code>/dev/mapper</code> (which should not
happen), you can interrupt rear restore and run <code>multipath -r</code> to reload
multipath.</p>
<h4 id="jsmeix_commented_at_2018-12-12_1618"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446646990">2018-12-12 16:18</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-12_1618" title="Permanent link">&para;</a></h4>
<p>@schabrolles<br />
thank you for solving this issue!</p>
<h4 id="jsmeix_commented_at_2018-12-13_0843"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2002#issuecomment-446886976">2018-12-13 08:43</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-13_0843" title="Permanent link">&para;</a></h4>
<p>Again this issue is one where using ReaR revealed<br />
that there was something wrong with the original system<br />
which shows an interesting "by the way feature" of ReaR:</p>
<p>Simply put:<br />
Run "rear mkbackup" on your original system<br />
and "rear recover" on your replacement hardware<br />
to verify that your original system is basically o.k.<br />
(and redo that after each change of your basic system).</p>
<p>Cf.<br />
<a href="https://github.com/rear/rear/issues/1907#issuecomment-434218293">https://github.com/rear/rear/issues/1907#issuecomment-434218293</a><br />
that actually belongs to
<a href="https://github.com/rear/rear/issues/1927">https://github.com/rear/rear/issues/1927</a></p>
<p>See also<br />
<a href="https://github.com/rear/rear/issues/1998#issuecomment-445188468">https://github.com/rear/rear/issues/1998#issuecomment-445188468</a></p>
<p>For another issue where using ReaR revealed two problems<br />
on the original system see for the first problem<br />
(the "Storix" software had installed a bad udev rule)<br />
<a href="https://github.com/rear/rear/issues/1796#issuecomment-386996844">https://github.com/rear/rear/issues/1796#issuecomment-386996844</a><br />
<a href="https://github.com/rear/rear/issues/1796#issuecomment-387461461">https://github.com/rear/rear/issues/1796#issuecomment-387461461</a><br />
<a href="https://github.com/rear/rear/issues/1796#issuecomment-387695097">https://github.com/rear/rear/issues/1796#issuecomment-387695097</a><br />
and for the second problem<br />
(non-working btrfs structure for SUSE snapper usage)<br />
<a href="https://github.com/rear/rear/issues/1796#issuecomment-388756113">https://github.com/rear/rear/issues/1796#issuecomment-388756113</a><br />
<a href="https://github.com/rear/rear/issues/1796#issuecomment-391288035">https://github.com/rear/rear/issues/1796#issuecomment-391288035</a></p>
<p>I think in the end examples like the above ones prove that in practice<br />
"Deployment via the recovery installer is a must" as described in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<p>I think I will document that "by the way feature" of ReaR more
explicitly in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<hr />
<p>[Export of Github issue for
<a href="https://github.com/rear/rear">rear/rear</a>.]</p>
              
            </div>
          </div>

<footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2024 - CC0 1.0 Universal<br />Give <a href="https://github.com/rear/rear-user-guide/issues/new?title=issues/2018-12-10.2002.issue.closed.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
