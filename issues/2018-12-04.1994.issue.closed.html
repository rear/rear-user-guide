<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="Relax-and-Recover (ReaR) User Guide Documentation"/>
    <meta property="og:description" content="This is an umbrella documentation project for all Relax-and-Recover (ReaR) kind of documentation ans starting with a good User Guide."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://relax-and-recover.org/rear-user-guide/"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://relax-and-recover.org/rear-user-guide/img/rear_logo_50.png"/>
    <meta property="og:image:width" content="50"/>
    <meta property="og:image:height" content="50"/>
    
    <title>#1994 Issue closed: Recovery from GRUB menu with Borg failed due to full disk in certain VM environment - Relax-and-Recover (ReaR) User Guide Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/rear.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "#1994 Issue closed: Recovery from GRUB menu with Borg failed due to full disk in certain VM environment";
        var mkdocs_page_input_path = "issues/2018-12-04.1994.issue.closed.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', "366986045", "auto");
        ga('send', 'pageview');
      </script>
    
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Relax-and-Recover (ReaR) User Guide Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">BASICS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/introduction.html">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/history.html">Bit of History</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/getting-started.html">Getting started with ReaR</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/configuration.html">Basic configuration</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SCENARIOS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../scenarios/index.html">Scenarios Overview</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">DEVELOPMENT</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../development/github-pr.html">Make a pull request with GitHub</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/index.html">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear27.html">Release Notes ReaR 2.7</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear26.html">Release Notes ReaR 2.6</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/knownproblems.html">Known Problems and Workarounds</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ISSUES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Issues History</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">#1994 Issue closed: Recovery from GRUB menu with Borg failed due to full disk in certain VM environment</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="1994_issue_closed_recovery_from_grub_menu_with_borg_failed_due_to_full_disk_in_certain_vm_environment"><a href="https://github.com/rear/rear/issues/1994">#1994 Issue</a> <code>closed</code>: Recovery from GRUB menu with Borg failed due to full disk in certain VM environment<a class="headerlink" href="#1994_issue_closed_recovery_from_grub_menu_with_borg_failed_due_to_full_disk_in_certain_vm_environment" title="Permanent link">&para;</a></h1>
<p><strong>Labels</strong>: <code>support / question</code>, <code>fixed / solved / done</code></p>
<h4 id="signum_opened_issue_at_2018-12-04_1438"><img src="https://avatars.githubusercontent.com/u/36562?v=4" width="50"><a href="https://github.com/Signum">Signum</a> opened issue at <a href="https://github.com/rear/rear/issues/1994">2018-12-04 14:38</a>:<a class="headerlink" href="#signum_opened_issue_at_2018-12-04_1438" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p>ReaR version ("/usr/sbin/rear -V"): Relax-and-Recover 2.3 / Git</p>
</li>
<li>
<p>OS version ("cat /etc/rear/os.conf" or "lsb_release -a" or "cat
    /etc/os-release"): Debian 9</p>
</li>
<li>
<p>ReaR configuration files ("cat /etc/rear/site.conf" and/or "cat
    /etc/rear/local.conf"):</p>
<p>OUTPUT=ISO<br />
OUTPUT_URL="nfs://nas/share/mnt/rear"<br />
GRUB_RESCUE=y<br />
SSH_UNPROTECTED_PRIVATE_KEYS=yes</p>
<p>BACKUP=BORG<br />
BORGBACKUP_HOST="nas"<br />
BORGBACKUP_USERNAME="rear"<br />
BORGBACKUP_REPO="/share/rear/ansible-test"</p>
<p>export BORG_PASSPHRASE="foobar"<br />
COPY_AS_IS_BORG=( "/root/.ssh/id_rsa" "/root/rear" )</p>
<p>export BORG_RELOCATED_REPO_ACCESS_IS_OK=yes<br />
export BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK=yes</p>
<p>export PRE_BACKUP_SCRIPT=/root/rear/pre_backup<br />
export POST_BACKUP_SCRIPT=/root/rear/post_backup<br />
export POST_RECOVERY_SCRIPT=/root/rear/post_recovery</p>
<p>BACKUP_PROG_INCLUDE=( "${BACKUP_PROG_INCLUDE[@]}" '/dbsnap/*'
)</p>
<p>USE_SERIAL_CONSOLE=y</p>
<p>BORGBACKUP_PRUNE_WEEKLY=2<br />
export BACKUP_PROG_EXCLUDE=( "/var/lib/postgresql" )</p>
</li>
<li>
<p>Hardware (PC or PowerNV BareMetal or ARM) or virtual machine (KVM
    guest or PoverVM LPAR):</p>
</li>
</ul>
<p>Virtualbox VM with Debian 9 started by Vagrant and deployed by
Ansible.<br />
Used for staging of REAR configuration.</p>
<ul>
<li>System architecture (x86 compatible or PPC64/PPC64LE or what exact
    ARM device):</li>
</ul>
<p>AMD64</p>
<ul>
<li>Firmware (BIOS or UEFI or Open Firmware) and bootloader (GRUB or
    ELILO or Petitboot):</li>
</ul>
<p>VirtualBox BIOS<br />
GRUB</p>
<ul>
<li>Storage (lokal disk or SSD) and/or SAN (FC or iSCSI or FCoE) and/or
    multipath (DM or NVMe):</li>
</ul>
<p>Emulated disk via VirtualBox.</p>
<ul>
<li>Description of the issue (ideally so that others can reproduce it):</li>
</ul>
<p>I deployed a fresh Debian 9 system from a Vagrant box. I installed the
standalone "borg" binary<br />
and installed REAR.</p>
<p>When booting the VM I get "Relax and Recover" as third item in the GRUB
boot menu. I can choose that and boot the system. Recovery will start to
partition the disks correctly. Then "borg" is called and quits
immediately with messages like "Failed to write all bytes for
_bisect.so". Apparently that is caused by the root disk being full 100%
so borg fails to run.</p>
<p>However when I create a VirtualBox VM manually then there is space left
on "/" and recovering from the GRUB menu entry works as expected.</p>
<p>Booting from the ISO image works every time, too.</p>
<ul>
<li>Workaround, if any:</li>
</ul>
<p>Boot from ISO.</p>
<h4 id="jsmeix_commented_at_2018-12-04_1459"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444129804">2018-12-04 14:59</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-04_1459" title="Permanent link">&para;</a></h4>
<p>@gozora<br />
I assign it to you because it mentiones booting and Borg.</p>
<p>@Signum<br />
I know nothing at all about<br />
<code>Virtualbox VM with Debian 9 started by Vagrant and deployed by Ansible</code>.<br />
Can't you determine the available disks in that <code>Virtualbox VM</code><br />
or explain what <code>the root disk being full</code> actually means?</p>
<p>Also <code>there is space left on "/"</code> is not really meaningful for me<br />
because within the running ReaR recovery system what is<br />
mounted at '/' is only the ramdisk of the ReaR recovery system.<br />
The actual system disk i.e. the target system disk (e.g. /dev/sda/)<br />
is not mounted at '/' within the running ReaR recovery system.<br />
The target system disk (e.g. /dev/sda/) gets partitioned and<br />
filesystems get created and those filesystems get mounted<br />
at /mnt/local within the ReaR recovery system while <code>rear recover</code><br />
runs according to the data in disklayout.conf.</p>
<p>See also in<br />
<a href="https://github.com/rear/rear/blob/master/.github/ISSUE_TEMPLATE.md">https://github.com/rear/rear/blob/master/.github/ISSUE_TEMPLATE.md</a></p>
<pre>
Attachments, as applicable ("rear -D mkrescue/mkbackup/recover" debug log files):
</pre>

<p>and "Debugging issues with Relax-and-Recover" in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<h4 id="signum_commented_at_2018-12-04_1511"><img src="https://avatars.githubusercontent.com/u/36562?v=4" width="50"><a href="https://github.com/Signum">Signum</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444134477">2018-12-04 15:11</a>:<a class="headerlink" href="#signum_commented_at_2018-12-04_1511" title="Permanent link">&para;</a></h4>
<p>@jsmeix<br />
Sorry for the sparse information. It was not easy to get log files out
of the broken VM at that time.<br />
Would it be okay if I prepared a simple Git project with the necessary
files to run "Vagrant" in order to reproduce this issue? Or is this
asked too much? I would totally understand that.</p>
<p>Just as an enhancement idea: should REAR check if the recovery binary
(e.g. "borg") is available and can run before destroying the physical
disks and repartitioning them?</p>
<h4 id="signum_commented_at_2018-12-04_1513"><img src="https://avatars.githubusercontent.com/u/36562?v=4" width="50"><a href="https://github.com/Signum">Signum</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444135206">2018-12-04 15:13</a>:<a class="headerlink" href="#signum_commented_at_2018-12-04_1513" title="Permanent link">&para;</a></h4>
<p>Typo by the way… I meant to say there there is <em>no</em> space left on "/".</p>
<h4 id="gozora_commented_at_2018-12-04_1625"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444162489">2018-12-04 16:25</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-04_1625" title="Permanent link">&para;</a></h4>
<p>Hello @Signum,</p>
<p>Honestly I've never used booting ReaR rescue system from Grub (apart
from writing couple of patches...), I simply don't like it ;-).<br />
I'll try to reproduce your problem and we will see.</p>
<blockquote>
<p>should REAR check if the recovery binary (e.g. "borg") is available
...</p>
</blockquote>
<p><code>borg</code> binary normally should be available since it is checked during
<code>rear mkbackup/mkrescue</code>.</p>
<blockquote>
<p>and can run before destroying the physical disks and repartitioning
them?</p>
</blockquote>
<p>We are talking here about disaster recovery, your system is already
destroyed :-).</p>
<p>I'll check what can be done in terms of pre-checks</p>
<p>V.</p>
<h4 id="gozora_commented_at_2018-12-04_1653"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444172676">2018-12-04 16:53</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-04_1653" title="Permanent link">&para;</a></h4>
<p>I happen to have Fedora release 26 (Twenty Six) installed and configured
with ReaR Borg back end.<br />
I've let ReaR to create Grub boot entry:</p>
<pre><code>### BEGIN /etc/grub.d/45_rear ###
menuentry 'Relax-and-Recover' --class os {
          search --no-floppy --fs-uuid --set=root 44123a90-d84b-49d9-9fb4-70664dc08174
          echo 'Loading kernel /boot/rear-kernel ...'
          linux /rear-kernel  selinux=0
          echo 'Loading initrd /boot/rear-initrd.cgz (may take a while) ...'
          initrd /rear-initrd.cgz
}
### END /etc/grub.d/45_rear ###
</code></pre>
<p>Booted this entry and triggered <code>rear recover</code>.<br />
Everything works as it should.</p>
<p>After booting into ReaR recovery system, I've got following file system
layout:</p>
<pre><code>RESCUE fedora:~ # df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        374M     0  374M   0% /dev
tmpfs           493M     0  493M   0% /dev/shm
tmpfs           493M  6.9M  486M   2% /run
tmpfs           493M     0  493M   0% /sys/fs/cgroup
</code></pre>
<p>Then after starting <code>rear recover</code>:</p>
<pre><code>ESCUE fedora:~ # rear recover
Relax-and-Recover 2.4 / Git
Running rear recover (PID 595)
Using log file: /var/log/rear/rear-fedora.log
Running workflow recover within the ReaR rescue/recovery system
Comparing disks
Device sda has expected (same) size 8589934592 (will be used for recovery)
Disk configuration looks identical
Proceed with recovery (yes) otherwise manual disk layout configuration is enforced
(default 'yes' timeout 30 seconds)
yes
User confirmed to proceed with recovery
Start system layout restoration.
Creating partitions for disk /dev/sda (msdos)
Creating LVM PV /dev/sda2
Restoring LVM VG 'fedora'
Sleeping 3 seconds to let udev or systemd-udevd create their devices...
Creating filesystem of type xfs with mount point / on /dev/mapper/fedora-root.
Mounting filesystem /
Creating filesystem of type ext4 with mount point /boot on /dev/sda1.
Mounting filesystem /boot
Creating swap on /dev/mapper/fedora-swap
Disk layout created.
Starting Borg restore

=== Borg archives list ===
Host:       backup
Repository: /mnt/rear/borg/fedora

[1] rear_6  Wed, 2018-05-09 15:25:19
[2] rear_8  Mon, 2018-09-24 18:33:06

[3] Exit

Choose archive to recover from
(timeout 300 seconds)
1
Recovering from Borg archive rear_6
Borg OS restore finished successfully
Recreating directories (with permissions) from /var/lib/rear/recovery/directories_permissions_owner_group
Running mkinitrd...
Updated initrd with new drivers for kernel 4.14.13-200.fc26.x86_64.
Running mkinitrd...
Updated initrd with new drivers for kernel 4.14.16-200.fc26.x86_64.
Running mkinitrd...
Updated initrd with new drivers for kernel 4.15.17-200.fc26.x86_64.
Skip installing GRUB Legacy boot loader because GRUB 2 is installed (grub-probe or grub2-probe exist).
Installing GRUB2 boot loader...
Determining where to install GRUB2 (no GRUB2_INSTALL_DEVICES specified)
Found possible boot disk /dev/sda - installing GRUB2 there
Finished recovering your system. You can explore it under '/mnt/local'.
Exiting rear recover (PID 595) and its descendant processes
Running exit tasks

RESCUE fedora:~ # df -h
Filesystem               Size  Used Avail Use% Mounted on
devtmpfs                 375M     0  375M   0% /dev
tmpfs                    493M     0  493M   0% /dev/shm
tmpfs                    493M  6.9M  486M   2% /run
tmpfs                    493M     0  493M   0% /sys/fs/cgroup
/dev/mapper/fedora-root  6.2G  2.1G  4.2G  34% /mnt/local
/dev/sda1                976M  178M  753M  20% /mnt/local/boot
</code></pre>
<p>Now I start to think that I did not fully understand what your problem
actually is ....</p>
<p>How can you have full disk, when running <code>rear recover</code>, when
<code>rear recover</code> will newly partition your disk, so it is empty ?<br />
Can it be that you are restoring to smaller disk than your backup
archive ?<br />
If so, how come that</p>
<blockquote>
<p>Then "borg" is called and quits immediately</p>
</blockquote>
<p>it should take some time until your disk fills in ...</p>
<p>V.</p>
<h4 id="jsmeix_commented_at_2018-12-05_1006"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444429866">2018-12-05 10:06</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-05_1006" title="Permanent link">&para;</a></h4>
<p>@Signum<br />
I use neither Vagrant nor Ansible.<br />
I only use KVM/QEMU virtual machines that I set up with 'virt-manager'.</p>
<p>I think to analyze what the root cause is here the crucial point is
not<br />
with what tool or stack of tools the virtual machine was set up.</p>
<p>I think to analyze what the root cause is here the crucial point is<br />
to inspect your environment (in particular your disks <code>parted -l</code> and<br />
your block devices <code>lsblk</code> and what is mounted where <code>findmnt</code> )<br />
in your new booted virtual machine before you run 'rear recover'<br />
to find out what the difference is between using the ReaR ISO image<br />
versus your boot method via GRUB.</p>
<h4 id="jsmeix_commented_at_2018-12-05_1019"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444433984">2018-12-05 10:19</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-05_1019" title="Permanent link">&para;</a></h4>
<p>@gozora<br />
because of</p>
<pre>
I deployed a fresh Debian 9 system from a Vagrant box.
I installed the standalone "borg" binary and installed REAR.
</pre>

<p>I have the dim feeling @Signum may try to run 'rear recover'<br />
from inside the target system disk (e.g. from inside /dev/sda1)<br />
instead of running 'rear recover' outside of the target system disk<br />
as usual for installation systems and/or rescue systems that run<br />
on a ramdisk only within the target system's main memory?</p>
<h4 id="signum_commented_at_2018-12-05_1022"><img src="https://avatars.githubusercontent.com/u/36562?v=4" width="50"><a href="https://github.com/Signum">Signum</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444434789">2018-12-05 10:22</a>:<a class="headerlink" href="#signum_commented_at_2018-12-05_1022" title="Permanent link">&para;</a></h4>
<p>@gozora<br />
What I did:</p>
<ul>
<li>"rear mkbackup" on $SERVER with /etc/rear/local.conf having
    GRUB_RESCUE=yes</li>
<li>Reboot $SERVER</li>
<li>Choose "Relax-and-Recover" in the GRUB boot menu</li>
<li>Login as "root" once REAR has gone through the startup scripts</li>
<li>Find the "/" partition 100% full</li>
</ul>
<h4 id="jsmeix_commented_at_2018-12-05_1026"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444436227">2018-12-05 10:26</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-05_1026" title="Permanent link">&para;</a></h4>
<p>I had overlooked <code>GRUB_RESCUE=y</code> in the initial description.<br />
I understand now how the ReaR recovery system is booted via GRUB.</p>
<p>@Signum<br />
what exactly is the "/" partition for you here?<br />
In the running ReaR recovery system "/" is<br />
a ramdisk that contains the ReaR recovery system.</p>
<h4 id="gozora_commented_at_2018-12-05_1033"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444438059">2018-12-05 10:33</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-05_1033" title="Permanent link">&para;</a></h4>
<p>@Signum can you maybe try to add more RAM to your VM?<br />
It might be that ReaR recovery system (the one you've booted from Grub
menu) have slightly more memory requirements then your ordinary initrd
used for regular boot ...</p>
<p>V.</p>
<h4 id="jsmeix_commented_at_2018-12-05_1108"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444448126">2018-12-05 11:08</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-05_1108" title="Permanent link">&para;</a></h4>
<p>I tried <code>GRUB_RESCUE=y</code> on my SLES12 system<br />
(not with Borg but with BACKUP=NETFS using <code>tar</code>)<br />
and 'rear recover' (via 'Relax-and-Recover' in GRUB)<br />
did "just work" for me<br />
(with 1 GiB main memory in my KVM/QEMU virtual machine).</p>
<h4 id="signum_commented_at_2018-12-05_1354"><img src="https://avatars.githubusercontent.com/u/36562?v=4" width="50"><a href="https://github.com/Signum">Signum</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444492663">2018-12-05 13:54</a>:<a class="headerlink" href="#signum_commented_at_2018-12-05_1354" title="Permanent link">&para;</a></h4>
<p>This is my disk usage after booting into the system:</p>
<pre><code>&gt; df -h
Filesystem      Size  Used Avail Use% Mounted on
rootfs          210M  194M   16M  93% /
devtmpfs        210M     0  210M   0% /dev
tmpfs           247M     0  247M   0% /dev/shm
tmpfs           247M  3.4M  243M   2% /run
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           247M     0  247M   0% /sys/fs/cgroup
</code></pre>
<p>After calling "borg list" it instantly becomes:</p>
<pre><code>&gt; df -h
Filesystem      Size  Used Avail Use% Mounted on
rootfs          210M  210M     0 100% /
devtmpfs        210M     0  210M   0% /dev
tmpfs           247M     0  247M   0% /dev/shm
tmpfs           247M  3.4M  243M   2% /run
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs           247M     0  247M   0% /sys/fs/cgroup
</code></pre>
<p>…and borg fails with "Failed to write all bytes for libcrypto.so.1.0.0".</p>
<p>It looks like BorgBackup has created a directory /tmp/_MEIoKn9nB/ that
contains:</p>
<pre><code>total 15736
drwx------ 2 root root     920 Dec  4 22:12 .
drwxr-xr-x 3 root root      80 Dec  4 22:12 ..
-rwx------ 1 root root   36054 Dec  4 22:12 _bisect.so
-rwx------ 1 root root   57092 Dec  4 22:12 _bz2.so
-rwx------ 1 root root  181683 Dec  4 22:12 _codecs_cn.so
-rwx------ 1 root root  182206 Dec  4 22:12 _codecs_hk.so
-rwx------ 1 root root   73275 Dec  4 22:12 _codecs_iso2022.so
-rwx------ 1 root root  313190 Dec  4 22:12 _codecs_jp.so
-rwx------ 1 root root  166375 Dec  4 22:12 _codecs_kr.so
-rwx------ 1 root root  134008 Dec  4 22:12 _codecs_tw.so
-rwx------ 1 root root  611525 Dec  4 22:12 _ctypes.so
-rwx------ 1 root root  367413 Dec  4 22:12 _datetime.so
-rwx------ 1 root root 1949218 Dec  4 22:12 _decimal.so
-rwx------ 1 root root   78120 Dec  4 22:12 _hashlib.so
-rwx------ 1 root root   48695 Dec  4 22:12 _heapq.so
-rwx------ 1 root root  152383 Dec  4 22:12 _json.so
-rwx------ 1 root root   65569 Dec  4 22:12 _lsprof.so
-rwx------ 1 root root  110627 Dec  4 22:12 _lzma.so
-rwx------ 1 root root   48385 Dec  4 22:12 _md5.so
-rwx------ 1 root root  130437 Dec  4 22:12 _multibytecodec.so
-rwx------ 1 root root   52074 Dec  4 22:12 _multiprocessing.so
-rwx------ 1 root root   20814 Dec  4 22:12 _opcode.so
-rwx------ 1 root root  515672 Dec  4 22:12 _pickle.so
-rwx------ 1 root root   89207 Dec  4 22:12 _posixsubprocess.so
-rwx------ 1 root root   36731 Dec  4 22:12 _random.so
-rwx------ 1 root root   47306 Dec  4 22:12 _sha1.so
-rwx------ 1 root root   66784 Dec  4 22:12 _sha256.so
-rwx------ 1 root root   72941 Dec  4 22:12 _sha512.so
-rwx------ 1 root root  276203 Dec  4 22:12 _socket.so
-rwx------ 1 root root  148385 Dec  4 22:12 _struct.so
-rwx------ 1 root root  186992 Dec  4 22:12 array.so
-rwx------ 1 root root   81241 Dec  4 22:12 binascii.so
-rwx------ 1 root root  242736 Dec  4 22:12 borg.algorithms.checksums.so
-rwx------ 1 root root  192249 Dec  4 22:12 borg.chunker.so
-rwx------ 1 root root 4839421 Dec  4 22:12 borg.compress.so
-rwx------ 1 root root  563556 Dec  4 22:12 borg.crypto.low_level.so
-rwx------ 1 root root  649683 Dec  4 22:12 borg.hashindex.so
-rwx------ 1 root root  698128 Dec  4 22:12 borg.item.so
-rwx------ 1 root root  817363 Dec  4 22:12 borg.platform.linux.so
-rwx------ 1 root root  135495 Dec  4 22:12 borg.platform.posix.so
-rwx------ 1 root root   44526 Dec  4 22:12 fcntl.so
-rwx------ 1 root root   35988 Dec  4 22:12 grp.so
-rwx------ 1 root root   35320 Dec  4 22:12 libacl.so.1
-rwx------ 1 root root   18672 Dec  4 22:12 libattr.so.1
-rwx------ 1 root root   66824 Dec  4 22:12 libbz2.so.1.0
-rw-r--r-- 1 root root 1396736 Dec  4 22:12 libcrypto.so.1.0.0
</code></pre>
<p>…and that's pretty exactly 16 MB in size.</p>
<p>So either 210 MB is not enough for the root partition or it is not
expected that Borg creates that many files in /tmp.</p>
<h4 id="gozora_commented_at_2018-12-05_1416"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444499665">2018-12-05 14:16</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-05_1416" title="Permanent link">&para;</a></h4>
<p>@Signum that makes perfect sense.<br />
Standalone <code>borg</code> carries all its file and libraries in its executable
and extracts them to temporary location (mostly /tmp) upon its
execution.</p>
<p>I guess that you just need more RAM assigned to your VM and you are safe
...</p>
<p>V.</p>
<h4 id="signum_commented_at_2018-12-05_1448"><img src="https://avatars.githubusercontent.com/u/36562?v=4" width="50"><a href="https://github.com/Signum">Signum</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444510747">2018-12-05 14:48</a>:<a class="headerlink" href="#signum_commented_at_2018-12-05_1448" title="Permanent link">&para;</a></h4>
<p>@gozora<br />
You are totally right. RAM increased from 512 MB to 1024 MB and now the
RAM disk is large enough to start Borg. Oh, my. Sorry for being so
stupid not to track that down earlier. I rest my case. :)</p>
<p>This really seems to be a corner case. It was just unfortunate because
REAR destroys the existing partitions and then Borg fails. So perhaps it
could be checked if "borg" is executable. But in the end that's really a
minor issue.</p>
<p>Thanks everyone for lending me your brains.</p>
<h4 id="jsmeix_commented_at_2018-12-05_1459"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444514759">2018-12-05 14:59</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-05_1459" title="Permanent link">&para;</a></h4>
<p>@Signum<br />
thank you for your explanatory feedback what the root cause was.<br />
It helps us a lot to better imagine even special use-cases.</p>
<p>@gozora<br />
now I am thinking about to add another test to the recovery system<br />
startup scripts that checks for "sufficient" free ramdisk space in
advance<br />
before things fail in arbitrary weird ways later during "rear recover".</p>
<p>Currently I don't know what "sufficient free ramdisk space" could be<br />
to run "rear recover" successfully under usual circumstances.<br />
I guess a bit more than 16 MiB would be not too bad in general ;-)</p>
<p>On my KVM/QEMU system with 1GiB memory<br />
<code>df -h /</code> shows for the recovery system rootfs<br />
449M size where 228M are used (51%) and 222M are free<br />
before I run "rear recover" and after "rear recover" finished<br />
238M are used (54%) and 211M are free.</p>
<p>@Signum<br />
what does <code>df -h /</code> show for the recovery system rootfs<br />
before and after "rear recover" in your case?</p>
<p>I like to get an idea how much free ramdisk space is usually needed.</p>
<h4 id="gozora_commented_at_2018-12-05_1531"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444526414">2018-12-05 15:31</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-05_1531" title="Permanent link">&para;</a></h4>
<p>@jsmeix question of memory can be quite tricky.<br />
On my testing VM with Arch Linux I have really minimalist initramfs and
kernel:</p>
<pre><code>arch-efi:(/root)(root)# ll /boot/initramfs-linux.img /boot/vmlinuz-linux 
-rwxr-xr-x 1 root root 8052959 Oct 29 19:17 /boot/initramfs-linux.img
-rwxr-xr-x 1 root root 5875568 Oct 21 00:06 /boot/vmlinuz-linux
</code></pre>
<p>This means that my Arch VM will boot without any trouble with 256MB of
RAM.<br />
However if I run backup of Arch I have initrramfs (~200MB)</p>
<pre><code>arch-efi:(/root)(root)# ll /mnt/iso/isolinux/{initrd.cgz,kernel}
-rw------- 1 root root 221129886 Dec  4 18:45 /mnt/iso/isolinux/initrd.cgz
-rwxr-xr-x 1 root root   5875568 Oct 21 00:06 /mnt/iso/isolinux/kernel
</code></pre>
<p>Which will fail to boot with 256MB of RAM.</p>
<p>This is why I think that it might be a bit hard to find universal way
how to resolve this ...</p>
<p>V.</p>
<h4 id="gozora_commented_at_2018-12-05_1533"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444527580">2018-12-05 15:33</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-05_1533" title="Permanent link">&para;</a></h4>
<p>I guess @Signum was kind of lucky that he was able to boot, if his
initramfs would be ~16MB larger (e.g. by using MODULES=( 'all_modules'
)), his ReaR recovery system would not boot at all.</p>
<p>V.</p>
<h4 id="jsmeix_commented_at_2018-12-05_1546"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444532569">2018-12-05 15:46</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-05_1546" title="Permanent link">&para;</a></h4>
<p>@gozora<br />
I do not plan to abort when there is only very little free space<br />
but I like to show in any case how much free space there is<br />
(basically the <code>df -h /</code> output) and show a nice WARNING<br />
if there is only very little free space.</p>
<p>FYI:<br />
I did right now a recovery this way:</p>
<pre>
# export MIGRATION_MODE=false
# rear -D recover & for i in $( seq 120 ) ; do df -h / ; sleep 1 ; done
</pre>

<p>that shows me each second a <code>df -h /</code> output while "rear recover" is
running<br />
to (hopefully) detect significant space usage by some tools<br />
(e.g. temporary files that get deleted afterwards)<br />
but I found none.<br />
For me it increases steadily from 51% to 54% (which is 11 MiB).</p>
<h4 id="gozora_commented_at_2018-12-05_1555"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444536064">2018-12-05 15:55</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-05_1555" title="Permanent link">&para;</a></h4>
<p>@jsmeix just a short heads up on <code>df -h</code>.<br />
Not sure why but some of my test machines does not show <code>/</code> utilization
in ReaR recovery system:</p>
<p><em>Arch</em></p>
<pre><code>RESCUE arch-efi:~ # df -h /
Filesystem      Size  Used Avail Use% Mounted on
rootfs             0     0     0    - /
RESCUE arch-efi:~ # df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        890M     0  890M   0% /dev
tmpfs           997M     0  997M   0% /dev/shm
tmpfs           997M  8.3M  989M   1% /run
tmpfs           997M     0  997M   0% /sys/fs/cgroup
</code></pre>
<p><em>Fedora</em></p>
<pre><code>RESCUE fedora:~ # df -h /
Filesystem      Size  Used Avail Use% Mounted on
rootfs             0     0     0    - /
RESCUE fedora:~ # df 
Filesystem     1K-blocks  Used Available Use% Mounted on
devtmpfs          898160     0    898160   0% /dev
tmpfs            1020272     0   1020272   0% /dev/shm
tmpfs            1020272  8860   1011412   1% /run
tmpfs            1020272     0   1020272   0% /sys/fs/cgroup
</code></pre>
<p>So yet another complication I guess ...</p>
<p>V.</p>
<h4 id="gozora_commented_at_2018-12-05_1606"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444540639">2018-12-05 16:06</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-05_1606" title="Permanent link">&para;</a></h4>
<p>Heh, even more funny ...<br />
<em>Centos</em></p>
<pre><code>RESCUE centos69:~ # df -h
df: no file systems processed
RESCUE centos69:~ # mount
rootfs on / type rootfs (rw)
none on /proc type proc (rw,nosuid,nodev,noexec,relatime)
none on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)
none on /dev/pts type devpts (rw,relatime,gid=5,mode=620,ptmxmode=000)
</code></pre>
<p>V.</p>
<h4 id="jsmeix_commented_at_2018-12-06_0824"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444788149">2018-12-06 08:24</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-06_0824" title="Permanent link">&para;</a></h4>
<p>@gozora<br />
thank you so much for sharing your experience!<br />
You saved me from feeling hallucination because I had also experienced
the<br />
same both crazy <code>df</code> behaviour as you experienced within your recovery
system<br />
on my SLES12 KVM/QEMU virtual machine but somehow (on the same virtual<br />
machine with the same recovery system but booted anew for my various
attempts)<br />
it also "just worked" as shown in my above comments and I was wondering
about<br />
if I am hallucination that <code>df</code> had "just failed" for me in some other
attempts before.</p>
<h4 id="jsmeix_commented_at_2018-12-06_0826"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444788876">2018-12-06 08:26</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-12-06_0826" title="Permanent link">&para;</a></h4>
<p>Only a blind guess:<br />
Might those unexpected <code>df</code> failures within the recovery system<br />
perhaps be avoided by using <code>MODULES=( 'all_modules' )</code> ?</p>
<h4 id="gozora_commented_at_2018-12-06_0845"><img src="https://avatars.githubusercontent.com/u/12116358?u=1c5ba9dcee5ca3082f03029a7fbe647efd30eb49&v=4" width="50"><a href="https://github.com/gozora">gozora</a> commented at <a href="https://github.com/rear/rear/issues/1994#issuecomment-444793676">2018-12-06 08:45</a>:<a class="headerlink" href="#gozora_commented_at_2018-12-06_0845" title="Permanent link">&para;</a></h4>
<p>Hello @jsmeix,</p>
<p>This one looks better:</p>
<pre><code>RESCUE arch:~ # df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
devtmpfs       devtmpfs  1.9G     0  1.9G   0% /dev
tmpfs          tmpfs     2.0G     0  2.0G   0% /dev/shm
tmpfs          tmpfs     2.0G  8.3M  2.0G   1% /run
tmpfs          tmpfs     2.0G     0  2.0G   0% /sys/fs/cgroup
RESCUE arch:~ # df -hTa
Filesystem     Type        Size  Used Avail Use% Mounted on
rootfs         rootfs         0     0     0    - /
sysfs          sysfs          0     0     0    - /sys
proc           proc           0     0     0    - /proc
devtmpfs       devtmpfs    1.9G     0  1.9G   0% /dev
securityfs     securityfs     0     0     0    - /sys/kernel/security
tmpfs          tmpfs       2.0G     0  2.0G   0% /dev/shm
devpts         -              -     -     -    - /dev/pts
tmpfs          tmpfs       2.0G  8.3M  2.0G   1% /run
tmpfs          tmpfs       2.0G     0  2.0G   0% /sys/fs/cgroup
cgroup2        cgroup2        0     0     0    - /sys/fs/cgroup/unified
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/systemd
pstore         pstore         0     0     0    - /sys/fs/pstore
efivarfs       efivarfs       0     0     0    - /sys/firmware/efi/efivars
bpf            bpf            0     0     0    - /sys/fs/bpf
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/blkio
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/hugetlb
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/memory
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/pids
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/perf_event
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/net_cls,net_prio
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/freezer
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/devices
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/cpu,cpuacct
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/cpuset
cgroup         cgroup         0     0     0    - /sys/fs/cgroup/rdma
none           devpts         0     0     0    - /dev/pts
</code></pre>
<p>Excerpt from <code>df --help</code>:</p>
<pre><code>Mandatory arguments to long options are mandatory for short options too.
  -a, --all             include pseudo, duplicate, inaccessible file system
</code></pre>
<p>V.</p>
<hr />
<p>[Export of Github issue for
<a href="https://github.com/rear/rear">rear/rear</a>.]</p>
              
            </div>
          </div>

<footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2024 - CC0 1.0 Universal<br />Give <a href="https://github.com/rear/rear-user-guide/issues/new?title=issues/2018-12-04.1994.issue.closed.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
