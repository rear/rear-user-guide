<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="Relax-and-Recover (ReaR) User Guide Documentation"/>
    <meta property="og:description" content="This is an umbrella documentation project for all Relax-and-Recover (ReaR) kind of documentation ans starting with a good User Guide."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://relax-and-recover.org/rear-user-guide/"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://relax-and-recover.org/rear-user-guide/img/rear_logo_50.png"/>
    <meta property="og:image:width" content="50"/>
    <meta property="og:image:height" content="50"/>
    
    <title>#799 Issue closed: Clean up disks before recreating partitions/volumes/filesystems/... - Relax-and-Recover (ReaR) User Guide Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/rear.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "#799 Issue closed: Clean up disks before recreating partitions/volumes/filesystems/...";
        var mkdocs_page_input_path = "issues/2016-03-15.799.issue.closed.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', "366986045", "auto");
        ga('send', 'pageview');
      </script>
    
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Relax-and-Recover (ReaR) User Guide Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">BASICS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/introduction.html">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/history.html">Bit of History</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/getting-started.html">Getting started with ReaR</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/configuration.html">Basic configuration</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/backup_netfs.html">Example of BACKUP=NETFS</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SCENARIOS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../scenarios/index.html">Scenarios Overview</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">DEVELOPMENT</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../development/github-pr.html">Make a pull request with GitHub</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../development/squash-git-log-commments.html">How to squash git log comments into one line</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/index.html">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear29.html">Release Notes ReaR 2.9</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear28.html">Release Notes ReaR 2.8</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear27.html">Release Notes ReaR 2.7</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear26.html">Release Notes ReaR 2.6</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/knownproblems.html">Known Problems and Workarounds</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ISSUES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Issues History</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">#799 Issue closed: Clean up disks before recreating partitions/volumes/filesystems/...</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="799_issue_closed_clean_up_disks_before_recreating_partitionsvolumesfilesystems"><a href="https://github.com/rear/rear/issues/799">#799 Issue</a> <code>closed</code>: Clean up disks before recreating partitions/volumes/filesystems/...<a class="headerlink" href="#799_issue_closed_clean_up_disks_before_recreating_partitionsvolumesfilesystems" title="Permanent link">&para;</a></h1>
<p><strong>Labels</strong>: <code>enhancement</code>, <code>fixed / solved / done</code>, <code>severe improvement</code></p>
<h4 id="jsmeix_opened_issue_at_2016-03-15_1509"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> opened issue at <a href="https://github.com/rear/rear/issues/799">2016-03-15 15:09</a>:<a class="headerlink" href="#jsmeix_opened_issue_at_2016-03-15_1509" title="Permanent link">&para;</a></h4>
<p>Hereby I propose to let a "cleanupdisk" script run early<br />
(i.e. before anything is done with the harddisk,<br />
in particular before a "parted" command is run).</p>
<p>The purpose of the "cleanupdisk" script is to wipe any<br />
possibly remainders of various kind of metadata information<br />
from the harddisk that could belong to various higher layers<br />
of storage objects.</p>
<p>Currently (cf.
<a href="https://github.com/rear/rear/issues/540">https://github.com/rear/rear/issues/540</a>)
"wipefs" is run in<br />
130_include_filesystem_code.sh for each partition device node<br />
before a filesystem is created on that partition device node.</p>
<p>But after I wrote
<a href="https://github.com/rear/rear/issues/791#issuecomment-196212960">https://github.com/rear/rear/issues/791#issuecomment-196212960</a>
I noticed that running "wipefs" before filesystems are created is
probably too late.</p>
<p>I had this "too late" problem already recognized in
<a href="https://github.com/rear/rear/issues/540#issuecomment-71814659">https://github.com/rear/rear/issues/540#issuecomment-71814659</a>
(there "it failed for RHEL6 at the partitioning level because of old
data of the MD level so that before partitioning the MD tool would have
to be run to clean up old MD data") but unfortunately that had slipped
my lossy mind :-(</p>
<p>See
<a href="https://github.com/rear/rear/issues/791#issuecomment-196212960">https://github.com/rear/rear/issues/791#issuecomment-196212960</a>
for the reason why "wipefs" musts probably be run before anything is
done with the harddisk, in particular before a "parted" command is run
(excerpt):</p>
<pre>
For example if you re-use a harddisk that
had before LVM on it, it may happen that
after creating partitions from scatch on
that hsrddisk, udev may also trigger
to run LVM tools ...
When those LVM tools detect remaining
old/outdated LVM metadata information
on the harddisk, there could be arbitrarily
unexpected results (e.g. all of a sudden
LVM issues may get in your way regardless
that you only had called parted to create
partitions).
</pre>

<p>Here what I get in the ReaR recovery system<br />
directly after login as root<br />
on pristine new hardware<br />
(where "pristine new hardware" is a new<br />
from scratch created QEMU/KVM virtual<br />
machine with full hardware virtualization):</p>
<pre>
 # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 15 14:36 /dev/sda

# parted /dev/sda print
Error: /dev/sda: unrecognised disk label
Model: ATA QEMU HARDDISK (scsi)                                           
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags: 
</pre>

<p>And now ( tada - surprise! - not really ;-)<br />
what I get in the ReaR recovery system<br />
directly after login as root<br />
on same kind of a machine where I already<br />
had done a "rear recover" some time before<br />
(i.e. where a subsequent "rear recover" would run<br />
on a system where the harddisk was already in use):</p>
<pre>
# ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 15 14:31 /dev/sda
brw-rw---- 1 root disk 8, 1 Mar 15 14:31 /dev/sda1
brw-rw---- 1 root disk 8, 2 Mar 15 14:31 /dev/sda2

# parted /dev/sda print
Model: ATA QEMU HARDDISK (scsi)
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags: 

Number  Start   End     Size    Type     File system     Flags
 1      1049kB  1571MB  1570MB  primary  linux-swap(v1)  type=83
 2      1571MB  21.5GB  19.9GB  primary  ext4            boot, type=83
</pre>

<p>Accordingly I think ReaR should run something like</p>
<pre>
wipefs -a -f /dev/sda2
wipefs -a -f /dev/sda1
wipefs -a -f /dev/sda
</pre>

<p>to fully clean up the used harddisk before doing anything with it.</p>
<p>Regarding the '-f' option see
<a href="https://github.com/rear/rear/issues/540#issuecomment-163241543">https://github.com/rear/rear/issues/540#issuecomment-163241543</a></p>
<h4 id="jsmeix_commented_at_2016-03-15_1511"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-196868428">2016-03-15 15:11</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-03-15_1511" title="Permanent link">&para;</a></h4>
<p>@gdha @schlomo @thefrenchone @tbsky</p>
<p>I ask you all for feedback what you think about it.</p>
<h4 id="jsmeix_commented_at_2016-03-16_1204"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-197286229">2016-03-16 12:04</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-03-16_1204" title="Permanent link">&para;</a></h4>
<p>What wipefs results in the rear recovery system<br />
directly after login as root<br />
on the above mentioned (second) machine<br />
where the harddisk was already in use:</p>
<pre>
Welcome to Relax and Recover. Run "rear recover" to restore your system !

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:02 /dev/sda
brw-rw---- 1 root disk 8, 1 Mar 16 11:02 /dev/sda1
brw-rw---- 1 root disk 8, 2 Mar 16 11:02 /dev/sda2

RESCUE f197:~ # parted /dev/sda print
Model: ATA QEMU HARDDISK (scsi)
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags: 
Number  Start   End     Size    Type     File system     Flags
 1      1049kB  1571MB  1570MB  primary  linux-swap(v1)  type=83
 2      1571MB  21.5GB  19.9GB  primary  ext4            boot, type=83

RESCUE f197:~ # wipefs -a -f /dev/sda2
/dev/sda2: 2 bytes were erased at offset 0x00000438 (ext4): 53 ef

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:42 /dev/sda
brw-rw---- 1 root disk 8, 1 Mar 16 11:02 /dev/sda1
brw-rw---- 1 root disk 8, 2 Mar 16 11:42 /dev/sda2

RESCUE f197:~ # parted /dev/sda print
Model: ATA QEMU HARDDISK (scsi)
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags: 
Number  Start   End     Size    Type     File system     Flags
 1      1049kB  1571MB  1570MB  primary  linux-swap(v1)  type=83
 2      1571MB  21.5GB  19.9GB  primary                  boot, type=83

RESCUE f197:~ # wipefs -a -f /dev/sda1
/dev/sda1: 10 bytes were erased at offset 0x00000ff6 (swap): 53 57 41 50 53 50 41 43 45 32

RESCUE f197:~ # parted /dev/sda print
Model: ATA QEMU HARDDISK (scsi)
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags: 
Number  Start   End     Size    Type     File system  Flags
 1      1049kB  1571MB  1570MB  primary               type=83
 2      1571MB  21.5GB  19.9GB  primary               boot, type=83

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:42 /dev/sda
brw-rw---- 1 root disk 8, 1 Mar 16 11:42 /dev/sda1
brw-rw---- 1 root disk 8, 2 Mar 16 11:42 /dev/sda2

RESCUE f197:~ # wipefs -a -f /dev/sda
/dev/sda: 2 bytes were erased at offset 0x000001fe (dos): 55 aa

RESCUE f197:~ # parted /dev/sda print
Error: /dev/sda: unrecognised disk label
Model: ATA QEMU HARDDISK (scsi)                                           
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: unknown
Disk Flags: 

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:42 /dev/sda
brw-rw---- 1 root disk 8, 1 Mar 16 11:42 /dev/sda1
brw-rw---- 1 root disk 8, 2 Mar 16 11:42 /dev/sda2

RESCUE f197:~ # partprobe -s /dev/sda

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:47 /dev/sda
brw-rw---- 1 root disk 8, 1 Mar 16 11:42 /dev/sda1
brw-rw---- 1 root disk 8, 2 Mar 16 11:42 /dev/sda2

RESCUE f197:~ # parted /dev/sda mklabel msdos
Information: You may need to update /etc/fstab.

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:51 /dev/sda

RESCUE f197:~ # partprobe -s /dev/sda
/dev/sda: msdos partitions

RESCUE f197:~ # parted /dev/sda print
Model: ATA QEMU HARDDISK (scsi)
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: msdos
Disk Flags: 
Number  Start  End  Size  Type  File system  Flags

RESCUE f197:~ # parted /dev/sda mklabel gpt
Warning: The existing disk label on /dev/sda will be destroyed
and all data on this disk will be lost.
Do you want to continue?
Yes/No? y                                                                 
Information: You may need to update /etc/fstab.

RESCUE f197:~ # parted /dev/sda print
Model: ATA QEMU HARDDISK (scsi)
Disk /dev/sda: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags: 
Number  Start  End  Size  File system  Name  Flags

RESCUE f197:~ # ls -l /dev/sd*
brw-rw---- 1 root disk 8, 0 Mar 16 11:55 /dev/sda
</pre>

<p>Summary:</p>
<p>After wipefs the harddisk /dev/sda looks empty for parted<br />
but nevertheless regardless of a partprobe call<br />
the old/outdated partition device nodes (/dev/sda1and /dev/sda2)<br />
do not go away.</p>
<p>To make the old/outdated partition device nodes go away<br />
an explicit "parted /dev/sda mklabel" command is needed.</p>
<p>It seems to be fail-safe to set a hardcoded "msdos" dummy label<br />
in such a "cleanupdisk" script via "parted /dev/... mklabel msdos"<br />
to make old/outdated partition device nodes go away because<br />
later the actual disk label (e.g. "gpt") will be set<br />
in 10_include_partition_code.sh via</p>
<pre>
parted -s $device mklabel $label >&2
</pre>

<h4 id="thefrenchone_commented_at_2016-03-16_2101"><img src="https://avatars.githubusercontent.com/u/17705716?v=4" width="50"><a href="https://github.com/thefrenchone">thefrenchone</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-197547941">2016-03-16 21:01</a>:<a class="headerlink" href="#thefrenchone_commented_at_2016-03-16_2101" title="Permanent link">&para;</a></h4>
<p>wipefs would be great to have run earlier. It's code generated from
10_include_partition_code.sh that would occasionally fail or require
extra time for me. Just removing LVM information was usually enough.</p>
<p>At the moment I can't test this but hopefully soon I'll be able to. I'll
try adding the wipefs command to clear the disk earlier.</p>
<h4 id="jsmeix_commented_at_2016-03-18_1401"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-198374085">2016-03-18 14:01</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-03-18_1401" title="Permanent link">&para;</a></h4>
<p>@mattihautameki<br />
I remembered your issue
<a href="https://github.com/rear/rear/issues/649">https://github.com/rear/rear/issues/649</a>
that also seems to be related to disk cleanup (cf.
<a href="https://github.com/rear/rear/issues/649#issuecomment-148710502">https://github.com/rear/rear/issues/649#issuecomment-148710502</a>)
so that this issue here could be also of interest for you.</p>
<h4 id="jsmeix_commented_at_2016-05-23_0923"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-220929985">2016-05-23 09:23</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-05-23_0923" title="Permanent link">&para;</a></h4>
<p>For the fun of it:<br />
It seems SUSE implements a new parted option --wipesignatures,<br />
see<br />
<a href="https://bugzilla.opensuse.org/show_bug.cgi?id=980834">https://bugzilla.opensuse.org/show_bug.cgi?id=980834</a></p>
<p>For the current sources see the<br />
"parted-implement-wipesignatures-option.patch" at<br />
<a href="https://build.opensuse.org/package/show/Base:System/parted">https://build.opensuse.org/package/show/Base:System/parted</a></p>
<p>But it seems that fails curently in the same way<br />
as the initial comment in
<a href="https://github.com/rear/rear/issues/793#issue-139355299">https://github.com/rear/rear/issues/793#issue-139355299</a></p>
<p>It seems currently "everybody" has issues with "udev vs. parted",<br />
cf.
<a href="https://github.com/rear/rear/issues/791#issuecomment-220923467">https://github.com/rear/rear/issues/791#issuecomment-220923467</a></p>
<h4 id="jsmeix_commented_at_2016-05-23_0925"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-220930504">2016-05-23 09:25</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-05-23_0925" title="Permanent link">&para;</a></h4>
<p>Because the more I learn about it<br />
the more I get confused how to make it working<br />
I postpone this issue to any later unspecified<br />
"future rear version".</p>
<h4 id="jsmeix_commented_at_2016-05-30_0956"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-222457589">2016-05-30 09:56</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-05-30_0956" title="Permanent link">&para;</a></h4>
<p>An addedum regarding a higher stack of storage objects<br />
in particular regarding MD devices (aka Linux Software RAID):</p>
<p>One can try to remove mdadm superblocks from hardrives by</p>
<pre>
mdadm --zero-superblock /dev/sd{a,b,c,d}
</pre>

<p>to avoid hdds to be detected as mdadm devices.</p>
<p>Ideally only calling the generic "wipefs" tool<br />
would do all what is needed but perhaps<br />
we may have to also call the individual tools<br />
of each particular higher level storage object<br />
to really clean up everything properly.</p>
<h4 id="jsmeix_commented_at_2016-10-04_1243"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-251377041">2016-10-04 12:43</a>:<a class="headerlink" href="#jsmeix_commented_at_2016-10-04_1243" title="Permanent link">&para;</a></h4>
<p>FYI regarding<br />
"How do I delete a RAID volume that was created with mdadm?"</p>
<p>One may have a look at<br />
<a href="https://forums.opensuse.org/showthread.php/489778-How-do-I-delete-a-RAID-volume-that-was-created-with-mdadm">https://forums.opensuse.org/showthread.php/489778-How-do-I-delete-a-RAID-volume-that-was-created-with-mdadm</a></p>
<p>Key steps:</p>
<ul>
<li>
<p>Stop the RAID</p>
<pre>
<h1 id="mdadm_--stop_devmd0">mdadm --stop /dev/md0<a class="headerlink" href="#mdadm_--stop_devmd0" title="Permanent link">&para;</a></h1>
</pre>
</li>
<li>
<p>Remove the RAID signatures from every (!) part of the RAID</p>
<pre>
<h1 id="mdadm_--zero-superblock_devsd">mdadm --zero-superblock /dev/sd??<a class="headerlink" href="#mdadm_--zero-superblock_devsd" title="Permanent link">&para;</a></h1>
</pre>
<p>It should also work to use "wipefs".</p>
</li>
</ul>
<h4 id="gdha_commented_at_2017-01-21_1134"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-274256292">2017-01-21 11:34</a>:<a class="headerlink" href="#gdha_commented_at_2017-01-21_1134" title="Permanent link">&para;</a></h4>
<p>@jsmeix perhaps <strong>shred</strong> utility could also be useful
(<a href="http://www.computerhope.com/unix/shred.htm">http://www.computerhope.com/unix/shred.htm</a>).
I noticed that RH engineers are using this command to wipe a disk
(before doing rear recover test) - e.g. <code>shred /dev/sda</code></p>
<h4 id="schlomo_commented_at_2017-05-12_0523"><img src="https://avatars.githubusercontent.com/u/101384?v=4" width="50"><a href="https://github.com/schlomo">schlomo</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-300986654">2017-05-12 05:23</a>:<a class="headerlink" href="#schlomo_commented_at_2017-05-12_0523" title="Permanent link">&para;</a></h4>
<p>Let's just make sure that we don't touch the hard disks before</p>
<ol>
<li>
<p>The user actually starts <code>rear recover</code>.</p>
</li>
<li>
<p>The <code>verify</code> stage was run successfully.</p>
</li>
</ol>
<h4 id="jsmeix_commented_at_2017-05-12_0832"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-301017621">2017-05-12 08:32</a>:<a class="headerlink" href="#jsmeix_commented_at_2017-05-12_0832" title="Permanent link">&para;</a></h4>
<p>I think to have such a "cleanupdisk" script behave<br />
in compliance with how "rear recover" currently works<br />
the script must not work directly on the harddisk(s)<br />
but instead output commands that clean up disks<br />
into the LAYOUT_CODE script (i.e. diskrestore.sh).</p>
<p>Such an early "cleanupdisk" script would have to run<br />
before scripts like<br />
layout/prepare/GNU/Linux/100_include_partition_code.sh<br />
...<br />
layout/prepare/GNU/Linux/130_include_filesystem_code.sh<br />
to get the disk cleanup commands are at the beinning<br />
of the diskrestore.sh script, cf. my initial<br />
<a href="https://github.com/rear/rear/issues/799#issue-141001306">https://github.com/rear/rear/issues/799#issue-141001306</a></p>
<pre>
... "wipefs" musts probably be run before anything
is done with the harddisk, in particular before
a "parted" command is run ...
</pre>

<h4 id="schlomo_commented_at_2017-05-12_0901"><img src="https://avatars.githubusercontent.com/u/101384?v=4" width="50"><a href="https://github.com/schlomo">schlomo</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-301023691">2017-05-12 09:01</a>:<a class="headerlink" href="#schlomo_commented_at_2017-05-12_0901" title="Permanent link">&para;</a></h4>
<p>@jsmeix very good point. Yes, of course the cleanup stuff should be
added to the beginning of the <code>diskrestore.sh</code> and not run directly.</p>
<h4 id="jsmeix_commented_at_2018-06-27_1201"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-400647083">2018-06-27 12:01</a>:<a class="headerlink" href="#jsmeix_commented_at_2018-06-27_1201" title="Permanent link">&para;</a></h4>
<p>Right now I had an issue because of remaining LVM metadata on a used
disk:</p>
<p>0.)<br />
On the original system there is only /dev/sda with LVM on it.<br />
1.)<br />
I recovered that on replacement system with only /dev/sda without
problems.<br />
2.)<br />
I added a /dev/sdb to the replacement system (same size as /dev/sda).<br />
3.)<br />
I recovered the original system again on the replacement system<br />
but now I selected to map /dev/sda to /dev/sdb (in MIGRATION_MODE).<br />
4.)<br />
This lets LVM setup in diskrestore.sh fail because an LVM tool got
confused<br />
that a UUID which it should create on /dev/sdb already exists on
/dev/sda.<br />
5.)<br />
The manual solution was to go during "rear recover" into the ReaR
shell<br />
and do <code>wipefs -af /dev/sda1</code> (that was used for LVM before)<br />
and <code>wipefs -af /dev/sda</code> (to be on the safe side).<br />
6.)<br />
Afterwards the (unmodified) diskrestore.sh works.</p>
<h4 id="gdha_commented_at_2019-09-13_0737"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-531133632">2019-09-13 07:37</a>:<a class="headerlink" href="#gdha_commented_at_2019-09-13_0737" title="Permanent link">&para;</a></h4>
<p>@jsmeix Any plans to go forward with this enhancement as it is a long
outstanding item? I guess there are no volunteers to write this :-(</p>
<h4 id="jsmeix_commented_at_2019-09-13_1356"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-531247109">2019-09-13 13:56</a>:<a class="headerlink" href="#jsmeix_commented_at_2019-09-13_1356" title="Permanent link">&para;</a></h4>
<p>@gdha<br />
if time permits my plan would be to try out something<br />
and if that works reasonably well for me I would add<br />
an initial version of such a script that could be further<br />
improved step by step as needed<br />
BUT<br />
"time permits" never happens in practice because of various<br />
kind of annoying issues elsewhere, in particular an endless<br />
sequence of security issues in printing related bloatware<br />
(my other working area - I just hate that stuff)<br />
SO<br />
in practice never time for long term improvements in ReaR<br />
only time for some quick fix here some quick fix there<br />
ad nauseam...<br />
cf. "ad nauseam" in<br />
<a href="https://en.opensuse.org/SDB:CUPS_and_SANE_Firewall_settings">https://en.opensuse.org/SDB:CUPS_and_SANE_Firewall_settings</a></p>
<h4 id="jsmeix_commented_at_2020-03-13_0919"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-598626162">2020-03-13 09:19</a>:<a class="headerlink" href="#jsmeix_commented_at_2020-03-13_0919" title="Permanent link">&para;</a></h4>
<p>Since my SUSE Hack Week 19 continuing experiments<br />
<a href="https://github.com/rear/rear/issues/2254#issuecomment-595678350">https://github.com/rear/rear/issues/2254#issuecomment-595678350</a><br />
I have good and bad news:</p>
<p>The good news is that I could experience myself<br />
a few different kind of "horrible" consequences<br />
of remainder meta data on an already used disk<br />
in particular remainders of RAID and LVM metadata.</p>
<p>The bad news is that <code>wipefs</code> alone is insufficient,<br />
i.e. something like</p>
<pre><code>wipefs -a -f /dev/sdb3
wipefs -a -f /dev/sdb2
wipefs -a -f /dev/sdb1
wipefs -a -f /dev/sdb

wipefs -a -f /dev/sda3
wipefs -a -f /dev/sda2
wipefs -a -f /dev/sda1
wipefs -a -f /dev/sda
</code></pre>
<p>is insufficient in general, cf.<br />
<a href="https://github.com/rear/rear/issues/2019#issuecomment-476598723">https://github.com/rear/rear/issues/2019#issuecomment-476598723</a><br />
and the subsequent comments therein.</p>
<p>According to my personal (very limited) experience<br />
<code>wipefs</code> is only sufficient if LVM PVs are plain partitions<br />
but when the PVs are higher level storage objects<br />
like RAID1 block devices then <code>wipefs</code> of disk partitions<br />
is insufficient because it does not remove LVM metadata.</p>
<p>The current generic way how to deal with it is to<br />
"Prepare replacement hardware for disaster recovery", cf.<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a><br />
therein in particular the part about<br />
"you must completely zero out your replacement storage"<br />
BUT<br />
that workaround is not possible within reasonable time<br />
when one needs to recover from soft errors on the original machine<br />
i.e. when the exact same original disks are re-used "as is".</p>
<p>To recover from "minor" soft errors (i.e. when only files had been<br />
destroyed or messed up so that a backup restore is sufficient)<br />
the new <code>mountonly</code> workflow provides a solution.</p>
<p>To recover from arbitrary soft errors on the original machine<br />
it is mandatory to have an automated way to clean up<br />
all remainder meta data on the disks.</p>
<p>Details:</p>
<p>I experimented with a test virtual machine that has two disks<br />
with same partitioning where RAID1 is used for the partitions<br />
and the RAID1 block devices are used as PVs for LVM.</p>
<p>I did "rear mkbackup" on that machine and then "rear recover"<br />
on a second identical new virtual machine which "just worked" for me.</p>
<p>Then I used only the second machine for my actual experiments<br />
(to keep the first one as an untouched pristine original system).</p>
<p>On the second machine I implemented completely new experimental<br />
scripts from scratch to recreate the disk layout.<br />
My primary intent was to experiment how to recreate the disk layout<br />
without relying on kernel device names, cf.<br />
<a href="https://github.com/rear/rear/issues/2254">https://github.com/rear/rear/issues/2254</a></p>
<p>But my primary finding was how badly things fail<br />
when there is remainder meta data on the disk<br />
in particular remainders of RAID and LVM metadata.</p>
<p>When one recreates the disk layout (exactly as it was before)<br />
on the exact same disks that had been used before<br />
things fail badly, in particular LVM setup fails badly<br />
because of LVM metadata remainders on the disk.</p>
<p>As soon as partitioning and RAID1 block devices had been<br />
recreated by the recreation script, the LVM metadata remainders<br />
become "re-visible" again on the disk so that the LVM magic<br />
is finding again its old LVM thingies (PVs, VGs, LVs)<br />
and is building up the old LVM while at the same time<br />
the recreation script (that runs in parallel) tries to set up<br />
that LVM stuff anew which fails in unpredictable ways<br />
with arbitrary weird error messages because what exactly<br />
happens depends on random timing constraints.</p>
<h4 id="github-actions_commented_at_2020-07-02_0133"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-652727964">2020-07-02 01:33</a>:<a class="headerlink" href="#github-actions_commented_at_2020-07-02_0133" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<h4 id="gdha_commented_at_2020-12-09_1501"><img src="https://avatars.githubusercontent.com/u/888633?u=cdaeb31efcc0048d3619651aa18dd4b76e636b21&v=4" width="50"><a href="https://github.com/gdha">gdha</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-741827761">2020-12-09 15:01</a>:<a class="headerlink" href="#gdha_commented_at_2020-12-09_1501" title="Permanent link">&para;</a></h4>
<p>As the PR
<a href="https://github.com/rear/rear/pull/2514">https://github.com/rear/rear/pull/2514</a>
is in progress we better re-open this issue</p>
<h4 id="github-actions_commented_at_2021-02-08_0201"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-774818726">2021-02-08 02:01</a>:<a class="headerlink" href="#github-actions_commented_at_2021-02-08_0201" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<h4 id="jsmeix_commented_at_2021-03-16_1513"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-800349887">2021-03-16 15:13</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-16_1513" title="Permanent link">&para;</a></h4>
<p>I like to merge
<a href="https://github.com/rear/rear/pull/2514">https://github.com/rear/rear/pull/2514</a><br />
tomorrow afternoon unless there are objections<br />
to have at least a first step that can be used in ReaR.<br />
Then we can improve things as needed step by step.</p>
<h4 id="jsmeix_commented_at_2021-03-17_1324"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-801077224">2021-03-17 13:24</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-17_1324" title="Permanent link">&para;</a></h4>
<p>With
<a href="https://github.com/rear/rear/pull/2514">https://github.com/rear/rear/pull/2514</a>
merged<br />
disks can be wiped before recreating partitions/volumes/filesystems/...</p>
<p>There is now the new DISKS_TO_BE_WIPED in default.conf<br />
and for details and some background information see<br />
usr/share/rear/layout/recreate/default/README.wipe_disks<br />
<a href="https://github.com/rear/rear/blob/master/usr/share/rear/layout/recreate/default/README.wipe_disks">https://github.com/rear/rear/blob/master/usr/share/rear/layout/recreate/default/README.wipe_disks</a></p>
<p>This is currently new and experimental functionality so that<br />
currently by default via DISKS_TO_BE_WIPED='false' no disk is wiped<br />
to avoid possible regressions until this new feature was more tested<br />
by interested users via explicit DISKS_TO_BE_WIPED='' in local.conf</p>
<h4 id="jsmeix_commented_at_2021-03-17_1410"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/799#issuecomment-801112009">2021-03-17 14:10</a>:<a class="headerlink" href="#jsmeix_commented_at_2021-03-17_1410" title="Permanent link">&para;</a></h4>
<p>Phew!<br />
I created this issue on Mar 15 2016<br />
and closed it today on Mar 17 2021<br />
which is 5 years and two days later<br />
and it is still not completely solved, cf.<br />
<a href="https://github.com/rear/rear/pull/2514#issuecomment-801097878">https://github.com/rear/rear/pull/2514#issuecomment-801097878</a></p>
<hr />
<p>[Export of Github issue for
<a href="https://github.com/rear/rear">rear/rear</a>.]</p>
              
            </div>
          </div>

<footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2025 - CC0 1.0 Universal<br />Give <a href="https://github.com/rear/rear-user-guide/issues/new?title=issues/2016-03-15.799.issue.closed.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/rear/rear-user-guide" class="fa fa-code-fork" style="color: #fcfcfc"> rear/rear-user-guide</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
