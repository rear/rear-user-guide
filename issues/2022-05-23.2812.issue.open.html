<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <meta property="og:title" content="Relax-and-Recover (ReaR) User Guide Documentation"/>
    <meta property="og:description" content="This is an umbrella documentation project for all Relax-and-Recover (ReaR) kind of documentation ans starting with a good User Guide."/>
    <meta property="og:locale" content="en_US"/>
    <meta property="og:url" content="https://relax-and-recover.org/rear-user-guide/"/>
    <meta property="og:type" content="website"/>
    <meta property="og:image" content="https://relax-and-recover.org/rear-user-guide/img/rear_logo_50.png"/>
    <meta property="og:image:width" content="50"/>
    <meta property="og:image:height" content="50"/>
    
    <title>#2812 Issue open: Migrating to other PowerVM LPAR with different disks: Restore multipath problem: no WWIDs - Relax-and-Recover (ReaR) User Guide Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/rear.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "#2812 Issue open: Migrating to other PowerVM LPAR with different disks: Restore multipath problem: no WWIDs";
        var mkdocs_page_input_path = "issues/2022-05-23.2812.issue.open.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/rust.min.js"></script>
      <script>hljs.highlightAll();</script> 
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', "366986045", "auto");
        ga('send', 'pageview');
      </script>
    
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../index.html" class="icon icon-home"> Relax-and-Recover (ReaR) User Guide Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">WELCOME</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../welcome/index.html">Get started!</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">BASICS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/introduction.html">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/history.html">Bit of History</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/getting-started.html">Getting started with ReaR</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/configuration.html">Basic configuration</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../basics/backup_netfs.html">Example of BACKUP=NETFS</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SCENARIOS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../scenarios/index.html">Scenarios Overview</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">DEVELOPMENT</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../development/github-pr.html">Make a pull request with GitHub</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">RELEASE NOTES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/index.html">Release Notes</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear28.html">Release Notes ReaR 2.8</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear27.html">Release Notes ReaR 2.7</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/rear26.html">Release Notes ReaR 2.6</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../releasenotes/knownproblems.html">Known Problems and Workarounds</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ISSUES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="index.html">Issues History</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/contributing/index.html">Contributing</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../legal/license/index.html">License</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Relax-and-Recover (ReaR) User Guide Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html" class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">#2812 Issue open: Migrating to other PowerVM LPAR with different disks: Restore multipath problem: no WWIDs</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2812_issue_open_migrating_to_other_powervm_lpar_with_different_disks_restore_multipath_problem_no_wwids"><a href="https://github.com/rear/rear/issues/2812">#2812 Issue</a> <code>open</code>: Migrating to other PowerVM LPAR with different disks: Restore multipath problem: no WWIDs<a class="headerlink" href="#2812_issue_open_migrating_to_other_powervm_lpar_with_different_disks_restore_multipath_problem_no_wwids" title="Permanent link">&para;</a></h1>
<p><strong>Labels</strong>: <code>enhancement</code>, <code>support / question</code>,
<code>special hardware or VM</code></p>
<h4 id="markbertolin_opened_issue_at_2022-05-23_1525"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> opened issue at <a href="https://github.com/rear/rear/issues/2812">2022-05-23 15:25</a>:<a class="headerlink" href="#markbertolin_opened_issue_at_2022-05-23_1525" title="Permanent link">&para;</a></h4>
<h4 id="relax-and-recover_rear_issue_template">Relax-and-Recover (ReaR) Issue Template<a class="headerlink" href="#relax-and-recover_rear_issue_template" title="Permanent link">&para;</a></h4>
<p>Fill in the following items before submitting a new issue<br />
(quick response is not guaranteed with free support):</p>
<ul>
<li>
<p>ReaR version ("/usr/sbin/rear -V"):<br />
    Relax-and-Recover 2.4 / Git</p>
</li>
<li>
<p>OS version ("cat /etc/os-release" or "lsb_release -a" or "cat
    /etc/rear/os.conf"):</p>
</li>
</ul>
<!-- -->

<pre><code>NAME="SLES"
VERSION="12-SP3"
VERSION_ID="12.3"
PRETTY_NAME="SUSE Linux Enterprise Server 12 SP3"
ID="sles"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles_sap:12:sp3"
</code></pre>
<ul>
<li>ReaR configuration files ("cat /etc/rear/site.conf" and/or "cat
    /etc/rear/local.conf"):</li>
</ul>
<!-- -->

<pre><code>mm-001-hbp01:/etc/multipath # cat /etc/rear/site.conf
cat: /etc/rear/site.conf: No such file or directory

mm-001-hbp01:/etc/multipath # cat /etc/rear/local.conf|grep -v grep |grep -v '#'
MIGRATION_MODE='true'
BOOT_OVER_SAN=y
BACKUP=NETFS
AUTORESIZE_PARTITIONS=true
OUTPUT=ISO
AUTOEXCLUDE_MULTIPATH=n
REQUIRED_PROGS=( "${REQUIRED_PROGS[@]}" snapper chattr lsattr multipath )
BACKUP_URL=nfs://nfs-export.mtrmil.locale/u01/data
OUTPUT_URL=nfs://nfs-export.mtrmil.locale/u01/data
USE_STATIC_NETWORKING=y
BACKUP_PROG_EXCLUDE=("${BACKUP_PROG_EXCLUDE[@]}" '/hana/shared' '/hana/data' '/hana/log' '/media' '/var/tmp' '/var/crash' '/usr/sap' '/sapmnt/BPP' '/mnt')
SSH_ROOT_PASSWORD="zaq12wsx"
</code></pre>
<ul>
<li>
<p>Hardware vendor/product (PC or PowerNV BareMetal or ARM) or VM (KVM
    guest or PowerVM LPAR):<br />
    PowerVM LPAR</p>
</li>
<li>
<p>System architecture (x86 compatible or /PPC64LE or what exact ARM
    device):<br />
    PPC64</p>
</li>
<li>
<p>Firmware (BIOS or UEFI or Open Firmware) and bootloader (GRUB or
    ELILO or Petitboot):<br />
    GRUB2</p>
</li>
<li>
<p>Storage (local disk or SSD) and/or SAN (FC or iSCSI or FCoE) and/or
    multipath (DM or NVMe):<br />
    SAN FC and mutipath DM</p>
</li>
<li>
<p>Storage layout ("lsblk -ipo
    NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,LABEL,SIZE,MOUNTPOINT"):</p>
</li>
</ul>
<!-- -->

<pre><code>mm-001-hbp01:~ # lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,LABEL,SIZE,MOUNTPOINT
NAME                                            KNAME     PKNAME    TRAN TYPE  FSTYPE      LABEL         SIZE MOUNTPOINT
/dev/sda                                        /dev/sda                 disk                            120G
|-/dev/sda1                                     /dev/sda1 /dev/sda       part                              7M
`-/dev/sda2                                     /dev/sda2 /dev/sda       part  LVM2_member               120G
/dev/sdb                                        /dev/sdb                 disk                            120G
|-/dev/sdb1                                     /dev/sdb1 /dev/sdb       part                              7M
`-/dev/sdb2                                     /dev/sdb2 /dev/sdb       part  LVM2_member               120G
/dev/sdc                                        /dev/sdc                 disk                            120G
|-/dev/sdc1                                     /dev/sdc1 /dev/sdc       part                              7M
`-/dev/sdc2                                     /dev/sdc2 /dev/sdc       part  LVM2_member               120G
/dev/sdd                                        /dev/sdd                 disk                            120G
|-/dev/sdd1                                     /dev/sdd1 /dev/sdd       part                              7M
`-/dev/sdd2                                     /dev/sdd2 /dev/sdd       part  LVM2_member               120G
  `-/dev/mapper/system-root                     /dev/dm-0 /dev/sdd2      lvm   xfs                        60G /
/dev/sde                                        /dev/sde                 disk                            120G
`-/dev/mapper/360050763808102f52400000000000080 /dev/dm-1 /dev/sde       mpath                           120G
/dev/sdf                                        /dev/sdf                 disk                            120G
`-/dev/mapper/360050763808102f52400000000000080 /dev/dm-1 /dev/sdf       mpath                           120G
/dev/sdg                                        /dev/sdg                 disk                            120G
`-/dev/mapper/360050763808102f52400000000000080 /dev/dm-1 /dev/sdg       mpath                           120G
/dev/sdh                                        /dev/sdh                 disk                            120G
`-/dev/mapper/360050763808102f52400000000000080 /dev/dm-1 /dev/sdh       mpath                           120G
/dev/sr0                                        /dev/sr0                 rom   iso9660     RELAXRECOVER 92.9M
</code></pre>
<ul>
<li>Description of the issue (ideally so that others can reproduce it):</li>
</ul>
<p>I'm trying to restore a LPAR to anther IBM Power;<br />
when restore the LPAR I have duplicate PV and<br />
mutipath software not seems good:</p>
<pre><code>mm-001-hbp01:~ # pvscan
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sdb2 not /dev/sdc2
  Using duplicate PV /dev/sdb2 which is last seen, replacing /dev/sdc2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sda2 not /dev/sdb2
  Using duplicate PV /dev/sda2 which is last seen, replacing /dev/sdb2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sdc2 not /dev/sda2
  Using duplicate PV /dev/sdc2 which is last seen, replacing /dev/sda2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sdb2 not /[dev](url)/sdc2
  Using duplicate PV /dev/sdb2 which is last seen, replacing /dev/sdc2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sda2 not /dev/sdb2
  Using duplicate PV /dev/sda2 which is last seen, replacing /dev/sdb2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sdc2 not /dev/sda2
  Using duplicate PV /dev/sdc2 which is last seen, replacing /dev/sda2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sdb2 not /dev/sdc2
  Using duplicate PV /dev/sdb2 which is last seen, replacing /dev/sdc2
  Found duplicate PV xOiaj3oTeupHtFHRtOPqzB9ALugFrNSl: using /dev/sda2 not /dev/sdb2
  Using duplicate PV /dev/sda2 which is last seen, replacing /dev/sdb2
  PV /dev/sda2   VG system   lvm2 [119.99 GiB / 0    free]
  Total: 1 [119.99 GiB] / in use: 1 [119.99 GiB] / in no VG: 0 [0   ]

mm-001-hbp01:~ # mutipath -ll
If 'mutipath' is not a typo you can use command-not-found to lookup the package that contains it, like this:
    cnf mutipath
</code></pre>
<ul>
<li>
<p>Workaround, if any:</p>
</li>
<li>
<p>Attachments, as applicable ("rear -D mkrescue/mkbackup/recover"
    debug log files):<br />
<a href="https://github.com/rear/rear/files/8755935/rear-mm-001-hbp01.log">rear-mm-001-hbp01.log</a></p>
</li>
</ul>
<p>To paste verbatim text like command output or file content,<br />
include it between a leading and a closing line of three backticks like</p>
<pre><code>```
verbatim content
```
</code></pre>
<p>How coud I do?<br />
Many thx</p>
<p>Marco</p>
<h4 id="pcahyna_commented_at_2022-05-23_1549"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1134845800">2022-05-23 15:49</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-23_1549" title="Permanent link">&para;</a></h4>
<p>Your storage layout seems to show that the system disks are not using
multipath (LVM sits directly on top of <code>/dev/sd[a-d]2</code>, without any
multipath devices involved), so I wonder why should multipath be a
problem in this situation. Or is the output of <code>lsblk</code> wrong and there
should actually be multipath devices layered between the disks and LVM?</p>
<h4 id="markbertolin_commented_at_2022-05-23_2032"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1135111644">2022-05-23 20:32</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-05-23_2032" title="Permanent link">&para;</a></h4>
<p>Hi,<br />
the original server has multipath on and in good state:</p>
<pre><code>mm-001-h4p01:~ # multipath -ll
360050763808182f5fc00000000000009 dm-1 IBM,2145
size=4.5T features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:0:1 sdb  8:16   active ready running
| |- 2:0:0:1 sdr  65:16  active ready running
| |- 3:0:0:1 sdah 66:16  active ready running
| `- 4:0:0:1 sdax 67:16  active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:1:1 sdj  8:144  active ready running
  |- 2:0:1:1 sdz  65:144 active ready running
  |- 3:0:1:1 sdap 66:144 active ready running
  `- 4:0:1:1 sdbf 67:144 active ready running
360050763808182f5fc00000000000037 dm-7 IBM,2145
size=1.0G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:1:7 sdp  8:240  active ready running
| |- 2:0:1:7 sdaf 65:240 active ready running
| |- 3:0:1:7 sdav 66:240 active ready running
| `- 4:0:1:7 sdbl 67:240 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:0:7 sdh  8:112  active ready running
  |- 2:0:0:7 sdx  65:112 active ready running
  |- 3:0:0:7 sdan 66:112 active ready running
  `- 4:0:0:7 sdbd 67:112 active ready running
360050763808182f5fc00000000000036 dm-6 IBM,2145
size=1.0G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:0:6 sdg  8:96   active ready running
</code></pre>
<p>With the local.conf file set as above,<br />
I give the read mkbackup command and<br />
the ISO file is generated for the recovery<br />
of the operating system and the whole LPAR machine.<br />
It is carried with the scp command on the<br />
VIOS Virtual Media Repository and mounted it<br />
on the partition with loadopt command.<br />
After i start the LPAR and REAR RECOVER mask appear<br />
with the login prompt.<br />
I enter the Shell and give the "read -v recovery" command.<br />
Seems that the multipath service starts automatically<br />
(MIGRATION_MODE is true) but I continue to see 4 disks<br />
even if they are only one.<br />
By changing the disklayout.conf with the proposals given,<br />
I can restore but the multipath no longer works:<br />
WWIDS magically disappear in the configuration file and<br />
the machine sees the boot disk as if it were local,<br />
not in san.<br />
i do the command multipath -ll and i see.. nothings,<br />
only the prompt!!!<br />
Only after changing the file wwids with the correct number<br />
I have:</p>
<pre><code>mm-001-hbp01:~ # cd /etc/multipath/

mm-001-hbp01:/etc/multipath # ll
total 8
-rw------- 1 root root 200 May 23 12:17 bindings
-rw------- 1 root root 226 May 23 15:21 wwids

mm-001-hbp01:/etc/multipath # cat wwids
# Multipath wwids, Version : 1.0
# NOTE: This file is automatically maintained by multipath and multipathd.
# You should not need to edit this file in normal circumstances.
#
# Valid WWIDs:
/360050763808102f52400000000000080/
</code></pre>
<p>but</p>
<pre><code>mm-001-hbp01:~ # multipath
May 23 22:25:26 | 360050763808102f5240000000000007d: ignoring map
May 23 22:25:26 | 360050763808102f5240000000000007d: ignoring map
May 23 22:25:27 | 360050763808102f5240000000000007d: ignoring map
May 23 22:25:27 | 360050763808102f5240000000000007d: ignoring map
</code></pre>
<p>and</p>
<pre><code>mm-001-hbp01:~ # cat /etc/multipath/bindings
# Multipath bindings, Version : 1.0
# NOTE: this file is automatically maintained by the multipath program.
# You should not need to edit this file in normal circumstances.
#
# Format:
# alias wwid
#
</code></pre>
<p>There are some customization to put into local.conf<br />
or other files to work in good manner the<br />
multipath software layer ad the smooth recovery<br />
withouth stops?</p>
<p>Many thx<br />
Marco</p>
<h4 id="pcahyna_commented_at_2022-05-24_0910"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1135617514">2022-05-24 09:10</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-24_0910" title="Permanent link">&para;</a></h4>
<p>The <code>lsblk</code> command was executed on the original server? If so, why
doesn't it show <code>sdb</code> as part of a multipath device?</p>
<p>By the way, please quote the output of commands using triple backticks,
otherwise it is unreadable.</p>
<h4 id="markbertolin_commented_at_2022-05-24_1007"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1135706323">2022-05-24 10:07</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-05-24_1007" title="Permanent link">&para;</a></h4>
<p>Hi,<br />
I want to show a disks with mutipath into recover shell ...<br />
the original system is POWERPC with VIOS and NPIV<br />
with boot over SAN by</p>
<pre><code>mm-001-hbp02:~ # cat /etc/multipath.conf
defaults {
user_friendly_names no
}
devices {
device {
vendor "IBM"
product "2145"
path_grouping_policy group_by_prio
prio "alua"
path_checker "tur"
Path_selector "service-time 0"
failback "immediate"
rr_weight "priorities"
no_path_retry "fail"
rr_min_io_rq 10
dev_loss_tmo 600
fast_io_fail_tmo 5
}
}
</code></pre>
<p>have you some ideas to doi it?<br />
without multipathd i cannot restore the system.<br />
thx</p>
<h4 id="pcahyna_commented_at_2022-05-24_1027"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1135734904">2022-05-24 10:27</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-24_1027" title="Permanent link">&para;</a></h4>
<p>I don't get it. Is your original <code>lsblk</code> output on the original system
or not? If yes, why does not it show the same multipath devices as your
<code>multipath -ll</code> output? And are you restoring on a different system,
with different disks, than where the backup was created? Can you please
provide your <code>/var/lib/rear/layout/disklayout.conf</code> file?</p>
<h4 id="markbertolin_commented_at_2022-05-24_1235"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1135864534">2022-05-24 12:35</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-05-24_1235" title="Permanent link">&para;</a></h4>
<p>lsblk output isn't original ..<br />
I restored on a different system, with different new disks.<br />
The original lsblk and multipath are:</p>
<pre><code>mm-001-h4p01:/etc/multipath # lsblk
NAME                                      MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                                         8:0    0  120G  0 disk
├─sda1                                      8:1    0    7M  0 part
├─sda2                                      8:2    0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdb                                         8:16   0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sdc                                         8:32   0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdd                                         8:48   0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sde                                         8:64   0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdf                                         8:80   0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdg                                         8:96   0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdh                                         8:112  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdi                                         8:128  0  120G  0 disk
├─sdi1                                      8:129  0    7M  0 part
├─sdi2                                      8:130  0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdj                                         8:144  0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sdk                                         8:160  0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdl                                         8:176  0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdm                                         8:192  0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdn                                         8:208  0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdo                                         8:224  0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdp                                         8:240  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdq                                        65:0    0  120G  0 disk
├─sdq1                                     65:1    0    7M  0 part
├─sdq2                                     65:2    0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdr                                        65:16   0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sds                                        65:32   0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdt                                        65:48   0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdu                                        65:64   0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdv                                        65:80   0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdw                                        65:96   0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdx                                        65:112  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdy                                        65:128  0  120G  0 disk
├─sdy1                                     65:129  0    7M  0 part
├─sdy2                                     65:130  0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdz                                        65:144  0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sdaa                                       65:160  0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdab                                       65:176  0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdac                                       65:192  0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdad                                       65:208  0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdae                                       65:224  0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdaf                                       65:240  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdag                                       66:0    0  120G  0 disk
├─sdag1                                    66:1    0    7M  0 part
├─sdag2                                    66:2    0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdah                                       66:16   0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sdai                                       66:32   0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdaj                                       66:48   0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdak                                       66:64   0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdal                                       66:80   0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdam                                       66:96   0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdan                                       66:112  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdao                                       66:128  0  120G  0 disk
├─sdao1                                    66:129  0    7M  0 part
├─sdao2                                    66:130  0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdap                                       66:144  0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sdaq                                       66:160  0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdar                                       66:176  0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdas                                       66:192  0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdat                                       66:208  0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdau                                       66:224  0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdav                                       66:240  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdaw                                       67:0    0  120G  0 disk
├─sdaw1                                    67:1    0    7M  0 part
├─sdaw2                                    67:2    0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdax                                       67:16   0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sday                                       67:32   0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdaz                                       67:48   0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdba                                       67:64   0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdbb                                       67:80   0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdbc                                       67:96   0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdbd                                       67:112  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath
sdbe                                       67:128  0  120G  0 disk
├─sdbe1                                    67:129  0    7M  0 part
├─sdbe2                                    67:130  0  120G  0 part
└─360050763808182f5fc0000000000000d       254:0    0  120G  0 mpath
  ├─360050763808182f5fc0000000000000d-part1
  │                                       254:8    0    7M  0 part
  └─360050763808182f5fc0000000000000d-part2
                                          254:9    0  120G  0 part
    ├─system-root                         254:10   0   60G  0 lvm   /
    └─system-swap                         254:11   0   60G  0 lvm   [SWAP]
sdbf                                       67:144  0  4.6T  0 disk
└─360050763808182f5fc00000000000009       254:1    0  4.6T  0 mpath
  └─vg_data-lv_data                       254:15   0  4.6T  0 lvm   /hana/data
sdbg                                       67:160  0  512G  0 disk
└─360050763808182f5fc0000000000000a       254:2    0  512G  0 mpath
  └─vg_log-lv_log                         254:13   0  512G  0 lvm   /hana/log
sdbh                                       67:176  0  512G  0 disk
└─360050763808182f5fc0000000000000b       254:3    0  512G  0 mpath
  └─vg_shared-lv_shared                   254:12   0  512G  0 lvm   /hana/shared
sdbi                                       67:192  0   10G  0 disk
└─360050763808182f5fc0000000000000c       254:4    0   10G  0 mpath
  └─vg_usr-lv_usr                         254:14   0   10G  0 lvm   /usr/sap
sdbj                                       67:208  0    1G  0 disk
└─360050763808182f5fc00000000000031       254:5    0    1G  0 mpath
sdbk                                       67:224  0    1G  0 disk
└─360050763808182f5fc00000000000036       254:6    0    1G  0 mpath
sdbl                                       67:240  0    1G  0 disk
└─360050763808182f5fc00000000000037       254:7    0    1G  0 mpath

mm-001-h4p01:/etc/multipath # multipath -ll
360050763808182f5fc00000000000009 dm-1 IBM,2145
size=4.5T features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:0:1 sdb  8:16   active ready running
| |- 2:0:0:1 sdr  65:16  active ready running
| |- 3:0:0:1 sdah 66:16  active ready running
| `- 4:0:0:1 sdax 67:16  active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:1:1 sdj  8:144  active ready running
  |- 2:0:1:1 sdz  65:144 active ready running
  |- 3:0:1:1 sdap 66:144 active ready running
  `- 4:0:1:1 sdbf 67:144 active ready running
360050763808182f5fc00000000000037 dm-7 IBM,2145
size=1.0G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:1:7 sdp  8:240  active ready running
| |- 2:0:1:7 sdaf 65:240 active ready running
| |- 3:0:1:7 sdav 66:240 active ready running
| `- 4:0:1:7 sdbl 67:240 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:0:7 sdh  8:112  active ready running
  |- 2:0:0:7 sdx  65:112 active ready running
  |- 3:0:0:7 sdan 66:112 active ready running
  `- 4:0:0:7 sdbd 67:112 active ready running
360050763808182f5fc00000000000036 dm-6 IBM,2145
size=1.0G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:0:6 sdg  8:96   active ready running
| |- 2:0:0:6 sdw  65:96  active ready running
| |- 3:0:0:6 sdam 66:96  active ready running
| `- 4:0:0:6 sdbc 67:96  active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:1:6 sdo  8:224  active ready running
  |- 2:0:1:6 sdae 65:224 active ready running
  |- 3:0:1:6 sdau 66:224 active ready running
  `- 4:0:1:6 sdbk 67:224 active ready running
360050763808182f5fc0000000000000d dm-0 IBM,2145
size=120G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:0:0 sda  8:0    active ready running
| |- 2:0:0:0 sdq  65:0   active ready running
| |- 3:0:0:0 sdag 66:0   active ready running
| `- 4:0:0:0 sdaw 67:0   active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:1:0 sdi  8:128  active ready running
  |- 2:0:1:0 sdy  65:128 active ready running
  |- 3:0:1:0 sdao 66:128 active ready running
  `- 4:0:1:0 sdbe 67:128 active ready running
360050763808182f5fc0000000000000c dm-4 IBM,2145
size=10G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:1:4 sdm  8:192  active ready running
| |- 2:0:1:4 sdac 65:192 active ready running
| |- 3:0:1:4 sdas 66:192 active ready running
| `- 4:0:1:4 sdbi 67:192 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:0:4 sde  8:64   active ready running
  |- 2:0:0:4 sdu  65:64  active ready running
  |- 3:0:0:4 sdak 66:64  active ready running
  `- 4:0:0:4 sdba 67:64  active ready running
360050763808182f5fc00000000000031 dm-5 IBM,2145
size=1.0G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:1:5 sdn  8:208  active ready running
| |- 2:0:1:5 sdad 65:208 active ready running
| |- 3:0:1:5 sdat 66:208 active ready running
| `- 4:0:1:5 sdbj 67:208 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:0:5 sdf  8:80   active ready running
  |- 2:0:0:5 sdv  65:80  active ready running
  |- 3:0:0:5 sdal 66:80  active ready running
  `- 4:0:0:5 sdbb 67:80  active ready running
360050763808182f5fc0000000000000b dm-3 IBM,2145
size=512G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:0:3 sdd  8:48   active ready running
| |- 2:0:0:3 sdt  65:48  active ready running
| |- 3:0:0:3 sdaj 66:48  active ready running
| `- 4:0:0:3 sdaz 67:48  active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:1:3 sdl  8:176  active ready running
  |- 2:0:1:3 sdab 65:176 active ready running
  |- 3:0:1:3 sdar 66:176 active ready running
  `- 4:0:1:3 sdbh 67:176 active ready running
360050763808182f5fc0000000000000a dm-2 IBM,2145
size=512G features='1 retain_attached_hw_handler' hwhandler='1 alua' wp=rw
|-+- policy='service-time 0' prio=50 status=active
| |- 1:0:1:2 sdk  8:160  active ready running
| |- 2:0:1:2 sdaa 65:160 active ready running
| |- 3:0:1:2 sdaq 66:160 active ready running
| `- 4:0:1:2 sdbg 67:160 active ready running
`-+- policy='service-time 0' prio=10 status=enabled
  |- 1:0:0:2 sdc  8:32   active ready running
  |- 2:0:0:2 sds  65:32  active ready running
  |- 3:0:0:2 sdai 66:32  active ready running
  `- 4:0:0:2 sday 67:32  active ready running
</code></pre>
<h4 id="jsmeix_commented_at_2022-05-25_0701"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136844282">2022-05-25 07:01</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0701" title="Permanent link">&para;</a></h4>
<p>@markbertolin</p>
<p>first and foremost:<br />
I am not at all a multipath or SAN expert.<br />
I have no personal experience with multipath or SAN and<br />
I cannot reproduce multipath or SAN issues on my homeoffice laptop<br />
so all I can do is generic help from what I see in the ReaR code<br />
and from what I can imagine from the information you show here.</p>
<p>As far as I see you have two systems:</p>
<ul>
<li>mm-001-h4p01 is your original system according to your
    <a href="https://github.com/rear/rear/issues/2812#issuecomment-1135864534">https://github.com/rear/rear/issues/2812#issuecomment-1135864534</a></li>
<li>mm-001-hbp01 seems to be your replacement system</li>
</ul>
<p>On the original system you would normally run "rear mkbackup".<br />
Then you would normally boot the ReaR recovery system<br />
on your replacement hardware, log in as 'root' and<br />
run "rear recover" on your replacement hardware.</p>
<p>But your "rear mkbackup" log file<br />
<a href="https://github.com/rear/rear/files/8755935/rear-mm-001-hbp01.log">https://github.com/rear/rear/files/8755935/rear-mm-001-hbp01.log</a><br />
seems to be from "rear mkbackup" on your replacement system<br />
so I do not understand what that actually is.</p>
<p>Your "rear mkbackup" log file<br />
<a href="https://github.com/rear/rear/files/8755935/rear-mm-001-hbp01.log">https://github.com/rear/rear/files/8755935/rear-mm-001-hbp01.log</a><br />
contains (excerpts)</p>
<pre><code>egrep " echo 'disk | echo 'multipath | echo 'part " rear-mm-001-hbp01.log | cut -d "'" -f2

multipath /dev/mapper/360050763808182f5fc0000000000003a 1073741824 /dev/sdaf,/dev/sdan,/dev/sdav,/dev/sdbd,/dev/sdbl,/dev/sdh,/dev/sdp,/dev/sdx
multipath /dev/mapper/360050763808182f5fc0000000000001c 10737418240 /dev/sdac,/dev/sdak,/dev/sdas,/dev/sdba,/dev/sdbi,/dev/sde,/dev/sdm,/dev/sdu
multipath /dev/mapper/360050763808182f5fc00000000000039 1073741824 /dev/sdae,/dev/sdam,/dev/sdau,/dev/sdbc,/dev/sdbk,/dev/sdg,/dev/sdo,/dev/sdw
multipath /dev/mapper/360050763808182f5fc0000000000001b 128849018880 /dev/sda,/dev/sdag,/dev/sdao,/dev/sdaw,/dev/sdbe,/dev/sdi,/dev/sdq,/dev/sdy
part /dev/mapper/360050763808182f5fc0000000000001b 7340032 1048576 primary boot,prep /dev/mapper/360050763808182f5fc0000000000001b-part1
part /dev/mapper/360050763808182f5fc0000000000001b 128840630272 8388608 primary lvm /dev/mapper/360050763808182f5fc0000000000001b-part2
multipath /dev/mapper/360050763808182f5fc00000000000038 1073741824 /dev/sdad,/dev/sdal,/dev/sdat,/dev/sdbb,/dev/sdbj,/dev/sdf,/dev/sdn,/dev/sdv
multipath /dev/mapper/360050763808182f5fc0000000000001a 137438953472 /dev/sdab,/dev/sdaj,/dev/sdar,/dev/sdaz,/dev/sdbh,/dev/sdd,/dev/sdl,/dev/sdt
multipath /dev/mapper/360050763808182f5fc00000000000019 137438953472 /dev/sdaa,/dev/sdai,/dev/sdaq,/dev/sday,/dev/sdbg,/dev/sdc,/dev/sdk,/dev/sds
multipath /dev/mapper/360050763808182f5fc00000000000018 137438953472 /dev/sdah,/dev/sdap,/dev/sdax,/dev/sdb,/dev/sdbf,/dev/sdj,/dev/sdr,/dev/sdz
</code></pre>
<p>so this should be the <code>disk</code> <code>multipath</code> and <code>part</code> entries<br />
in your var/lib/rear/layout/disklayout.conf file<br />
(you don't have normal <code>disk</code> entries because you have only
<code>multipath</code>).</p>
<p>What confuses me are the non-matching WWIDs as far as I see.<br />
None of the /dev/mapper/WWIDs in the disklayout.conf<br />
match the WWIDs that are shown in the 'lsblk' output in<br />
<a href="https://github.com/rear/rear/issues/2812#issuecomment-1135864534">https://github.com/rear/rear/issues/2812#issuecomment-1135864534</a><br />
which are</p>
<pre><code>360050763808182f5fc00000000000009
360050763808182f5fc0000000000000a
360050763808182f5fc0000000000000b
360050763808182f5fc0000000000000c
360050763808182f5fc0000000000000d
360050763808182f5fc00000000000031
360050763808182f5fc00000000000036
360050763808182f5fc00000000000037
</code></pre>
<p>so it seems the disklayout.conf does not match the original system?</p>
<p>But I am not a multipath expert so I may confuse things here.</p>
<h4 id="jsmeix_commented_at_2022-05-25_0709"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136859985">2022-05-25 07:09</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0709" title="Permanent link">&para;</a></h4>
<p>@pcahyna<br />
thank you for your help here - I really need it!</p>
<p>I have a question:<br />
I failed to find out by Googling for "multipath WWID"<br />
how the WWID in <code>/dev/mapper/WWID</code> is determined.<br />
All documentation I found only tells that a WWID is set<br />
but I didn't find explained how the WWID is determined.<br />
Is the WWID autogenerated from scratch as a random number<br />
or is the WWID a real existing value from the disk hardware?</p>
<p>For example on my homeoffice laptop I have</p>
<pre><code># lsblk -ipdo KNAME,WWN,PTUUID,PARTUUID,UUID /dev/sda
KNAME    WWN                PTUUID                               PARTUUID UUID
/dev/sda 0x5000039462b83c55 9be0015e-cf90-4c6b-80ac-c4ea89832553

# /usr/lib/udev/scsi_id -gud /dev/sda
35000039462b83c55

# cat /sys/block/sda/device/wwid
naa.5000039462b83c55
</code></pre>
<p>so what <code>scsi_id</code> shows matches the <code>lsblk</code> WWN of my /dev/sda<br />
and that WWN matches the /sys/block/sda/device/wwid content<br />
(I do not understand the leading <code>0x</code> versus <code>3</code> versus <code>naa.</code>
differences)<br />
but I don't know if that WWN/WWID is a random number<br />
that is autogenerated by some software (e.g. the kernel)<br />
or if it is a real existing value from my disk hardware?</p>
<h4 id="pcahyna_commented_at_2022-05-25_0729"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136890137">2022-05-25 07:29</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_0729" title="Permanent link">&para;</a></h4>
<p>@jsmeix</p>
<blockquote>
<p>so it seems the disklayout.conf does not match the original system?</p>
</blockquote>
<p>indeed:</p>
<blockquote>
<p>I restored on a different system, with different new disks.</p>
</blockquote>
<p>and I suspect this may be part of the problem - I restored PowerVM LPARs
with multipath many times with success (not with VIOS, but that should
not matter too much), but always on the same machine and I am not sure
whether <code>MIGRATION_MODE</code> can cope with WWID changes in this situation
(different hardware, different disks).</p>
<p>Concerning</p>
<blockquote>
<p>but I didn't find explained how the WWID is determined.<br />
Is the WWID autogenerated from scratch as a random number<br />
or is the WWID a real existing value from the disk hardware?</p>
</blockquote>
<p>-- it is real existing value from hardware, similar to a MAC address for
NICs. There are several types of such persistent identifiers on disks
and I must admit I am not a big expert on this topic, so I can't answer
your question on leading <code>3</code> or <code>naa</code> (but hopefully this is not very
important for understanding the problem in question).</p>
<h4 id="pcahyna_commented_at_2022-05-25_0733"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136894247">2022-05-25 07:33</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_0733" title="Permanent link">&para;</a></h4>
<p>Also WWIDs are not that much related to multipath, it's just that
without multipath you can mostly ignore them, with multipath you need
them (or some other persistent identifier), because you need to tell
which device nodes correspond to the same physical device.</p>
<h4 id="jsmeix_commented_at_2022-05-25_0733"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136894294">2022-05-25 07:33</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0733" title="Permanent link">&para;</a></h4>
<p>@pcahyna<br />
thank you so much!</p>
<p>I don't need hardware specific details.<br />
That the WWID/WWN is a hardware value is the important information.</p>
<p>So different hardware (in particular different disks)<br />
result different WWID/WWN values so the WWID/WWN values<br />
from the original system for <code>multipath</code> in disklayout.conf<br />
cannot match different hardware (in particular different disks)<br />
and therefore "rear recover" can not "just work".</p>
<h4 id="pcahyna_commented_at_2022-05-25_0739"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136899601">2022-05-25 07:39</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_0739" title="Permanent link">&para;</a></h4>
<p>In RHEL, multipath seem to be configured by default to name devices like
<code>mpatha</code>, <code>mpathb</code>, ..., without embedding the WWID in the device name,
so part of the problem is avoided there. Still, I am not sure it would
work properly, because one also needs to transform WWIDs in the
/etc/multipath/* config files.</p>
<h4 id="jsmeix_commented_at_2022-05-25_0751"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136910199">2022-05-25 07:51</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0751" title="Permanent link">&para;</a></h4>
<p>@markbertolin</p>
<p>as far as we (@pcahyna and @jsmeix) found out<br />
you do a migration onto different hardware with ReaR.</p>
<p>Migration onto different hardware does not "just work".</p>
<p>For some sufficiently simple cases migration onto<br />
only a bit different hardware could be even "relatively easy"<br />
via some basic user dialogs in ReaR's MIGRATION_MODE.</p>
<p>In general regarding different hardware:</p>
<p>When you do not have fully compatible replacement hardware<br />
then recreating the system becomes what we call a MIGRATION.</p>
<p>Cf. "Fully compatible replacement hardware is needed" in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<p>Migrating a system onto somewhat different hardware<br />
will usually not work "out of the box" with ReaR, see<br />
MIGRATION_MODE in default.conf currently online at<br />
<a href="https://github.com/rear/rear/blob/master/usr/share/rear/conf/default.conf#L397">https://github.com/rear/rear/blob/master/usr/share/rear/conf/default.conf#L397</a></p>
<p>Regarding migration to a system with a bit smaller or a bit bigger
disk<br />
see in conf/default.conf the description of the config variables<br />
AUTORESIZE_PARTITIONS<br />
AUTORESIZE_EXCLUDE_PARTITIONS<br />
AUTOSHRINK_DISK_SIZE_LIMIT_PERCENTAGE<br />
AUTOINCREASE_DISK_SIZE_THRESHOLD_PERCENTAGE</p>
<p>I reccommend to not use AUTORESIZE_PARTITIONS="yes"<br />
with layout/prepare/default/430_autoresize_all_partitions.sh<br />
because that may result bad aligned partitions in particular<br />
bad aligned for what flash memory based disks (i.e. SSDs) need<br />
that usually need a 4MiB or 8MiB alignment (a too small value<br />
will result lower speed and less lifetime of flash memory devices),<br />
see the comment at USB_PARTITION_ALIGN_BLOCK_SIZE<br />
in default.conf</p>
<p>In general regarding system migration with ReaR<br />
(e.g. to a system with substantially different disk size):</p>
<p>In general migrating a system onto different hardware<br />
(where "hardware" could be also a virtual machine)<br />
does not "just work", cf. "Inappropriate expectations" in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<p>In sufficiently simple cases it may "just work" but in general<br />
do not expect too much built-in intelligence from a program<br />
(written in plain bash which is not a programming language<br />
that is primarily meant for artificial intelligence ;-)<br />
that would do the annoying legwork for you.</p>
<p>In general ReaR is first and foremost meant to recreate<br />
a system as much as possible exactly as it was before<br />
on as much as possible same replacement hardware.</p>
<h4 id="jsmeix_commented_at_2022-05-25_0755"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136913776">2022-05-25 07:55</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0755" title="Permanent link">&para;</a></h4>
<p>Current ReaR does not support to migrate WWIDs.<br />
So migrating WWIDs needs to be done manually<br />
which means all values disklayout.conf need to be<br />
manually adapted to match the new hardware<br />
i.e. not only WWIDs but also all other values<br />
that do not match the new hardware need to be<br />
manually adapted to match the new hardware.</p>
<h4 id="jsmeix_commented_at_2022-05-25_0809"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136927106">2022-05-25 08:09</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0809" title="Permanent link">&para;</a></h4>
<p>Regardless that I am not at all SAN storage expert<br />
I am thinking about the following:</p>
<p>On the one hand ReaR supports bare metal recovery.<br />
For example ReaR supports to to recreate a system<br />
on new compatible hardware with a new built-in disk<br />
(provided the new disk is not substantially smaller).</p>
<p>On the other hand it seems ReaR does not support<br />
to recreate a system on a new PowerVM LPAR<br />
with new SAN disks.</p>
<p>I think the difference is that ReaR is only meant<br />
to recreate local disks but ReaR is not meant<br />
to recreate SAN storage.</p>
<p>I think the reason is that in general ReaR is not meant<br />
to recreate any remote things.</p>
<h4 id="pcahyna_commented_at_2022-05-25_0814"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136932550">2022-05-25 08:14</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_0814" title="Permanent link">&para;</a></h4>
<blockquote>
<p>When you do not have fully compatible replacement hardware<br />
then recreating the system becomes what we call a MIGRATION.</p>
</blockquote>
<p>The problem is, if your disk(s) die, and you replace them with a
perfectly compatible disk(s), the WWIDs still change and there is no way
to get the same WWIDs as before. So this case should be handled somehow
even without <code>MIGRATION_MODE</code>, otherwise ReaR does not fulfill even
"appropriate expectations".</p>
<h4 id="pcahyna_commented_at_2022-05-25_0816"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136935691">2022-05-25 08:16</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_0816" title="Permanent link">&para;</a></h4>
<p>Note that you don't need a SAN to obtain a multipath setup. It is enough
to have e.g. SAS disks and connect them using both ports (SAS disks are
dual-port, at least those I saw).</p>
<h4 id="jsmeix_commented_at_2022-05-25_0828"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136948749">2022-05-25 08:28</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0828" title="Permanent link">&para;</a></h4>
<p>@pcahyna<br />
regarding your<br />
<a href="https://github.com/rear/rear/issues/2812#issuecomment-1136932550">https://github.com/rear/rear/issues/2812#issuecomment-1136932550</a></p>
<pre><code>if your disk(s) die, and you replace them
with a perfectly compatible disk(s),
the WWIDs still change and there is no way
to get the same WWIDs as before
</code></pre>
<p>I think this is the reason why ReaR normally<br />
does not store WWID/WWN values in disklayout.conf<br />
except in case of <code>multipath</code> when the<br />
default <code>/dev/mapper/WWID</code> device names are used.</p>
<p>I think ReaR should support to recreate a system<br />
on a new PowerVM LPAR with new SAN disks<br />
when the new SAN and multipath disk layout<br />
is same as it was on the original system<br />
i.e. same number of disks with same size<br />
and same multipath structure.</p>
<p>This would be an enhancement for a future ReaR version.</p>
<h4 id="didacog_commented_at_2022-05-25_0831"><img src="https://avatars.githubusercontent.com/u/5380209?u=163f1571e6b9c9c7df94e2c6ca152b0a7406b52d&v=4" width="50"><a href="https://github.com/didacog">didacog</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136952808">2022-05-25 08:31</a>:<a class="headerlink" href="#didacog_commented_at_2022-05-25_0831" title="Permanent link">&para;</a></h4>
<p>Hello, just trying to help here. we've had good results with MPIO setup
migrations, but we use this base configuration for SAN boot:</p>
<pre><code>...
AUTOEXCLUDE_MULTIPATH=n
BOOT_OVER_SAN=y
MODULES=( ${MODULES[@]} dm-multipath )
MODULES_LOAD=( ${MODULES_LOAD[@]} dm-multipath )
...
</code></pre>
<p>On the other hand this is a rear 2.4, so I'll suggest to try with rear
2.6, as is SLES12 sp3.</p>
<p>Hope this can help.</p>
<p>Kind regards,<br />
Didac</p>
<h4 id="pcahyna_commented_at_2022-05-25_0834"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136956755">2022-05-25 08:34</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_0834" title="Permanent link">&para;</a></h4>
<p>I will need to look, because it is possible that it actually works for
this case already, e.g. by regenerating /etc/multipath/wwids
dynamically, or by transforming them with <code>sed</code>, or something similar,
especially if WWIDs are not used as device names under /dev/mapper (but
the <code>sed</code> transformation could handle even this case, because it could
transform even <code>disklayout.conf</code> itself ). Maybe it is something that
was fixed in ReaR 2.6, as @didacog suggests?</p>
<h4 id="jsmeix_commented_at_2022-05-25_0856"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136979031">2022-05-25 08:56</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0856" title="Permanent link">&para;</a></h4>
<p>My totally offhanded thinking is<br />
that migrating WWID/WWN values in disklayout.conf<br />
could be done similar as migrating disk device nodes<br />
(like '/dev/sda' -&gt; '/dev/sdb' and '/dev/sdb' -&gt; '/dev/sda')<br />
in disklayout.conf which we do via the function<br />
apply_layout_mappings() in lib/layout-functions.sh<br />
that is called primarily in<br />
layout/prepare/default/320_apply_mappings.sh<br />
and also in<br />
finalize/GNU/Linux/250_migrate_disk_devices_layout.sh<br />
finalize/GNU/Linux/260_rename_diskbyid.sh</p>
<p>Additionally a user dialog to map WWID/WWN values as in<br />
layout/prepare/default/300_map_disks.sh<br />
so the user has final power to decide about the mapping.</p>
<p>There is something about 'multipath' in<br />
layout/prepare/default/300_map_disks.sh<br />
but I never tested how it behaves with multipath<br />
because I do not use SAN or multipath.</p>
<h4 id="jsmeix_commented_at_2022-05-25_0913"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1136996581">2022-05-25 09:13</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_0913" title="Permanent link">&para;</a></h4>
<p>During "rear recover" multipath is activated<br />
in the ReaR recovery system with a basic setup via<br />
layout/prepare/GNU/Linux/210_load_multipath.sh<br />
which in particular loads the dm-multipath kernel module.</p>
<p>A "rear -D recover" debug log file could help to see<br />
what actually happened during "rear recover".</p>
<h4 id="pcahyna_commented_at_2022-05-25_1049"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1137091807">2022-05-25 10:49</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-05-25_1049" title="Permanent link">&para;</a></h4>
<p>As the first step I believe the current Git master code, or at least
2.6, should be tested, because it may behave differently or even fix the
issue.</p>
<h4 id="jsmeix_commented_at_2022-05-25_1058"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1137098862">2022-05-25 10:58</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-05-25_1058" title="Permanent link">&para;</a></h4>
<p>@markbertolin</p>
<p>see the sections<br />
"Testing current ReaR upstream GitHub master code"<br />
and<br />
"Debugging issues with Relax-and-Recover"<br />
in<br />
<a href="https://en.opensuse.org/SDB:Disaster_Recovery">https://en.opensuse.org/SDB:Disaster_Recovery</a></p>
<h4 id="markbertolin_commented_at_2022-05-26_0817"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1138285927">2022-05-26 08:17</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-05-26_0817" title="Permanent link">&para;</a></h4>
<p>Hi ,<br />
many thx for your updates ,,, I try all and I test it, pls stand by...<br />
Marco</p>
<h4 id="markbertolin_commented_at_2022-05-26_0821"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1138289060">2022-05-26 08:21</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-05-26_0821" title="Permanent link">&para;</a></h4>
<p>PS: the Powers are twins, the systems are identical harware.</p>
<h4 id="markbertolin_commented_at_2022-06-01_0854"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1143321178">2022-06-01 08:54</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-06-01_0854" title="Permanent link">&para;</a></h4>
<p>Hi,<br />
i have tried some time.. the multipath goes on but some path was
dead/unknown:</p>
<pre><code>RESCUE mm-001-hbp02:~ # multipath -d
: 360050763808102f5240000000000008a undef IBM,2145
size=128G features='0' hwhandler='1 alua' wp=undef
|-+- policy='service-time 0' prio=50 status=undef
| |- 1:0:0:1 sdc  8:32   active ready running
| `- 2:0:0:1 sdd  8:48   active ready running
`-+- policy='service-time 0' prio=10 status=undef
  |- 1:0:1:1 sds  65:32  undef ready running
  `- 2:0:1:1 sdt  65:48  undef ready running
</code></pre>
<p>into the /var/lib/rear/layout/disklayout.conf conf file i have also:</p>
<pre><code>#multipath /dev/mapper/360050763808102f52400000000000086 1073741824 unknown /dev/sdac,/dev/sdad,/dev/sdm,/dev/sdn
#multipath /dev/mapper/360050763808102f52400000000000085 1073741824 unknown /dev/sdaa,/dev/sdab,/dev/sdk,/dev/sdl
multipath /dev/mapper/360050763808102f5240000000000008b 10737418240 unknown /dev/sdi,/dev/sdj,/dev/sdy,/dev/sdz
multipath /dev/mapper/360050763808102f5240000000000008a 137438953472 unknown /dev/sdd,/dev/sde,/dev/sds,/dev/sdt
multipath /dev/mapper/360050763808102f52400000000000089 137438953472 unknown /dev/sdb,/dev/sdf,/dev/sdu,/dev/sdv
multipath /dev/mapper/360050763808102f52400000000000088 137438953472 unknown /dev/sdg,/dev/sdh,/dev/sdw,/dev/sdx
#multipath /dev/mapper/360050763808102f52400000000000087 1073741824 unknown /dev/sdae,/dev/sdaf,/dev/sdo,/dev/sdp
multipath /dev/mapper/360050763808102f5240000000000007e 128849018880 msdos /dev/sda,/dev/sdc,/dev/sdq,/dev/sdr
part /dev/mapper/360050763808102f5240000000000007e 7340032 2097152 primary boot,prep /dev/mapper/360050763808102f5240000000000007e-part1
part /dev/mapper/360050763808102f5240000000000007e 128839577600 9441280 primary lvm /dev/mapper/360050763808102f5240000000000007e-part2
</code></pre>
<p>the restore start good</p>
<pre><code>RESCUE mm-001-hbp02:~ # rear -v -D recover
Relax-and-Recover 2.6 / 2020-06-17
Running rear recover (PID 17289)
Using log file: /var/log/rear/rear-mm-001-hbp02.log
Running workflow recover within the ReaR rescue/recovery system
Starting required daemons for NFS: RPC portmapper (portmap or rpcbind) and rpc.statd if available.
Started RPC portmapper 'rpcbind'.
RPC portmapper 'rpcbind' available.
RPC status rpc.statd available.
Starting rpc.idmapd failed.
Using backup archive '/tmp/rear.KL96sXAHLH28YHH/outputfs/m
[HMCLogs-31052022-2.txt](https://github.com/rear/rear/files/8812649/HMCLogs-31052022-2.txt)
m-001-hbp02/backup.tar.gz'
Will do driver migration (recreating initramfs/initrd)
Calculating backup archive size
Backup archive size is 33G      /tmp/rear.KL96sXAHLH28YHH/outputfs/mm-001-hbp02/backup.tar.gz (compressed)
Setting up multipathing
Activating multipath
multipath activated
Starting multipath daemon
multipathd started
Listing multipath device found
360050763808102f52400000000000086 dm-8 IBM,2145 size=1.0G
360050763808102f52400000000000085 dm-7 IBM,2145 size=1.0G
360050763808102f5240000000000008b dm-6 IBM,2145 size=10G
360050763808102f5240000000000008a dm-3 IBM,2145 size=128G
360050763808102f52400000000000089 dm-4 IBM,2145 size=128G
360050763808102f52400000000000088 dm-5 IBM,2145 size=128G
360050763808102f52400000000000087 dm-9 IBM,2145 size=1.0G
360050763808102f5240000000000007e dm-0 IBM,2145 size=120G
Enforced manual disk layout configuration (MIGRATION_MODE is 'true')
Using /dev/mapper/360050763808102f5240000000000008b (same name and same size) for recreating /dev/mapper/360050763808102f5240000000000008b
Using /dev/mapper/360050763808102f5240000000000008a (same name and same size) for recreating /dev/mapper/360050763808102f5240000000000008a
Using /dev/mapper/360050763808102f52400000000000089 (same name and same size) for recreating /dev/mapper/360050763808102f52400000000000089
Using /dev/mapper/360050763808102f52400000000000088 (same name and same size) for recreating /dev/mapper/360050763808102f52400000000000088
Using /dev/mapper/360050763808102f5240000000000007e (same name and same size) for recreating /dev/mapper/360050763808102f5240000000000007e
Current disk mapping table (source =&gt; target):
  /dev/mapper/360050763808102f5240000000000008b =&gt; /dev/mapper/360050763808102f5240000000000008b
  /dev/mapper/360050763808102f5240000000000008a =&gt; /dev/mapper/360050763808102f5240000000000008a
  /dev/mapper/360050763808102f52400000000000089 =&gt; /dev/mapper/360050763808102f52400000000000089
  /dev/mapper/360050763808102f52400000000000088 =&gt; /dev/mapper/360050763808102f52400000000000088
  /dev/mapper/360050763808102f5240000000000007e =&gt; /dev/mapper/360050763808102f5240000000000007e
</code></pre>
<p>but after it fall down :</p>
<pre><code>UserInput -I LAYOUT_MIGRATION_CONFIRM_MAPPINGS needed in /usr/share/rear/layout/prepare/default/300_map_disks.sh line 275
</code></pre>
<p>I attach the logs files.</p>
<p>My config path is:</p>
<pre><code>RESCUE mm-001-hbp02:~ # cat /etc/multipath.conf
defaults {
user_friendly_names no
}
devices {
device {
vendor "IBM"
product "2145"
path_grouping_policy group_by_prio
prio "alua"
path_checker "tur"
path_selector "service-time 0"
failback "immediate"
rr_weight "priorities"
no_path_retry "fail"
rr_min_io_rq 10
dev_loss_tmo 600
fast_io_fail_tmo 5
}
}
</code></pre>
<p>Could help me pls?<br />
thx and regards<br />
Marco</p>
<h4 id="pcahyna_commented_at_2022-06-01_1203"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1143519126">2022-06-01 12:03</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-06-01_1203" title="Permanent link">&para;</a></h4>
<p>@markbertolin I don't see the log files... and how it "fell down"?
UserInput seems to indicate that it merely needs confirmation from you
(on console).<br />
By the way, it is interesting that the multipath device names like
<code>360050763808102f52400000000000087</code> did not change (if you are restoring
to different disks, the WWIDs should be different).</p>
<h4 id="pcahyna_commented_at_2022-06-01_1210"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1143525331">2022-06-01 12:10</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-06-01_1210" title="Permanent link">&para;</a></h4>
<p>Actually, have the WWIDs changed or not? You say</p>
<blockquote>
<p>into the /var/lib/rear/layout/disklayout.conf conf file i have also:</p>
</blockquote>
<pre><code>#multipath /dev/mapper/360050763808102f52400000000000086 1073741824 unknown /dev/sdac,/dev/sdad,/dev/sdm,/dev/sdn
#multipath /dev/mapper/360050763808102f52400000000000085 1073741824 unknown /dev/sdaa,/dev/sdab,/dev/sdk,/dev/sdl
multipath /dev/mapper/360050763808102f5240000000000008b 10737418240 unknown /dev/sdi,/dev/sdj,/dev/sdy,/dev/sdz
multipath /dev/mapper/360050763808102f5240000000000008a 137438953472 unknown /dev/sdd,/dev/sde,/dev/sds,/dev/sdt
</code></pre>
<p>but in your original storage layout I don't see any
<code>360050763808102f524</code> devices. Where does this
<code>/var/lib/rear/layout/disklayout.conf</code> come from? Is it how it is on the
original system? If so, why does it show different devices than in your
comment
<a href="https://github.com/rear/rear/issues/2812#issuecomment-1135111644">https://github.com/rear/rear/issues/2812#issuecomment-1135111644</a>
?</p>
<h4 id="jsmeix_commented_at_2022-06-01_1309"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1143591832">2022-06-01 13:09</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-06-01_1309" title="Permanent link">&para;</a></h4>
<p>@markbertolin<br />
can you show us the output of the command</p>
<pre><code># lsblk -ipo NAME,KNAME,PKNAME,TRAN,TYPE,FSTYPE,LABEL,SIZE,MOUNTPOINT,UUID,WWN
</code></pre>
<p>on you original system<br />
and also<br />
on your replacement system - you may run that command<br />
inside the booted ReaR recovery system after you logged in<br />
there as 'root' but without launching "rear recover".</p>
<h4 id="pcahyna_commented_at_2022-06-01_1320"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1143604045">2022-06-01 13:20</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-06-01_1320" title="Permanent link">&para;</a></h4>
<p>@markbertolin I would be also interested in indicating for your
<code>disklayout.conf</code> snippets whether they come from the original system or
the replacement system in the rescue environment - I believe that ReaR
does some <code>sed</code> transformation on <code>disklayout.conf</code>, so after running
<code>rear recover</code> the file might be different from the original system.</p>
<h4 id="markbertolin_commented_at_2022-06-01_1654"><img src="https://avatars.githubusercontent.com/u/106096670?v=4" width="50"><a href="https://github.com/markbertolin">markbertolin</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1143873261">2022-06-01 16:54</a>:<a class="headerlink" href="#markbertolin_commented_at_2022-06-01_1654" title="Permanent link">&para;</a></h4>
<p>Hi,<br />
in attachment , the original and rescue logs files<br />
<a href="https://github.com/rear/rear/files/8816182/HMCLogs-31052022-2-2.TXT">HMCLogs-31052022-2-2.TXT</a><br />
<a href="https://github.com/rear/rear/files/8816183/lsblk_output_original.log">lsblk_output_original.log</a></p>
<p>Many thx for your support!!<br />
Marco</p>
<h4 id="pcahyna_commented_at_2022-06-02_1231"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1144809064">2022-06-02 12:31</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-06-02_1231" title="Permanent link">&para;</a></h4>
<p>@markbertolin Your lsblk_output_original.log does not match the one
shown in
<a href="https://github.com/rear/rear/issues/2812#issuecomment-1135864534">https://github.com/rear/rear/issues/2812#issuecomment-1135864534</a>
where you said that "The original lsblk and multipath are: ...". So,
which one is really the original? Also, can you please answer my
question</p>
<blockquote>
<p>Where does this /var/lib/rear/layout/disklayout.conf come from? Is it
how it is on the original system? If so, why does it show different
devices than in your comment
<a href="https://github.com/rear/rear/issues/2812#issuecomment-1135111644">https://github.com/rear/rear/issues/2812#issuecomment-1135111644</a>
?</p>
</blockquote>
<h4 id="pcahyna_commented_at_2022-06-02_1239"><img src="https://avatars.githubusercontent.com/u/26300485?u=9105d243bc9f7ade463a3e52e8dd13fa67837158&v=4" width="50"><a href="https://github.com/pcahyna">pcahyna</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1144815993">2022-06-02 12:39</a>:<a class="headerlink" href="#pcahyna_commented_at_2022-06-02_1239" title="Permanent link">&para;</a></h4>
<p>It could be also helpful to have the log file after the disk recreation
script has failed ("The disk layout recreation script failed... View
'rear recover' log file (/var/log/rear/rear-mm-001-hbp02.log)")</p>
<h4 id="github-actions_commented_at_2022-09-06_0408"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1237636237">2022-09-06 04:08</a>:<a class="headerlink" href="#github-actions_commented_at_2022-09-06_0408" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<h4 id="jsmeix_commented_at_2022-09-19_1424"><img src="https://avatars.githubusercontent.com/u/1788608?u=925fc54e2ce01551392622446ece427f51e2f0ce&v=4" width="50"><a href="https://github.com/jsmeix">jsmeix</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1251096019">2022-09-19 14:24</a>:<a class="headerlink" href="#jsmeix_commented_at_2022-09-19_1424" title="Permanent link">&para;</a></h4>
<p>Only as a side note FYI:<br />
We already have some obscure WWID migration code in<br />
layout/prepare/default/010_prepare_files.sh<br />
and<br />
finalize/GNU/Linux/250_migrate_lun_wwid.sh<br />
where some <code>LUN_WWID_MAP</code> file $CONFIG_DIR/lun_wwid_mapping.conf<br />
can be used.<br />
But I neither understand that code not its commits like<br />
<a href="https://github.com/rear/rear/commit/e1a704b641e1ae1d92ba1e19dd756e05b128b9b5">https://github.com/rear/rear/commit/e1a704b641e1ae1d92ba1e19dd756e05b128b9b5</a><br />
and<br />
<a href="https://github.com/rear/rear/commit/e822ad69a8ce8dec6132741806008db9c6c3b429">https://github.com/rear/rear/commit/e822ad69a8ce8dec6132741806008db9c6c3b429</a><br />
Furthermore I fail to find any documentation<br />
about <code>lun_wwid_mapping</code> or <code>LUN_WWID_MAP</code><br />
that explains what the idea behind is.<br />
The above two scripts are the only files in ReaR that<br />
contain <code>lun_wwid_mapping</code> or <code>LUN_WWID_MAP</code> and<br />
finalize/GNU/Linux/250_migrate_lun_wwid.sh<br />
applies the mappinh only to the restored files<br />
etc/elilo.conf (if exists) and etc/fstab<br />
so there is no code in ReaR that maps WWIDs<br />
before the storage layout is recreated.</p>
<h4 id="github-actions_commented_at_2022-11-19_0259"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1320767151">2022-11-19 02:59</a>:<a class="headerlink" href="#github-actions_commented_at_2022-11-19_0259" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<h4 id="github-actions_commented_at_2023-01-28_0232"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1407262204">2023-01-28 02:32</a>:<a class="headerlink" href="#github-actions_commented_at_2023-01-28_0232" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<h4 id="github-actions_commented_at_2023-04-02_0219"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1493202478">2023-04-02 02:19</a>:<a class="headerlink" href="#github-actions_commented_at_2023-04-02_0219" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<h4 id="github-actions_commented_at_2023-06-03_0235"><img src="https://avatars.githubusercontent.com/in/15368?v=4" width="50"><a href="https://github.com/apps/github-actions">github-actions</a> commented at <a href="https://github.com/rear/rear/issues/2812#issuecomment-1574577881">2023-06-03 02:35</a>:<a class="headerlink" href="#github-actions_commented_at_2023-06-03_0235" title="Permanent link">&para;</a></h4>
<p>Stale issue message</p>
<hr />
<p>[Export of Github issue for
<a href="https://github.com/rear/rear">rear/rear</a>.]</p>
              
            </div>
          </div>

<footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
    <p>Copyright 2024 - CC0 1.0 Universal<br />Give <a href="https://github.com/rear/rear-user-guide/issues/new?title=issues/2022-05-23.2812.issue.open.html">feedback</a> on this page.</p>
    
  </div>
</footer>

        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/rear/rear-user-guide" class="fa fa-code-fork" style="color: #fcfcfc"> rear/rear-user-guide</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
